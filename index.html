<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="On The Journey To Truth">
<meta property="og:type" content="website">
<meta property="og:title" content="Guoxing Lan">
<meta property="og:url" content="https://lankuohsing.github.io/blog/index.html">
<meta property="og:site_name" content="Guoxing Lan">
<meta property="og:description" content="On The Journey To Truth">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Guoxing Lan">
<meta name="twitter:description" content="On The Journey To Truth">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lankuohsing.github.io/blog/"/>





  <title>Guoxing Lan</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
	<a href="https://github.com/lankuohsing"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Guoxing Lan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Journey To Truth</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://lankuohsing.github.io/blog" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://lankuohsing.github.io/blog/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://lankuohsing.github.io/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://lankuohsing.github.io/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://lankuohsing.github.io/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/15/神经网络中的优化方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/15/神经网络中的优化方法/" itemprop="url">神经网络中的优化方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-15T22:25:43+08:00">
                2018-06-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Mini-batch-decent方法"><a href="#1-Mini-batch-decent方法" class="headerlink" title="1. Mini-batch decent方法"></a>1. Mini-batch decent方法</h2><h3 id="1-1-Batch-vs-mini-batch"><a href="#1-1-Batch-vs-mini-batch" class="headerlink" title="1.1. Batch vs. mini-batch"></a>1.1. Batch vs. mini-batch</h3><p>Batch：利用矢量化编程的方法，对整个训练集运用梯度下降法。梯度每下降一小步，都要处理整个训练集。这样的效率比较慢。<br>Mini-batch：将训练集拆分为更小的训练集，成为小批量训练集（mini-batch)<br>Mini-batch t: $X^{\{t\}},Y^{\{t\}}$<br>对每个mini-batch都进行一次完整的前向和反向传播过程，当对所有的mini-batch都进行了前向和反向过程后，我们称完成了对训练集的一次遍历<strong>（epoch）</strong>。<br>Batch gradient descent，原则上cost应该是单调下降（除非learning rate太大了）；Mini-batch gradient descent，整体趋势下降，但是局部是振荡的。</p>
<h3 id="1-2-Choosing-mini-batch-size"><a href="#1-2-Choosing-mini-batch-size" class="headerlink" title="1.2. Choosing mini-batch size"></a>1.2. Choosing mini-batch size</h3><ul>
<li>如果mini-batch size=m： 等价于batch gradient descent，一般可以收敛到全局最小值点；</li>
<li>如果mini-batch size=1：等价于stochastic gradient descent，不一定收敛到全局最小值点，一般会在该点处振荡。<br>如果训练集较小（&lt;2000），就使用batch gradient descent；否则，可以选择64到512之间（2的幂数）的mini-batch size。确保可以放入CPU/GPU的内存中</li>
</ul>
<h2 id="2-指数加权平均方法（exponentially-weighted-averages）"><a href="#2-指数加权平均方法（exponentially-weighted-averages）" class="headerlink" title="2. 指数加权平均方法（exponentially weighted averages）"></a>2. 指数加权平均方法（exponentially weighted averages）</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90-%E5%AF%BB%E6%89%BE%E6%B8%A9%E5%BA%A6%E8%B6%8B%E5%8A%BF.png"></center></p>
<p><center>图2.1 指数加权平均例子-寻找温度趋势</center></p>
<script type="math/tex; mode=display">
\begin{align*}
v_0&=0\\
v_1&=0.9v_0+0.1\theta_1\\
v_2&=0.9v_1+0.1\theta_2\\
v_3&=0.9v_2+0.1\theta_3\\
\vdots\\
\tag{2-1}
\end{align*}</script><p>第t天的指数平均值的通项公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
v_t&=\beta v_{t-1}+(1-\beta)\theta_t \\
&=(1-\beta)\left(\theta_t+\beta\theta_{t-1}+\cdots+\beta^{k}\theta_{t-k}+\cdots+\beta^{t-1}\theta_{1}\right) \\
\tag{2-2}
\end{align*}</script><p>近似公式：</p>
<script type="math/tex; mode=display">
v_t\approx \frac{1}{1-\beta}\ days'\ temperature\tag{2-3}</script><p>如图2.2所示，当$\beta$增大时，曲线向右平移（绿线）；$\beta$减小时，曲线振荡加剧（黄线），</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%CE%B2%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%9B%B2%E7%BA%BF%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D.png"></center></p>
<p><center>图2.2 β大小对曲线形状的影响</center></p>
<h3 id="2-1-Bias-Correction（偏差修正）"><a href="#2-1-Bias-Correction（偏差修正）" class="headerlink" title="2.1. Bias Correction（偏差修正）"></a>2.1. Bias Correction（偏差修正）</h3><p>原因：$v_0=0$导致初始阶段的点估计不准<br>解决方法：用$\frac{v_t}{1-\beta^t}$代替$v_t$</p>
<h2 id="3-Gradient-descent-with-momentum（动量梯度下降）"><a href="#3-Gradient-descent-with-momentum（动量梯度下降）" class="headerlink" title="3. Gradient descent with momentum（动量梯度下降）"></a>3. Gradient descent with momentum（动量梯度下降）</h2><p>背景问题：当目标函数的等高线为图3.1所示时，梯度下降的过程中可能会发生振荡：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%97%B6%E6%8C%AF%E8%8D%A1%E7%9A%84%E4%BE%8B%E5%AD%90.JPG"></center></p>
<p><center>图3.1 梯度下降振荡的例子></center><br>Momentum：<br>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
v_{dw}&=\beta v_{dw}+(1-\beta)dw \\
v_{db}&=\beta v_{db}+(1-\beta)db \\
w&:=w-\alpha v_{dw};\\
b&:=b-\alpha v_{db}\\
\tag{3-1}
\end{align*}</script><p>采用前面提到的指数加权平均可以使梯度的下降过程更平滑。<br>一般$\beta$取0.9就好，而且实际中一般不用修正偏差，因为迭代几步后偏差就自动减小很多了。</p>
<h2 id="4-RMSprop（Root-Mean-Square-prop，均方根传递）"><a href="#4-RMSprop（Root-Mean-Square-prop，均方根传递）" class="headerlink" title="4. RMSprop（Root Mean Square prop，均方根传递）"></a>4. RMSprop（Root Mean Square prop，均方根传递）</h2><p>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
s_{dw}&=\beta s_{dw}+(1-\beta)dw^2 \\
s_{db}&=\beta s_{db}+(1-\beta)db^2 \\
w&:=w-\alpha \frac{dw}{\sqrt{ s_{dw}}};\\
b&:=b-\alpha \frac{db}{\sqrt{ s_{db}}}\\
\tag{4-1}
\end{align*}</script><p>垂直方向除以一个较大的数，水平方向除以一个较小的数（假设b是垂直方向，w是水平方向）。为了防止分母出现零的情况，可以在分母加上一个小的$\epsilon$</p>
<h2 id="5-Adam优化算法"><a href="#5-Adam优化算法" class="headerlink" title="5. Adam优化算法"></a>5. Adam优化算法</h2><p>Adam的本质是将动量和RMSprop结合起来。<br>$v_{dw}=0,s_{dw}=0.v_{db}=0,s_{db}=0.$<br>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
v_{dw}&=\beta_1 v_{dw}+(1-\beta_1)dw \\
v_{db}&=\beta_1 v_{db}+(1-\beta_1)db \\
s_{dw}&=\beta_2 s_{dw}+(1-\beta_2)dw^2 \\
s_{db}&=\beta_2 s_{db}+(1-\beta_2)db^2 \\
V_{dw}^{corrected}&=v_{dw}/\left(1-\beta_1^t\right),V_{db}^{corrected}=v_{db}/\left(1-\beta_1^t\right)\\
S_{dw}^{corrected}&=s_{dw}/\left(1-\beta_2^t\right),S_{db}^{corrected}=s_{db}/\left(1-\beta_2^t\right)\\
w&:=w-\alpha \frac{V_{dw}^{corrected}}{\sqrt{ S_{dw}^{corrected}}};\\
b&:=b-\alpha \frac{V_{db}^{corrected}}{\sqrt{ S_{db}^{corrected}}}\\
\tag{5-1}
\end{align*}</script><p>超参数：<br>$\alpha$:人工调整<br>$\beta_1:0.9$，$(dw)$<br>$\beta_2:0.999$，$(dw^2)$<br>$\epsilon$:$10^{-8}$</p>
<h2 id="6-学习率衰减（learning-rate-decay）"><a href="#6-学习率衰减（learning-rate-decay）" class="headerlink" title="6. 学习率衰减（learning rate decay）"></a>6. 学习率衰减（learning rate decay）</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E9%9A%BE%E4%BB%A5%E7%9C%9F%E6%AD%A3%E6%94%B6%E6%95%9B%E7%9A%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png"></center></p>
<p><center>图6.1 固定学习率导致不能完全收敛的示意图</center><br>解决方法：让学习率$\alpha$逐渐下降。<br>下降的形式：</p>
<ul>
<li>$\alpha=\frac{1}{1+decay-rate\ *\ epoch-num}$</li>
<li>$\alpha=0.95^{epoch-num}\cdot\alpha_0$</li>
<li>$\alpha=\frac{k}{\sqrt epoch-num}\alpha_0$</li>
<li>…</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/12/神经网络中的正则化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/12/神经网络中的正则化/" itemprop="url">神经网络中的正则化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T21:39:58+08:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Adding regularization will often help To prevent overfitting problem (high variance problem ).  </p>
<h2 id="1-Logistic-regression"><a href="#1-Logistic-regression" class="headerlink" title="1. Logistic regression"></a>1. Logistic regression</h2><p>回忆一下训练时的优化目标函数</p>
<script type="math/tex; mode=display">
\min \limits_{w,b}J\left(w,b\right), \ \ \ \ w\in\mathbb{R}^{n_x},b\in\mathbb{R} \tag{1-1}</script><p>其中</p>
<script type="math/tex; mode=display">
J\left(w,b\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)\\  \tag{1-2}</script><p>$L_2 \ \ regularization $ (most commonly used)：  </p>
<script type="math/tex; mode=display">J\left(w,b\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)+\frac{\lambda}{2m}\left\lVert w \right\rVert_2^2\\  \tag{1-3}</script><p>其中</p>
<script type="math/tex; mode=display">\left\lVert w \right\rVert_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw\tag{1-4}</script><p>Why do we regularize just the parameter w? Because w Is usually a high dimensional parameter vector while b is A scalar. Almost all The parameters are in w rather than b.<br>$L_1 \ \ regularization $  </p>
<script type="math/tex; mode=display">
J\left(w,b\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)+\frac{\lambda}{m}\left\lvert w \right\rvert_1\tag{1-5}</script><p>其中</p>
<script type="math/tex; mode=display">
\left\lvert w \right\rvert_1=\sum_j^{n_x}\left\lvert w_j \right\rvert \tag{1-6}</script><p>w will end up being sparse. In other words the w vector will have a lot of zeros in it. This can help with compressing the model a little.  </p>
<h2 id="2-Neural-network-“Frobenius-norm”"><a href="#2-Neural-network-“Frobenius-norm”" class="headerlink" title="2. Neural network “Frobenius norm”"></a>2. Neural network “Frobenius norm”</h2><script type="math/tex; mode=display">J\left(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]}\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)+\frac{\lambda}{2m}\sum_{l=1}^{L}{\left\lVert w \right\rVert_2^2 }\tag{2-1}</script><p>其中</p>
<script type="math/tex; mode=display">
\left\lVert w^{[l]} \right\rVert_F^2=\sum_i^{n^{[l-1]}}\sum_j^{n^{[l]}}\left(w_{ij}\right)^2  \tag{2-2}</script><p>$L_2$ regulation is also called <strong>Weight decay</strong>:  </p>
<script type="math/tex; mode=display">
\begin{align*} dw^{[l]}&=\left(from\ backprop\right)+\frac{\lambda}{m}w^{[l]}\\  w^{l}:&=w^{[l]}-\alpha dw^{[l]}\\  &=\left(1-\frac{\alpha\lambda}{m}\right)w^{[l]}-\alpha(from\ backprop)\\
\tag{2-3}
\end{align*}</script><p>能够防止权重$w$过大，从而避免过拟合</p>
<h2 id="3-inverted-dropout"><a href="#3-inverted-dropout" class="headerlink" title="3. inverted dropout"></a>3. inverted dropout</h2><p>对于不同的训练样本都可以随机消除一部分结点<br><strong>反向随机失活</strong>（前向和后向都需要dropout）：</p>
<script type="math/tex; mode=display">
\begin{align*}  d^3&=np.random.rand(a_3.shape[0],a_3.shape[1]) < keep.prob\\  a^3&=np.multiply(a_3,d_3)\ \ \ \#a3*d3, element\ wise\ multiplication\\  a^3/&=keep.prob\ \ \ \#in\ order\ to\ not\ reduce\ the\ expected\ value\ of\ a^3\ \ inverted\ dropout\\  z^{[4]}&=w^{[4]}a^{[3]}+b^{[4]}\\  z^{[4]}/&=keep.prob\\
\tag{3-1}
\end{align*}</script><p>this inverted dropout technique by dividing by the keep.prob, it ensures that the expected value of a3 remains the same. This makes test time easier because you have less of a scaling problem.<br><strong>测试时不需要使用drop out</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/12/神经网络中的激活函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/12/神经网络中的激活函数/" itemprop="url">神经网络中的激活函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T21:37:51+08:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>$tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$效果严格地比$sigmoid$函数好，因为该函数的对称中心在$(0,0)$，具有将数据归一化为0均值的效果。当然，二分类的输出层的激活函数还是一般用$sigmoid(z)$，因为$sigmod$函数能将输出值映射到$0\sim1$之间（概率值）<br>$Relu(z)=max(0,z)$出现后，神经网络默认都用$Relu$函数（rectified linear）来作为激活函数。此时一般默认$z&gt;0$<br>$leaky(z)=max(0.01z,z)$可以避免$z&lt;0$时斜率为零的情况  输出层有时也用线性激活函数（房价预测）  </p>
<h2 id="1-Sigmoid-activation-function"><a href="#1-Sigmoid-activation-function" class="headerlink" title="1. Sigmoid activation function"></a>1. Sigmoid activation function</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid.png"></center></p>
<p><center>图1.1 激活函数-sigmoid</center></p>
<script type="math/tex; mode=display">
\begin{align*}  a&=g(z) \\  &=\frac{1}{1+e^{-z}}\\  \tag{1-1}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\frac{e^{-z}}{1+e^{-z}}\\  &=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)\\  &=g(z)\left(1-g(z)\right)\\  &=a(1-a)\\  \tag{1-2}  \end{align*}</script><h2 id="2-Tanh-activation-function"><a href="#2-Tanh-activation-function" class="headerlink" title="2. Tanh activation function"></a>2. Tanh activation function</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh.png"></center></p>
<p><center>图2.1 激活函数-tanh</center></p>
<script type="math/tex; mode=display">
\begin{align*}  
a&=g(z) \\  &=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\  \tag{2-1}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\frac{e^{-z}}{1+e^{-z}}\\  &=\frac{\left(e^{z}+e^{-z}\right)^2-\left(e^z-e^{-z}\right)^2}{\left(e^z+e^{-z}\right)^2}\\  &=1-\left(g(z)\right)^2\\  &=1-a^2\\  \tag{2-2}  \end{align*}</script><h2 id="3-ReLU-and-Leaky-ReLU"><a href="#3-ReLU-and-Leaky-ReLU" class="headerlink" title="3. ReLU and Leaky ReLU"></a>3. ReLU and Leaky ReLU</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/ReLU.png"></center></p>
<p><center>图3.1 激活函数-ReLU</center><br><strong>ReLU:</strong>  </p>
<script type="math/tex; mode=display">
\begin{align*}  a&=g(z) \\  &=max(0,z)\\  \tag{3-1}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\left\{  \begin{aligned}  0\quad if\ z<0\\  1\quad if\ z\geq0  \end{aligned}  \right.  \tag{3-2}  \end{align*}</script><p><strong>Leaky ReLU:</strong>  </p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Leaky-ReLU.png"></center></p>
<p><center>图3.2 激活函数-Leaky ReLU</center></p>
<script type="math/tex; mode=display">
\begin{align*}  a&=g(z) \\  &=max(0.01z,z)\\  \tag{3-3}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\left\{  \begin{aligned}  0.01\quad if\ z<0\\  1\quad if\ z\geq0  \end{aligned}  \right.  \tag{3-4}  \end{align*}</script><h2 id="4-选择激活函数的准则"><a href="#4-选择激活函数的准则" class="headerlink" title="4.选择激活函数的准则"></a>4.选择激活函数的准则</h2><ul>
<li>如果处理的问题是二分类问题，输出为0和1，那么输出层选择sigmoid函数，其他神经元选择ReLU(有时也可用tanh)，理论上Leaky ReLU比ReLU好，但是实践中差不多。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/05/从logistic回归到神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/05/从logistic回归到神经网络/" itemprop="url">从logistic回归到神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T23:59:23+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>有任何问题请联系 <a href="mailto:languoxing@126.com" target="_blank" rel="noopener">languoxing@126.com</a></p>
<p>本文是深度学习入门教程序列的第一篇，通过介绍logistic回归的原理，进而引入一层神经网络的前向传播和后向传播公式，并提供了示例代码。</p>
<h2 id="1-logistic回归详解"><a href="#1-logistic回归详解" class="headerlink" title="1.logistic回归详解"></a>1.logistic回归详解</h2><p>logistic回归模型是用来解决二分类问题的，因此我们将首先在概率的框架下描述什么是分类问题。分类问题的一般描述如下图所示：</p>
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0.png" width="500" height="150" alt="分类问题的一般描述"></center>
<center>图1.1 分类问题的一般描述</center>
用X代表输入空间，是输入向量的所有可能取值的集合，也叫特征空间；Y代表输出空间，是输出的所有可能取值的集合。$x^{(i)}\in X$代表特征空间的一个样本，$y^{(i)}\in Y$代表其类别（标签）。分类问题的一般描述可总结为：利用已知标签的训练集$X_{train}$训练出一个模型，该模型包含一个映射关系$h:X\rightarrow Y$，$h$应该能够对新的数据点$x^{(m+1)}$预测其类别$y^{(m+1)}$，并且预测结果应该尽可能好。通常情况下，这种“好”的标准为正确率尽可能高。

以二分类问题为例，用0和1代表可能的类别，也即$Y=\{0,1\}$。我们从概率的框架下来讨论二分类问题：给定输入特征向量$x$，我们希望估计出它分别属于两类的概率$P(y=0|x;\theta)$和$P(y=1|x;\theta)$。因为现在讨论的是二分类问题，可令$\hat y=P(y=1|x;\theta)$，那么只要估计出$\hat y$就可以了。在估计之前，需要选择合适形式的函数对各类别的后验概率建模，一种最简单也是最笨的方法就是令$\hat y=w ^T x+b$，也即线性回归。但是这样可能会导致$\hat y$的值大于1或者大于0，这和概率的定义相违背。因此，我们可以在线性回归的表达式前面加上一层sigmoid函数，也即$\hat y=\sigma\left(\omega ^T x+b\right)$。sigmoid函数的表达式为：
$$  \sigma (z)=\frac{1}{1+e^{-z}}\\  \tag{1-1}  $$
其函数图像为：
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/600px-Logistic-curve.png" width="300" height="200" alt="sigmoid函数图像"></center>
<center>图1.2 logistic函数图像</center>
到此，我们得到了logistic回归模型：
$$  \hat y=\frac{1}{1+e^{(-\theta^Tx)}}  \tag{1-2}  $$
注意，在$(1-2)$中，我们将参数项b用$x_0$表示，这样能够写成更紧凑的形式。为了突出参数$\theta$，我们用$h_\theta$表示公式$(2)$中的函数，它代表了在已知特征$x$的情况下，类别为$y=1$的概率，也即$P(y=1|x;\theta)=h_\theta$；显然,$P(y=0|x;\theta)=1-h_\theta$
假设我们的目标是让分类的错误率最小（即最小错误率决策准则，这是最普遍的一种分类准则）。不难证明，最小化错误等价于最大化各类的后验概率。因此，若$(1-2)$的值大于0.5，则判定为类别1；否则判定为类别0。

注意，logistic模型仍然是一个线性分类模型，因为它的决策面是$0=w ^T x+b$，我一个线性决策面。
## 2.损失函数的选取
选取好了模型后，接下来要选取损失函数（Loss function），然后在训练集上利用一定的算法（例如梯度下降法）最小化损失函数，从而确定$h_{\theta}$中的参数$\theta$。一种很自然的想法是选取$L\left(\hat y,y\right)=\frac{1}{2}\left(\hat y-y\right)^2$作为损失函数，但是这会导致在后面学习参数的过程中，最优问题不是一个凸问题。我们在这里采用极大似然估计的方法来推导出一个更合理的损失函数，并且该损失函数是凸函数。
### 2.1.最大化后验概率与极大似然估计
回忆一下极大似然估计的思想：对于可观测的样本$X$及其观测值$Y$，写出该观测值的概率表达式（记为$L\left(\theta\right)$)，该概率表达式一般依赖于参数$\theta$，极大似然估计的目标是寻找$\theta$的估计值$\hat \theta$使得$L\left(\theta\right)$最大。
对于某一个观测样本$x^{(i)}$和观测值$y^{(i)}$,有：
$$
\begin{align*}
L(\theta)&=P(y^{(i)}|x;\theta)\\
&=(h_\theta(x))^{1\left(y^{(i)}=1\right)}(1-h_\theta(x))^{1\left(y^{(i)}=0\right)}\\
&=(h_\theta(x))^y(1-h_\theta(x))^{1-y}\\
\tag{2-1}  
\end{align*}
$$
$L(\theta)$也叫似然函数。
对数似然函数为：
$$
\begin{align*}  l(\theta) &=\log L(\theta)  \\  
&=y\log\left(h_\theta(x)\right)+(1-y)\log  (1-h_\theta(x))  \tag{2-2}  \end{align*}
$$
$(2-2)$就是单个样本的损失函数。下面讨论训练集上的代价函数（cost function）。
对于多个观测样本$X$和观测值$Y$，似然函数可写成：
$$
\begin{align*}  L(\theta) &=P(\boldsymbol{Y}|\boldsymbol{X};\theta)  \\  &=\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta)  \\  &=\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\\  \tag{2-3}  
\end{align*}
$$
对数似然函数为：
$$
\begin{align*}  l(\theta) &=\log L(\theta)  \\  &=\sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log  (1-h_\theta(x^{(i)}))\\  \tag{2-4}  \end{align*}
$$
因为对数似然函数需要最大化，而损失函数需要最小化，因此我们选择如下表达式作为损失函数：
$$  J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log  (1-h_\theta(x^{(i)}))\right]  \tag{2-5}  
$$
$(2-5)$就是代价函数（cost function），它等价于对所有的$(2-2)$取平均。
损失函数（loss function）是定义在单个样本上的，代价函数（cost function）是定义在多个样本上的。

## 3.梯度下降方法求解最优的参数$w$和$b$（一层神经网络）
利用logistic可以构造一个只含有输出层的神经网络，其结构图如下：
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/single_layer_NN.png" alt="single layer NN"></center>
<center>图3.1 用logistic回归表示的单层神经网络结构图</center>

<p>下面介绍其作为神经网络时的前向传播、反向传播过程，为后面理解更复杂的神经网络打下基础。<br>logistic回归的代价函数的凸函数，因此可以用梯度下降的方法求得全局最优值，并且与初始化的方式无关，一般利用零初始化。</p>
<script type="math/tex; mode=display">
\begin{align*}
w&:=w-\alpha\frac{\partial J(w,b)}{\partial w}\\
b&:=b-\alpha\frac{\partial J(b,b)}{\partial b}\\
\tag{3-1}
\end{align*}</script><p>注意，Andrew NG在Cousera开设的深度学习课程中，求偏导用的是符号$d$，我们这里不区分$\partial$和$d$。<br>下面我们讨论只有输入层和输出层（激活函数为sigmoid函数）的简单神经网络的前向传播和反向传播过程，也即logistic回归。</p>
<h3 id="3-1-前向传播"><a href="#3-1-前向传播" class="headerlink" title="3.1.前向传播"></a>3.1.前向传播</h3><script type="math/tex; mode=display">
\begin{align*}  X&=\left[x^{(1)},x^{(2)},\cdots,x^{(m)}\right]\\  Z&=\left[z^{(1)},z^{(2)},\cdots,z^{(m)}\right]\\  
&=[w^Tx^{(1)}+b,w^Tx^{(2)}+b,\cdots,w^Tx^{(m)}+b]\\  A&=\left[a^{(1)},a^{(2)},\cdots,a^{(m)}\right]\\
&=\left[\sigma\left(a^{(1)}\right),\sigma\left(a^{(2)}\right),\cdots,\sigma\left(a^{(m)}\right)\right]\\
Y&=\left[y^{(1)},y^{(2)},\cdots,y^{(m)}\right]\\    \tag{3-2}  \end{align*}</script><p>$(3-2)$中从上到下的顺序可以代表前向传播过程。</p>
<h3 id="3-2-反向传播"><a href="#3-2-反向传播" class="headerlink" title="3.2.反向传播"></a>3.2.反向传播</h3><p>先对求导公式进行一些化简：</p>
<script type="math/tex; mode=display">
\begin{align*}  
\rm d\it a  &= \frac{\rm{d}\it{L(a,y)}}{\rm{d}\it{a}}\\  
&= -\frac{y}{a}+\frac{1-y}{1-a}\\  
\tag{3-3}  
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  
\rm d\it z&= \frac{\rm{d}\it{L(a,y)}}{\rm{d}\it{z}}\\  
&= \frac{\rm{d}\it{L(a,y)}}{\rm{d}\it{a}}\cdot  
\frac{\rm d \it a}{\rm d \it z}\\  &= \left(-\frac{y}{a}+\frac{1-y}{1-a}\right)\cdot a(1-a)\\  
&=a-y \\  
\tag{3-4}  
\end{align*}</script><p>有了$(3-4)$，我们就能得到$\rm{d}\it{Z}=A-Y$。因此反向传播过程如下：</p>
<script type="math/tex; mode=display">
\begin{align*} 
\rm{d}\it{Z}&=A-Y\\
\rm d\it w&= \frac{1}{m}\cdot X\cdot \rm{d}\it{Z}^T\\  &=\frac{1}{m}X\left(A-Y\right)^T \\  \rm d\it b&= \frac{1}{m}\cdot\sum_{i=1}^{m}\left(\rm{d}\it{Z^{(i)}}\right) \\  \tag{3-5}  \end{align*}</script><p>注意，$(3-5)$的流程可以理解为先对每一个样本求损失函数关于$z$的梯度，并对每个样本求出$\rm{d}\it{w}$，在对所有样本的$\rm{d}\it{w}$球平均，因此有$1/m$。</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/logistic%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.JPG" alt="logistic回归梯度下降"></center></p>
<p><center>图3.2 logistic回归梯度下降</center><br>也可以对$m$个样本的代价函数一次性直接求偏导，这需要一定的向量微分和复合函数微分的知识。这样的话，在$\rm{d}\it{Z}$中就会有一个$1/m$，那么在求$\rm{d}\it{w}$时就不用另外再加一个$1/m$了，并且形状可能互为转置，最终的结果是一样的。具体过程这里省略，有兴趣的读者可以自行推导。</p>
<h2 id="4-单层神经网络示例代码"><a href="#4-单层神经网络示例代码" class="headerlink" title="4.单层神经网络示例代码"></a>4.单层神经网络示例代码</h2><p>网络结构见图3.1<br>请点击下面的超链接跳转至github页面：<br><a href="https://github.com/lankuohsing/Coursera-Deep-Learning-Specialization/tree/master/Neural%20Networs%20and%20Deep%20Learning" target="_blank" rel="noopener">用logistic回归实现一层神经网络用来识别猫的图片</a><br>其中Logistic_Regression_with_a_Neural_Network_mindset.ipynb文件包含详细的注释和图示，但需要在jupyternotebook中运行；Logistic_Regression_with_a_Neural_Network_mindset.py文件可直接用“python”命令运行，两者的功能是一样的。</p>
<h2 id="5-两层神经网络"><a href="#5-两层神经网络" class="headerlink" title="5.两层神经网络"></a>5.两层神经网络</h2><h3 id="5-1-前向传播"><a href="#5-1-前向传播" class="headerlink" title="5.1.前向传播"></a>5.1.前向传播</h3><script type="math/tex; mode=display">
\begin{align*}  X&=\left[x^{(1)},x^{(2)},\cdots,x^{(m)}\right]\\  
W^{[1]}&=\left[w_1^{[1]},w_2^{[1]},\cdots,w_{n_1}^{[1]}\right]^{T}\\
b^{[1]}&=\left[b_1^{[1]},b_2^{[1]},\cdots,b_{n_1}^{[1]}\right]\\
Z^{[1]}&=W^{[1]}X+b^{[1]}\\
A^{[1]}&=g^{[1]}\left(Z^{[1]}\right)\\
W^{[2]}&=\left[w_1^{[2]},w_2^{[2]},\cdots,w_{n_2}^{[2]}\right]^{T}\\
b^{[2]}&=\left[b_1^{[2]},b_2^{[2]},\cdots,b_{n_2}^{[2]}\right]\\
Z^{[2]}&=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}&=g^{[2]}\left(Z^{[2]}\right)\\
Y&=\left[y^{(1)},y^{(2)},\cdots,y^{(m)}\right]\\    \tag{5-1}  \end{align*}</script><p>$(5-1)$中从上到下的顺序可以代表前向传播过程。</p>
<h3 id="5-2-反向传播"><a href="#5-2-反向传播" class="headerlink" title="5.2.反向传播"></a>5.2.反向传播</h3><script type="math/tex; mode=display">
\begin{align*} 
{\rm d}Z^{[2]}&=A^{[2]}-Y\\
{\rm d}W^{[2]}&=\frac{1}{m}{\rm d}Z^{[2]}A^{[1]\ T}\\  
{\rm d}b^{[2]}&=\frac{1}{m}np.sum\left({\rm d}Z^{[2]},axis=1,keepdims=True\right)\\  
{\rm d}A^{[1]}&={W^{[2]}}^T{\rm d}Z^{[2]}\\
{\rm d}Z^{[1]}&={\rm d}A^{[1]}*{g^{[1]}}'\left(Z^{[1]}\right)\\
{\rm d}W^{[1]}&=\frac{1}{m}{\rm d}Z^{[1]}A^{[0]\ T}\\  
{\rm d}b^{[1]}&=\frac{1}{m}np.sum\left({\rm d}Z^{[1]},axis=1,keepdims=True\right)\\  \tag{5-2}
\end{align*}</script><p>$(5-2)$中从上到下的顺序可以代表前向传播过程。</p>
<h3 id="5-3-关于参数的初始化"><a href="#5-3-关于参数的初始化" class="headerlink" title="5.3.关于参数的初始化"></a>5.3.关于参数的初始化</h3><p>在单层神经网络（logistic回归）中，可以将参数初始化wie全零值；但是在多层神经网络中，$W$不能初始化为0，否则每一层的各个节点对应的$W$会训练成一样的值；$b$可以初始化为0  $W$不能初始化为特别大，浅层网络一般是0.01量级；因为太大了的话，激活函数的自变量要么正向很大，要么负向很大，导致斜率趋近于0 ，收敛速度很慢。尤其是你使用$sigmoid$或$tanh$激活函数。如果不使用这些激活函数，但是你是二分类问题（输出层是sigmoid函数），也会出现这个问题.</p>
<h3 id="5-4-2层神经网络示例代码"><a href="#5-4-2层神经网络示例代码" class="headerlink" title="5.4.2层神经网络示例代码"></a>5.4.2层神经网络示例代码</h3><p>对如下二维平面上的点进行分类：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/planar.png"></center></p>
<p><center>图5.1 二维平面上的非线性二分类数据集例子</center><br>网络结构如下：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2_layer_NN.png"></center></p>
<p><center>图5.2 2层神经网络结构例子</center><br>点击下面的超链接查看完整代码：<br><a href="https://github.com/lankuohsing/Coursera-Deep-Learning-Specialization/tree/master/Neural%20Networs%20and%20Deep%20Learning" target="_blank" rel="noopener">2层神经网络的示例代码</a><br>其中Palnar_data_clf_with_one_hidden_layer.ipynb文件包含详细的注释和图示，但需要在jupyternotebook中运行；Palnar_data_clf_with_one_hidden_layer.py文件可直接用“python”命令运行，两者的功能是一样的。</p>
<h2 id="6-深层神经网络"><a href="#6-深层神经网络" class="headerlink" title="6.深层神经网络"></a>6.深层神经网络</h2><h3 id="6-1-每层的参数及变量的尺寸"><a href="#6-1-每层的参数及变量的尺寸" class="headerlink" title="6.1.每层的参数及变量的尺寸"></a>6.1.每层的参数及变量的尺寸</h3><script type="math/tex; mode=display">\begin{align*}  z^{[l]}&:\left(n^{[l]},1\right)\\  a^{[l]}&:\left(n^{[l]},1\right)\\  W^{[l]}&:\left(n^{[l]},n^{[l-1]}\right)\\  b^{[l]}&:\left(n^{[l]},1\right)\\  dW^{[l]}&:\left(n^{[l]},n^{[l-1]}\right)\\  db^{[l]}&:\left(n^{[l]},1\right)\\  \tag{6-1}
\end{align*}</script><h3 id="6-2-前向传播递推公式"><a href="#6-2-前向传播递推公式" class="headerlink" title="6.2.前向传播递推公式"></a>6.2.前向传播递推公式</h3><script type="math/tex; mode=display">
\begin{align*}  X&=\left[x^{(1)},x^{(2)},\cdots,x^{(m)}\right]\\  
W^{[l]}&=\left[w_1^{[l]},w_2^{[l]},\cdots,w_{n_l}^{[l]}\right]^{T}\\
b^{[l]}&=\left[b_1^{[l]},b_2^{[l]},\cdots,b_{n_l}^{[l]}\right]\\
Z^{[l]}&=W^{[l]}A^{[l-1]}+b^{[l]}\\
A^{[l]}&=g^{[l]}\left(Z^{[l]}\right)\\
W^{[l+1]}&=\left[w_1^{[l+1]},w_2^{[l+1]},\cdots,w_{n_l+1}^{[l+1]}\right]^{T}\\
b^{[l+1]}&=\left[b_1^{[l+1]},b_2^{[l+1]},\cdots,b_{n_2}^{[l+1]}\right]\\
Z^{[l+1]}&=W^{[l+1]}A^{[l]}+b^{[l+1]}\\
A^{[l+1]}&=g^{[l+1]}\left(Z^{[l+1]}\right)\\
Y&=\left[y^{(1)},y^{(2)},\cdots,y^{(m)}\right]\\    \tag{6-2}  \end{align*}</script><h3 id="6-3-反向传播递推公式"><a href="#6-3-反向传播递推公式" class="headerlink" title="6.3.反向传播递推公式"></a>6.3.反向传播递推公式</h3><script type="math/tex; mode=display">
\begin{align*} {\rm d}Z^{[l]}&={\rm d}A^{[l]}*{g^{[l]}}'\left(Z^{[l]}\right)\\  
{\rm d}W^{[l]}&=\frac{1}{m}{\rm d}Z^{[l]}A^{[l-1]\ T}\\  
{\rm d}b^{[l]}&=\frac{1}{m}np.sum\left({\rm d}Z^{[l]},axis=1,keepdims=True\right)\\  
{\rm d}A^{[l-1]}&={W^{[l]}}^T{\rm d}Z^{[l]}\\  
\tag{6-3}
\end{align*}</script><h3 id="6-4-多层神经网络的示例代码"><a href="#6-4-多层神经网络的示例代码" class="headerlink" title="6.4.多层神经网络的示例代码"></a>6.4.多层神经网络的示例代码</h3><p>点击如下超链接。<br><a href="https://github.com/lankuohsing/Coursera-Deep-Learning-Specialization/tree/master/Neural%20Networs%20and%20Deep%20Learning" target="_blank" rel="noopener">对比了2层神经网络和5层神经网络在对猫分类时的性能</a><br>Deep Neural Network - Application.py<br>Deep Neural Network - Application.ipynb</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/01/27/Hello-World/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/01/27/Hello-World/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-27T15:11:17+08:00">
                2018-01-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/About-Me/" itemprop="url" rel="index">
                    <span itemprop="name">About Me</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Hi! I am Guoxing Lan(兰国兴)! I’m a master student majoring in Control Science and Engineering in Tsinghua University, Beijing, China. My interests are in theories and applications of Machine Learning and Deep Learning. My research work during my Masters Program is focused on Data-driven Fault Diagnosis and Remaining Useful Estimation of Aircraft Engine. I am going to record and share my journey of studying Artificial Intelligence here. You are wlso welcome to  visit my other Blog on CSDN—<a href="https://blog.csdn.net/thuchina" target="_blank" rel="noopener"><strong>lankuohsing的博客</strong></a> to find things not only restrict to AI. Click <a href="https://github.com/lankuohsing" target="_blank" rel="noopener"><strong>here</strong></a> to visit my GitHub! I hope we can enjoy the joy of sharing knowledge !</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="images/avatar.jpg"
                alt="Guoxing Lan" />
            
              <p class="site-author-name" itemprop="name">Guoxing Lan</p>
              <p class="site-description motion-element" itemprop="description">On The Journey To Truth</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://lankuohsing.github.io/blog/archives">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async="" src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
  <i class="fa fa-user-md"></i>
  <span id="busuanzi_container_site_uv">
    visitor count:<span id="busuanzi_value_site_uv"></span>
  </span>
</div>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guoxing Lan</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
