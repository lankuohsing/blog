<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="On The Journey To Truth">
<meta property="og:type" content="website">
<meta property="og:title" content="Guoxing Lan">
<meta property="og:url" content="https://lankuohsing.github.io/blog/index.html">
<meta property="og:site_name" content="Guoxing Lan">
<meta property="og:description" content="On The Journey To Truth">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Guoxing Lan">
<meta name="twitter:description" content="On The Journey To Truth">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lankuohsing.github.io/blog/"/>





  <title>Guoxing Lan</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
	<a href="https://github.com/lankuohsing"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Guoxing Lan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Journey To Truth</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://lankuohsing.github.io/blog" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://lankuohsing.github.io/blog/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://lankuohsing.github.io/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://lankuohsing.github.io/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://lankuohsing.github.io/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2021/02/03/TensorFlow-PyTorch中张量-Tensor-的底层存储方式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2021/02/03/TensorFlow-PyTorch中张量-Tensor-的底层存储方式/" itemprop="url">TensorFlow/PyTorch中张量(Tensor)的底层存储方式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-02-03T00:39:25+08:00">
                2021-02-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>@[toc]</p>
<h1 id="0-张量（Tensor）基本概念回顾"><a href="#0-张量（Tensor）基本概念回顾" class="headerlink" title="0. 张量（Tensor）基本概念回顾"></a>0. 张量（Tensor）基本概念回顾</h1><p>张量（Tensor）其实就是多维数组，类似于NumPy里面的np.array。<br>这里的维度，更准确的讲法应该叫阶（rank），这是为了跟向量（vector）的维度区分开的。vector其实就是rank为1的张量，我们说一个vector是n维的其实是说它有n个分量（标量）。而如果张量的维度（阶）是n维的，并不是说它有n个标量分量，而是说在表示这个张量时需要用n个坐标轴。每个轴上都可以有多个分量。为了表示每个轴上的分量个数，引入形状（shape）的概念。<br>例如，一个三阶的张量，可以理解为是一个立方体。假设它的shape是$[3,2,5]$,那么下图就是一个具体的例子（下面两张图的来源均为<a href="https://www.tensorflow.org/guide/tensor" target="_blank" rel="noopener">https://www.tensorflow.org/guide/tensor</a>）：<br><img src="https://img-blog.csdnimg.cn/20210203231202217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图0.1 一个rank为3的tensor的例子</center>

<p>而一个四阶张量的例子则用下图表示：<br><img src="https://img-blog.csdnimg.cn/20210203231212171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图0.2 一个rank为4的tensor的例子</center>

<p>TensorFlow/PyTorch中使用比较多的tensor的阶为4，shape为$[Batch, Height, Weight, Features]$.</p>
<p>n阶张量的排列规律如下图所示（图片来源<a href="https://syborg.dev/posts/understanding-tensors-in-pytorch" target="_blank" rel="noopener">https://syborg.dev/posts/understanding-tensors-in-pytorch</a>）：<br><img src="https://img-blog.csdnimg.cn/20210203231222692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图0.3 1-6阶tensor的shape规律</center>

<p>可以将规律总结为：从shape列表的最右边往左遍历，最开始三个阶按照“下-右-里”的顺序排列，然后打包成一个group，再将整个group按照“下-右-里”的顺序排列，满三次后再打包成一个group，如此往复循环。。</p>
<h1 id="1-tensor在计算机内存中的存储方式"><a href="#1-tensor在计算机内存中的存储方式" class="headerlink" title="1. tensor在计算机内存中的存储方式"></a>1. tensor在计算机内存中的存储方式</h1><p>前面提到的rank和shape，都是数学上的定义，实际在内存中是一维存储的。<br>排列规律为：</p>
<ol>
<li>假设rank为n，即shape列表的size为n.</li>
<li>初始位置为原点</li>
<li>将shape[n-1]方向上的元素按照这个方向上的坐标递增的顺序进行线性排列。</li>
<li>shape[n-1]方向上的元素排完后再将shape[n-2]上的坐标递增，继续排shape[n-1]方向上的元素，直到shape[n-2]方向上的坐标到了最大值，并且排完了这个坐标上的shape[n-1]方向的元素，此时shape[n-2]方向上坐标归零。</li>
<li>将shape[n-3]上的坐标递增，继续3,4，直到shape[n-3]方向上的坐标和shape[n-2]方向上的坐标到了最大值，并且排完了这个坐标上的shape[n-1]方向的元素，此时shape[n-3]和shape[n-2]方向上坐标归零</li>
<li>对shape[n-i] (i=4,5,…,n)上的坐标递增,并重复前面的过程。</li>
</ol>
<p>一般情况下，假设高阶tensor的下标为coordinates, 则转换为一维向量的下标index过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">index = coordinates[0];</span><br><span class="line">for(int i=0; i&lt;coordinates.size(); i++)&#123;</span><br><span class="line">index = index * shape[i] + coordinates[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/10/21/凸优化基础知识笔记-凸集、凸函数、凸优化问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/10/21/凸优化基础知识笔记-凸集、凸函数、凸优化问题/" itemprop="url">凸优化基础知识笔记-凸集、凸函数、凸优化问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-21T23:02:07+08:00">
                2020-10-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Mathematical-Fundamentals/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematical Fundamentals</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>[TOC]</p>
<h1 id="1-凸集"><a href="#1-凸集" class="headerlink" title="1. 凸集"></a>1. 凸集</h1><p>集合$C$被称为凸集，如果C中任意两点间的线段仍然在$C$中。即对于任意$x_1,x_2\in C$和满足$0\leq \theta \leq 1$的$\theta$都有</p>
<script type="math/tex; mode=display">
\theta x_1+(1-\theta)x_1\in C\\
\tag{1-1}</script><h1 id="2-凸函数"><a href="#2-凸函数" class="headerlink" title="2. 凸函数"></a>2. 凸函数</h1><p>凸函数的原始定义：</p>
<blockquote>
<p>函数$f:{\rm{R}}^n\rightarrow{\rm{R}}$是凸的，如果${\rm dom}\ f$是凸集，且对于任意$x,y\in {\rm dom}\ f$和任意$0\leq \theta\leq 1$，有</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta)y)\leq \theta f(x)+(1-\theta)f(y)\tag{2-1}</script></blockquote>
<p>严格凸：上式中当$x\not=y$且$0\leq \theta \leq 1$时，不等式严格成立（即取小于号）<br><strong>几何意义</strong>：上述不等式意味着点$(x,f(x))$和$(y,f(y))$之间的线段在函数$f$的图像上方。</p>
<h2 id="2-1-凸函数的一阶条件"><a href="#2-1-凸函数的一阶条件" class="headerlink" title="2.1. 凸函数的一阶条件"></a>2.1. 凸函数的一阶条件</h2><blockquote>
<p>假设$f$可微（即其梯度$\nabla f$在开集${\rm dom}\ f$内处处存在），则函数$f$是凸函数的充要条件是${\rm dom}\ f$是凸集且对于任意$x,y\in {\rm dom}\ f$，下式成立：</p>
<script type="math/tex; mode=display">
f(y)\geq f(x)+\nabla f(x)^T(y-x)\tag{2-2}</script></blockquote>
<p><strong>几何意义</strong>：凸函数的一阶Taylor近似是原函数的一个全局下估计，也即凸函数任意一点处的切线都在原函数图像的下方。反之亦然（充分必要条件）<br>2.2. 凸函数的二阶条件</p>
<blockquote>
<p>假设$f$二阶可微，即对于开集${\rm dom}\ f$内的任意一点，它的Hessian矩阵或者二阶导数$\nabla ^2f$存在，则函数$f$是凸函数的充要条件是其Hessian矩阵是半正定阵：即对于所有的$x\in {\rm dom}\ f$有：</p>
<script type="math/tex; mode=display">
\nabla^2f(x)\succeq 0\tag{2-3}</script></blockquote>
<p><strong>几何意义</strong>：函数图像在点$x$处具有正（向上）的曲率。</p>
<h2 id="2-1-凸函数例子"><a href="#2-1-凸函数例子" class="headerlink" title="2.1. 凸函数例子"></a>2.1. 凸函数例子</h2><p><strong>常见的凸函数：</strong></p>
<ul>
<li><strong>指数函数</strong>：$e^{ax},\forall a \in R$</li>
<li><strong>范数: $\lVert x\rVert_p=\left(\lvert x_1\rvert^p+\lvert x_2\rvert^p+\cdots+\lvert x_n\rvert^p\right)^{1/p},p\geq 1$</strong>。${\rm R}^n上的任意范数均为凸函数$。</li>
<li><strong>负熵函数</strong>：函数$xlog{x}$在其定义域（$R_{++}或者R_X$）上是凸函数。</li>
</ul>
<h1 id="3-凸优化问题"><a href="#3-凸优化问题" class="headerlink" title="3. 凸优化问题"></a>3. 凸优化问题</h1><p>优化问题的<strong>标准形式</strong>：</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ \ &f_0(x)\\
s.t.\ \ &f_i(x)\leq 0,i=1,2,\cdots,m\\
&h_i(x)=0,i=1,2,\cdots,p\\
\tag{3-1}
\end{align*}</script><p>我们称$x\in R^n$为<strong>优化变量</strong>，称函数$f_0:R^n\rightarrow R$为为<strong>目标函数</strong>或代价函数；不等式$f_i(x)\leq 0$称为不等式约束，$h_i:R^n\rightarrow R$称为等式约束。优化问题的定义域是目标函数和约束函数的定义域的交集。满足约束条件的定义域中的点称为可行点；所有可行点的集合称为可行集。<br>问题$(3-1)$的最优值$p^{\star}$定义为:</p>
<script type="math/tex; mode=display">
\begin{align*}
p=\inf\{&f_0(x)|\\
&f_i(x)\leq 0,i=1,2,\cdots,m,h_i(x)=0,i=1,2,\cdots,p\}\\
\tag{3-2}
\end{align*}</script><p>如果问题不可行，则$p^{\star}=\infty$</p>
<p><strong>凸优化问题</strong>的标准形式</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ \ &f_0(x)\\
s.t.\ \ &f_i(x)\leq 0,i=1,2,\cdots,m\\
&a_i^Tx=b_i,i=1,2,\cdots,p\\
\tag{3-3}
\end{align*}</script><p>其中，$f_0,f_1,\cdots,f_m$是凸函数<br>凸优化问题与一般优化问题的标准形式的区别在于以下三点：</p>
<ul>
<li>目标函数必须是凸的</li>
<li>不等式约束函数必须是凸的</li>
<li>等式约束函数必须是仿射函数</li>
</ul>
<p>至于为什么等式约束必须是仿射函数，这里有个直观的解释：等式约束可以看成要同时满足$h_i(x)\leq 0$和$-h_i(x)\leq 0$,为了满足不等式约束的条件，要求$h_i(x)$同时是凸函数和凹函数，这样的函数只能是仿射函数。</p>
<p>凸优化问题有一个很好的性质：任意局部最优解也是全局最优解。<br>对于无约束条件的凸优化问题，$x$是其最优解的充要条件是：</p>
<script type="math/tex; mode=display">
\nabla f_0 (x)=0 \\
\tag{3-2}</script><h1 id="4-对偶"><a href="#4-对偶" class="headerlink" title="4. 对偶"></a>4. 对偶</h1><h2 id="4-1-Lagrange函数与Lagrange对偶"><a href="#4-1-Lagrange函数与Lagrange对偶" class="headerlink" title="4.1. Lagrange函数与Lagrange对偶"></a>4.1. Lagrange函数与Lagrange对偶</h2><p>回到前面提到的标准形式的优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ \ &f_0(x)\\
s.t.\ \ &f_i(x)\leq 0,i=1,2,\cdots,m\\
&h_i(x)=0,i=1,2,\cdots,p\\
\tag{4-1}
\end{align*}</script><p>注意，这里没有要求是凸优化问题。<br><strong>Lagrange对偶</strong>的基本思想是，在目标函数中考虑$(4-1)$的约束条件，即添加约束条件的加权和，得到增广的目标函数，称之为<strong>Lagrange函数：</strong></p>
<script type="math/tex; mode=display">
L(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^{m}{\lambda _if_i(x)} + \sum_{i=1}^{p}{\nu _ih_i(x)}\\
\tag{4-2}</script><p>注意，Lagrange函数的定义域是$D\times R^m\times R^p$，在后面的讨论中，我们会假设$\lambda_i\geq 0$<br>向量$\lambda$和$\nu$成为对偶变量，或者是问题$(4-1)$的Lagrange乘子向量。<br>Lagrange对偶函数定义为Lagrange函数关于x取得的最小值：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda,\nu)&=\mathop{inf}\limits_{x\in D}L(x,\lambda,\nu)\\
&= \mathop{inf}\limits_{x\in D}\left(f_0(x)+\sum_{i=1}^{m}{\lambda _if_i(x)} + \sum_{i=1}^{p}{\nu _ih_i(x)}\right)\\
\tag{4-3}
\end{align*}</script><p>Lagrange对偶函数是Lagrange函数的<strong>逐点下确界有</strong>，有个很重要的性质：无论原问题是不是凸的，Lagrange对偶函数都是凹函数。下面分别从理论上进行证明，以及从几何上形象地解释。<br><strong>理论证明</strong>：<br>不难看出，$g(\lambda,\nu)$是关于$\lambda,\nu$的仿射函数，为了书写简简洁，我们用一个长的向量$\mu$代表$(\lambda,\nu)$<br>要想证明$g(\lambda,\nu)$是凹函数，只需证明$\forall \mu_1,\mu_2$下式都成立：</p>
<script type="math/tex; mode=display">
g(\theta \mu_1+(1-\theta)\mu_2)\geq \theta g(\mu_1)+(1-\theta)g(\mu_2)\\
\tag{4-4}</script><p>下面是证明过程：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\theta \mu_1+(1-\theta)\mu_2)&=\mathop{min}\limits_{x}L(x,\theta \mu_1+(1-\theta)\mu_2)\\
&=\mathop{min}\limits_{x}\left(\theta L(x, \mu_1)+(1-\theta)L(x, \mu_2)\right)\\
&\geq \mathop{min}\limits_{x}\left(\theta L(x, \mu_1)\right)+\mathop{min}\limits_{x}\left((1-\theta)L(x, \mu_2)\right)\\
&=\theta\mathop{min}\limits_{x}\left( L(x, \mu_1)\right)+(1-\theta)\mathop{min}\limits_{x}\left(L(x, \mu_2)\right)\\
&=\theta g(\mu_1)+(1-\theta)g(\mu_2)\\
\tag{4-5}
\end{align*}</script><p>得证！<br>注意，第一步到第二步是因为$L(x,\mu)$是关于$u$的仿射函数；第二步到第三步是因为，地二步中取得最小值时，括号中两项中的$x$是取相同的值的，而第三步中两项分别取最小值不要求$x$一定取相同值（也即能够比第二步涵盖更多情况），因此第三步可能取到的最小值肯定小于或等于第二步的最小值。<br><strong>几何解释</strong>如下：<br>由于$L(x,\mu)$是关于$u$的仿射函数，我们将$\mu$退化为1维来形象地解释。$L(x,\mu)$可以看成是许多的直线簇组成。$g(x,\mu)$可以理解成：当$\mu$取某一个值时，取曲线簇在这个值上的最小值，遍历所有$\mu$，将曲线簇的一些最小值作为$g(x,\mu)$的值域。因此，$g(x,\mu)$可以看成下图中黄色区域的边界线，显然是一个凹函数。<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/%E5%87%B8%E4%BC%98%E5%8C%96/%E9%80%90%E7%82%B9%E4%B8%8B%E7%A1%AE%E7%95%8C%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A.JPG" alt="逐点下确界几何解释"></p>
<p>此外，Lagrange对偶函数还有如下性质：<br>$\forall \lambda \succeq 0$(每一维都大于0)和$\nu$，都有</p>
<script type="math/tex; mode=display">
g(\lambda,\nu)\leq p^{\star}\\
\tag{4-6}</script><p>其中$p^{\star}$是原问题的最优值。也即，对偶函数构成了原问题的最优值的下界。</p>
<h2 id="4-2-共轭函数"><a href="#4-2-共轭函数" class="headerlink" title="4.2. 共轭函数"></a>4.2. 共轭函数</h2><p>设函数$f:R^n\rightarrow R$，定义$f^{\star}:R^n\rightarrow R$为：</p>
<script type="math/tex; mode=display">
f^{\star}(y)=\mathop{sup}\limits_{x\in dom\ f}\left(y^Tx-f(x)\right)\\
\tag{4-7}</script><p>此函数成为函数$f$的共轭函数。共轭函数是一系列仿射函数的逐点上确界，所以必然是一个凸函数。<br>对于负熵函数$xlog{x}$，它的共轭函数不难推导出是$f^{\star}(y)=e^{y-1}$,这在后面会用到</p>
<h2 id="4-3-Lagrange对偶问题"><a href="#4-3-Lagrange对偶问题" class="headerlink" title="4.3. Lagrange对偶问题"></a>4.3. Lagrange对偶问题</h2><p>由$(4-6)$可以看到，对于任意一组$(\lambda,\nu)$，其中$\lambda \succeq0$,Lagrange对偶函数给出了优化问题$(4-1)$的最优值$p^{\star}$的一个下界。我们来看一下从Lagrange函数得到的<strong>最好下界</strong>。该问题可以表述为如下优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
maximize\ \ g(\lambda,\nu)\\
subject\ to\ \ \lambda\succeq 0
\tag{4-8}
\end{align*}</script><p>上述问题被称为<strong>原问题</strong>的<strong>Lagrange对偶问题</strong>。<br>满足$\lambda \succeq 0$和$g(\lambda,\nu)&gt;0$的一组$(\lambda,\nu)$被称为一组<strong>对偶可行解</strong>。如果一组$(\lambda^{\star},\nu^{\star})$是对偶问题的最优解，那么称它是<strong>对偶最优解</strong>或者<strong>最优Lagrange乘子</strong>。<br>由于$g(\lambda,\nu)&gt;0$必然是凹函数，且约束条件是凸函数，所以问题$(4-8)$必然是一个凸优化问题。<br>因此<strong>Lagrange对偶问题是一个凸优化问题，与原问题的凸性无关</strong></p>
<p>记Lagrange对偶问题的最优值为$d^{\star}$，原问题的最优值为$p^{\star}$。显然有$d^{\star}\leq p^{\star}$，这个性质称为<strong>弱对偶性</strong>。</p>
<h2 id="4-4-强对偶性与Slater约束准则"><a href="#4-4-强对偶性与Slater约束准则" class="headerlink" title="4.4. 强对偶性与Slater约束准则"></a>4.4. 强对偶性与Slater约束准则</h2><p>如果前面的有$d^{\star}=p^{\star}$，则<strong>强对偶性</strong>成立。<br>强对偶性成立的一个简单的约束条件是：存在一点$x\in relint\ D$使得下式成立：</p>
<script type="math/tex; mode=display">
f_i(x)< 0,i1,\cdots,m,Ax=b\\
\tag{4-9}</script><p>如果不等式约束函数中有一些是仿射函数时，Slater条件可以进一步改进为：不是仿射函数的那些不等式约束函数需要满足$(4-9)$。换言之，仿射不等式不需要严格成立。<br>由此可以得到一个推论：当所有约束条件是线性等式或线性不等式且$dom\ f_0$是开集时，改进的Slater条件就是可行性条件。也即只要问题是可行的，强对偶性就成立。<br>Boyd的《Convex Optimization》一书中，证明了<strong>当原问题是凸问题且Slater条件成立时，强对偶性成立。</strong></p>
<h2 id="4-5-最优性条件"><a href="#4-5-最优性条件" class="headerlink" title="4.5. 最优性条件"></a>4.5. 最优性条件</h2><p>注意，此小节讨论的问题并不要求是凸问题。</p>
<h3 id="4-5-1-互补松弛性"><a href="#4-5-1-互补松弛性" class="headerlink" title="4.5.1. 互补松弛性"></a>4.5.1. 互补松弛性</h3><p>如果强对偶性成立，则有：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_0(x^{\star})&=g(\lambda^{\star},\nu^{\star})\\
&=\mathop{inf}\limits_{x}\left(f_0(x)+\sum_{i=1}^{m}{\lambda _i^{\star}f_i(x)} + \sum_{i=1}^{p}{\nu _i^{\star}h_i(x)}\right))\\
&\leq f_0(x^{\star})+\sum_{i=1}^{m}{\lambda _i^{\star}f_i(x^{\star})} + \sum_{i=1}^{p}{\nu _i^{\star}h_i(x^{\star})}\\
&\leq f_0(x^{\star})\\
\tag{4-10}
\end{align*}</script><p>上式可以得到几个有用的结论：</p>
<ul>
<li>由于第三个不等式取等号，说明$L(x,\lambda^{\star},\nu^{\star})$在$x^{\star}$处取得局部最小值，也即该点处导数为0</li>
<li>$\lambda_i^{\star}f_i(x^{\star})=0,i=1,2,\cdots,m$，这个称为<strong>互补松弛条件</strong>，意味着在最优点处，不等式约束要么取等号$(f_i(x^{\star})=0)$，要么它对应的Lagrange乘子为零$\lambda_i^{\star}=0$</li>
</ul>
<h3 id="4-5-2-KKT最优性条件"><a href="#4-5-2-KKT最优性条件" class="headerlink" title="4.5.2. KKT最优性条件"></a>4.5.2. KKT最优性条件</h3><p>这小节讨论的目标函数$f_0$和约束函数$f_1,f_2,\cdots,f_m,h_1,h_2,\cdots,h_p$是可微的，但并不要求它们都是凸函数。<br>结合上一小节的内容，我们可以推出，对于目标函数和约束函数可微的任意优化问题，如果强对偶性成立，则任一对偶问题的最优解和对偶问题的最优解必须满足下列的式子：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_i(x^{\star})&\leq 0 ,i=1,2,\cdots,m\\
h_i(x^{\star})&=0,i=1,2,\cdots,p\\
\lambda_i^{\star}&\geq 0,i=1,2,\cdots,m\\
\lambda_i^{\star}f_i(x^{\star})&=0,i=1,2,\cdots,m\\
\nabla f_0(x^{\star})+\sum_{i=1}^{m}{\lambda _i^{\star}\nabla f_i(x^{\star})} + \sum_{i=1}^{p}{\nu _i^{\star}\nabla h_i(x^{\star})}&=0\\
\tag{4-9}
\end{align*}</script><p>上式被称为<strong>非凸问题的KKT条件</strong>：<br><strong>如果原问题是凸问题，则满足KKT条件的点也是原、对偶问题的最优解</strong>。这个定理很重要！<br>上述定理的证明：前面两个条件说明了$x^{\star}$是原问题的可行解；因为$\lambda^{\star}\geq 0$，所以$L(x,\lambda^{\star},\nu^{\star})$是x的凸函数；最优一个条件说明了Lagrange函数在$x^{\star}$处导数为零，也即Lagrange函数取得全局最小值，因此此时有：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda^{\star},\nu^{\star})&=L(x^{\star},\lambda^{\star},\nu^{\star})\\
&=f_0(x^{\star})+\sum_{i=1}^{m}{\lambda _i^{\star}f_i(x^{\star})} + \sum_{i=1}^{p}{\nu _i^{\star}h_i(x^{\star})}\\
&=f_0(x^{\star})\\
\tag{4-10}
\end{align*}</script><p>上述意味着对偶间隙为0，强对偶性成立，因此得证。</p>
<h3 id="4-5-3-通过解对偶问题求解原问题"><a href="#4-5-3-通过解对偶问题求解原问题" class="headerlink" title="4.5.3. 通过解对偶问题求解原问题"></a>4.5.3. 通过解对偶问题求解原问题</h3><p>由前面可知，如果强对偶性成立，且存在一个对偶最优解$(\lambda^{\star},\nu^{\star})$，那么任意原问题最优点也是$L(x,\lambda^{\star},\nu^{\star})$的最优解。利用这个性质，我们可以从对偶最优方程中去求解原问题最优解。确切的讲，如果强对偶性成立，对偶最优解$(\lambda^{\star},\nu^{\star})$已知，并且下列问题的解唯一：</p>
<script type="math/tex; mode=display">
min\ f_0(x)+\sum_{i=1}^{m}{\lambda _if_i(x)} + \sum_{i=1}^{p}{\nu _ih_i(x)}\tag{4-11}</script><p>（Lagrange函数是严格凸函数时上述最优化问题的解是唯一的），如果上式问题的解是原问题的可行解，那么它就是原问题的最优解；如果它不是原问题的可行解，那么原问题不存在最优解（或者无法达到）。当对偶问题比原问题更容易求解时，上述方法很有意义。</p>
<h1 id="5-利用Lagrange对偶求解最优化问题的例子"><a href="#5-利用Lagrange对偶求解最优化问题的例子" class="headerlink" title="5. 利用Lagrange对偶求解最优化问题的例子"></a>5. 利用Lagrange对偶求解最优化问题的例子</h1><h2 id="5-1-熵的最大化问题"><a href="#5-1-熵的最大化问题" class="headerlink" title="5.1. 熵的最大化问题"></a>5.1. 熵的最大化问题</h2><p>这个例子在机器学习中可能会经常遇到。问题描述如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ f_0(x)&=\sum_{i=1}^{n}{x_ilog{x_i}}\\
s.t.\ Ax&\preceq b\\
1^Tx&=1\\
\tag{5-1}
\end{align*}</script><p>定义域为$R_{++}$<br>记目标函数为$f_0(x)$Lagrange函数为：</p>
<script type="math/tex; mode=display">
\begin{align*}
L(x,\lambda,\nu)=f_0(x)+\lambda^T(Ax-b)+\nu(\vec 1^Tx-1)\\
\tag{5-2}
\end{align*}</script><p>Lagrange对偶函数为：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda,\nu)&=\mathop{inf}\limits_{x}\ \left(f_0(x)+\lambda^T(Ax-b)+\nu(1^Tx-1)\right)\\
&=-b^T\lambda-\nu+\mathop{inf}\limits_{x}\ \left(f_0(x)+(A^T\lambda+\vec 1\nu)^Tx\right)\\
&=-b^T\lambda-\nu-\mathop{sup}\limits_{x}\ \left(-f_0(x)-(A^T\lambda+\vec 1\nu)^Tx\right)\\
&=-b^T\lambda-\nu-f_0^{\star}\left(-(A^T\lambda+\vec 1\nu)\right)\\
\tag{5-3}
\end{align*}</script><p>其中$f_0^{\star}$是$f_0$的共轭函数。对于负熵函数$xlog{x}$，它的共轭函数不难推导出是$f^{\star}(y)=e^{y-1}$，因此不难得出$(5-3)$可进一步化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda,\nu)&=\mathop{inf}\limits_{x}\ \left(f_0(x)+\lambda^T(Ax-b)+\nu(1^Tx-1)\right)\\
&=-b^T\lambda-\nu-\sum_{i=1}^{n}e^{\left(-a_i^T\lambda-\nu-1\right)}\\
\tag{5-3}
\end{align*}</script><p>假设原问题可行，也即Slater条件成立（注意这里的约束条件都是仿射函数），那么此时强对偶性成立。因此对Lagrange函数求最小值即可求得原问题的最小值解。注意到Lagrange函数是严格凸函数，很容易求得最小值点</p>
<script type="math/tex; mode=display">
x_i^{\star}=exp\left(-(a_i^T\lambda^{\star}+\nu^{\star}+1)\right),i=1,2,\cdots,n\\
\tag{5-4}</script><p>其中$a_i$是$A$的列向量，如果$x^{\star}$是原问题的可行解，则必定是原问题的最优解。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/10/03/seq2seq入门详解：从RNN到Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/10/03/seq2seq入门详解：从RNN到Attention/" itemprop="url">seq2seq入门详解：从RNN到Attention</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-03T23:09:18+08:00">
                2020-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>[TOC]</p>
<h1 id="1-前馈神经网络的缺点"><a href="#1-前馈神经网络的缺点" class="headerlink" title="1. 前馈神经网络的缺点"></a>1. 前馈神经网络的缺点</h1><p>对于输入向量中个分量的位置信息不感知，也即无法利用序列型输入特征向量中的位置信息（将个分量调换顺序最后训练出的模型是等价的），但是在实际的任务中，各分量是有先后关系的。例如，我们在理解一段文本时，孤立地理解每个字或者词是不够的，还要将它们作为一个整体的序列来理解。</p>
<h1 id="2-循环神经网络RNN"><a href="#2-循环神经网络RNN" class="headerlink" title="2. 循环神经网络RNN"></a>2. 循环神经网络RNN</h1><h2 id="2-1-RNN的基本结构与数学定义"><a href="#2-1-RNN的基本结构与数学定义" class="headerlink" title="2.1. RNN的基本结构与数学定义"></a>2.1. RNN的基本结构与数学定义</h2><p>输入层的维数是$(n_x,m,T_x)$,其中$n_x$是每个训练样本的维数，例如输入词one-hot向量的大小，也即词典大小；$m$是一个batch的大小；$T_x$是输入序列的长度。</p>
<p>输出层的维数是$(n_y,m,T_y)$,其中$n_y$是输出预测向量的维数；$m$是一个batch的大小；$T_y$是输出序列的长度。</p>
<p>我们先研究输入向量和输出向量相等，即$n_x=n_y$的情况，结构图如下所示（图片来源<a href="https://www.coursera.org/learn/nlp-sequence-models/notebook/X20PE/building-a-recurrent-neural-network-step-by-step）。" target="_blank" rel="noopener">https://www.coursera.org/learn/nlp-sequence-models/notebook/X20PE/building-a-recurrent-neural-network-step-by-step）。</a><br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/%E6%A0%87%E5%87%86RNN%E6%A8%A1%E5%9E%8B-%E8%BE%93%E5%85%A5%E7%BB%B4%E6%95%B0%E7%AD%89%E4%BA%8E%E8%BE%93%E5%87%BA%E7%BB%B4%E6%95%B0.png" alt="RNN的基本结构"></p>
<center>图2.1 RNN基本结构</center>


<p>上下标说明举例：$a_5^{(2)[3]<4>}$表示第2个训练样本，第3层，第4个时刻，激活函数输出向量的第5维。<br>每个RNN-Cell的内部结构见下图<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/RNN%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83.png" alt="RNN的一个基本单元"></4></p>
<center>图2.2 RNN的一个基本单元</center>

<p>注意，输出$\hat y$是状态向量$a$经过线性变换再经过softmax变换得到的。</p>
<script type="math/tex; mode=display">
\begin{align*}
a^{\langle t\rangle}&=tanh\left(W_{ax}x^{\langle t\rangle}+W_{aa}a^{\langle t-1\rangle}+b_a\right)\\
\hat y^{\langle t\rangle}&=softmax\left(W_{ya}a^{\langle t\rangle}+b_y\right)\\
\tag{2-1}
\end{align*}</script><p>需要注意的是，在不同的RNN-Cell中，上述公式里面的参数$W,b$都是共享的。</p>
<h2 id="2-2-输入输出长度的讨论"><a href="#2-2-输入输出长度的讨论" class="headerlink" title="2.2. 输入输出长度的讨论"></a>2.2. 输入输出长度的讨论</h2><h3 id="2-2-1-n-x-n-y-n"><a href="#2-2-1-n-x-n-y-n" class="headerlink" title="2.2.1. $n_x=n_y=n$"></a>2.2.1. $n_x=n_y=n$</h3><p>第一种情况是输入输出长度相等的情况，如下图所示（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/RNN-%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%95%BF%E5%BA%A6%E7%9B%B8%E7%AD%89.jpg" alt="RNN-输入输出长度相等"></p>
<center>图2.3 RNN-输入输出长度相等</center>

<p>常用于序列标注模型，例如命名实体识别模型中。</p>
<h3 id="2-2-2-n-x-n-n-y-1"><a href="#2-2-2-n-x-n-n-y-1" class="headerlink" title="2.2.2. $n_x=n,n_y=1$"></a>2.2.2. $n_x=n,n_y=1$</h3><p>第二种情况是输入长度为N，输出长度为1（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/RNN-%E8%BE%93%E5%87%BA%E9%95%BF%E5%BA%A6%E4%B8%BA1.jpg" alt="RNN-输出长度为1"></p>
<center>图2.4 RNN-输出长度为1</center>

<p>模型只在最后一个时刻输出，常用于文本分类模型</p>
<h3 id="2-2-3-n-x-1-n-y-n"><a href="#2-2-3-n-x-1-n-y-n" class="headerlink" title="2.2.3. $n_x=1,n_y=n$"></a>2.2.3. $n_x=1,n_y=n$</h3><p>第三种情况是输入长度为1，输出长度为N。uti实现时，可以将输入作为最开始时刻的输入，也可以作为所有时刻的输入（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/RNN-%E8%BE%93%E5%85%A5%E9%95%BF%E5%BA%A6%E4%B8%BA1.jpg" alt="RNN-输入长度为1"></p>
<center>图2.5 RNN-输入长度为1</center>

<p><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/RNN-%E8%BE%93%E5%85%A5%E9%95%BF%E5%BA%A6%E4%B8%BA1%EF%BC%88%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81%E5%9C%A8%E6%89%80%E6%9C%89%E6%97%B6%E5%88%BB%E9%87%8D%E5%A4%8D%E4%BD%BF%E7%94%A8%EF%BC%89.jpg" alt="RNN-输入长度为1（输入特征在所有时刻重复使用）"></p>
<center>图2.6 RNN-输入长度为1（输入特征在所有时刻重复使用）</center>

<p>常用于文字生成模型中。</p>
<h3 id="2-2-4-n-x-n-n-y-m-，Encoder-Decoder模型"><a href="#2-2-4-n-x-n-n-y-m-，Encoder-Decoder模型" class="headerlink" title="2.2.4. $n_x=n,n_y=m$，Encoder-Decoder模型"></a>2.2.4. $n_x=n,n_y=m$，Encoder-Decoder模型</h3><p>第四种情况是输入长度为N,输出长度为M的情况，也即Encoder-Decoder模型（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/Encoder-Decoder.png" alt="Encoder-Decoder模型"></p>
<center>图2.7 Encoder-Decoder模型，输入输出长度为一般情况的RNN</center>

<p>常用于语音识别、机器翻译等场景。在后面的章节中我们会详细介绍Encoder-Decoder模型</p>
<h1 id="3-RNN的复杂变种"><a href="#3-RNN的复杂变种" class="headerlink" title="3. RNN的复杂变种"></a>3. RNN的复杂变种</h1><h2 id="3-1-GRU-Gated-Recurrent-Unit"><a href="#3-1-GRU-Gated-Recurrent-Unit" class="headerlink" title="3.1. GRU(Gated Recurrent Unit)"></a>3.1. GRU(Gated Recurrent Unit)</h2><p>GRU的提出是为了解决RNN难以学习到输入序列中的长距离信息的问题。<br>GRU引入一个新的变量——记忆单元，简称$C$。$C^{\langle t\rangle}$其实就是$a^{\langle t\rangle}$<br>$C$的表达式不是一步到位的，首先定义$C$的候选值$\tilde C$:</p>
<script type="math/tex; mode=display">
\tilde C^{\langle t\rangle}=tanh\left(W_c[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)</script><p>更新门：</p>
<script type="math/tex; mode=display">
\Gamma_u=\sigma\left(W_u[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)</script><p>在实际训练好的网络中$\Gamma$要么很接近1要么很接近0，对应着输入序列里面有些元素起作用有些元素不起作用。</p>
<script type="math/tex; mode=display">
C^{\langle t\rangle}=\Gamma_u*\tilde C^{\langle t\rangle}+（1-\Gamma_u）* C^{\langle t-1\rangle}</script><p>也即输入序列的有些元素，记忆单元不需要更新，有些元素需要更新。</p>
<blockquote>
<p>The cat, which already ate …, was full</p>
</blockquote>
<p>cat后面的词直到was之前，都不需要更新$C$,直接等于cat对应的$C$<br>可以解决梯度消失的问题.输出层的梯度可以传播到cat处</p>
<p>注：$C$和$\Gamma$都可以是想聊，它们在相乘时采用的是element-wise的乘法。当为向量时，与cat的单复数无关的词对应的$\Gamma$可能有些维度为零，有些维度不为零。为零的维度，是用来保留cat的单复数信息的；不为零的维度可能是保留其他语义信息的，比如是不是food呀之类的<br>目前讨论的是简化版的GRU，结构图如下<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/GRU.png" alt=""></p>
<center>图3.1GRU的一个基本单元</center>

<p>完整的GRU：</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[\Gamma_r*C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_r&=\sigma\left(W_r[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_r\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+（1-\Gamma_u）* C^{\langle t-1\rangle}\\
a^{\langle t\rangle}&=C^{\langle t\rangle}\\
\tag{3-1}
\end{align*}</script><p>$\Gamma_r$表示了$\tilde C^{\langle t\rangle}$和$C^{\langle t-1\rangle}$之间的相关程度</p>
<h2 id="3-2-LSTM-Long-Short-Term-Memory"><a href="#3-2-LSTM-Long-Short-Term-Memory" class="headerlink" title="3.2. LSTM(Long Short-Term Memory)"></a>3.2. LSTM(Long Short-Term Memory)</h2><p>没有了$\Gamma_r$，将$1-\Gamma_u$用$\Gamma_f$代替</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_f&=\sigma\left(W_f[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_f\right)\\
\Gamma_o&=\sigma\left(W_o[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_o\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+\Gamma_f* C^{\langle t-1\rangle}\\
a^{\langle t\rangle}&=\Gamma_o*tanh\left(C^{\langle t\rangle}\right)\\
\tilde y^{\langle t\rangle}&=softmax(a^{\langle t\rangle})\\
\tag{3-2}
\end{align*}</script><p>(注意公式里面的$\Gamma_u$等价于图片中的$\Gamma_i$)</p>
<p><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/LSTM_%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83.png" alt="LSTM基本单元"></p>
<center>图3.2 LSTM的一个基本单元</center>

<p><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/%E6%A0%87%E5%87%86LSTM%E6%A8%A1%E5%9E%8B-%E8%BE%93%E5%85%A5%E7%BB%B4%E6%95%B0%E7%AD%89%E4%BA%8E%E8%BE%93%E5%87%BA%E7%BB%B4%E6%95%B0.png" alt="标准LSTM模型-输入维数等于输出维数"></p>
<center>图3.3 标准LSTM模型-输入维数等于输出维数</center>

<h3 id="3-2-1-peephole连接"><a href="#3-2-1-peephole连接" class="headerlink" title="3.2.1. peephole连接"></a>3.2.1. peephole连接</h3><p><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/LSTM%E5%8F%98%E7%A7%8D-peephole.png" alt="LSTM-peephole"></p>
<center>图3.4 LSTM——peephole</center>

<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[a^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_f&=\sigma\left(W_f[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_f\right)\\
\Gamma_o&=\sigma\left(W_o[c^{\langle t\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_o\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+\Gamma_f* C^{\langle t-1\rangle}\\
a^{\langle t\rangle}&=\Gamma_o*tanh\left(C^{\langle t\rangle}\right)\\
\tilde y^{\langle t\rangle}&=softmax(a^{\langle t\rangle})\\
\tag{3-3}
\end{align*}</script><h3 id="3-2-2-projection"><a href="#3-2-2-projection" class="headerlink" title="3.2.2 projection"></a>3.2.2 projection</h3><p>对隐藏层状态a进行一次线性变换，降低其维数</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[a^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_f&=\sigma\left(W_f[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_f\right)\\
\Gamma_o&=\sigma\left(W_o[c^{\langle t\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_o\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+\Gamma_f* C^{\langle t-1\rangle}\\
a_0^{\langle t\rangle}&=\Gamma_o*tanh\left(C^{\langle t\rangle}\right)\\
a^{\langle t\rangle}&=W_{proj}a_0^{\langle t\rangle}+b_{proj}\\
\tilde y^{\langle t\rangle}&=softmax(a^{\langle t\rangle})\\
\tag{3-4}
\end{align*}</script><h1 id="4-Encoder-Decoder模型"><a href="#4-Encoder-Decoder模型" class="headerlink" title="4. Encoder-Decoder模型"></a>4. Encoder-Decoder模型</h1><p>由前面的章节我们知道，Encoder-Decoder模型就是输入输出长度为一般情况的RNN模型，示意图如下：<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/Encoder-Decoder.png" alt="Encoder-Decoder"></p>
<center>图4.1 Encoder-Decoder</center>

<p>其中Encoder负责将输入进行编码，得到语义编码向量C；Decoder负责将语义编码向量C进行解码，得到输出。以机器翻译为例，英文作为输入，输出为中文。可以用如下的数学模型来表示：</p>
<script type="math/tex; mode=display">
\begin{align*}
input&=&lt x_1,x_2,\cdots,x_n &gt\\
C&=f(input)\\
y_i&=g( C,y_1,y_2,\cdots,y_{i-1} ),i=1,2,\cdots,m\\
output&=&lt y_1,y_2,\cdots,y_m &gt\\
\tag{4-1}
\end{align*}</script><p>从Encoder得到C的方式有多种，可以将Encoder最后一个时刻的隐藏状态作为C，也可以将所有的隐藏状态进行某种变换得到C。<br>语义编码C在Decoder中的作用当时有多种，常见的有如下两种<br>（1） C作为Decoder的初始状态$h_0$。<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/C%E4%BD%9C%E4%B8%BADecoder%E7%9A%84%E5%88%9D%E5%A7%8B%E7%8A%B6%E6%80%81h0.png" alt="C作为Decoder的初始状态h0"></p>
<center>图4.2 C作为Decoder的初始状态h0</center>

<p>（2） C作为Decoder的每一步输入。<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/RNN/C%E4%BD%9C%E4%B8%BADecoder%E7%9A%84%E6%AF%8F%E4%B8%80%E6%AD%A5%E8%BE%93%E5%85%A5.png" alt="C作为Decoder的每一步输入"></p>
<center>图4.3 C作为Decoder的每一步输入</center>

<p>具体公式和应用场景见下一小节。</p>
<h2 id="4-1-几种典型的encoder-decoder"><a href="#4-1-几种典型的encoder-decoder" class="headerlink" title="4.1. 几种典型的encoder-decoder"></a>4.1. 几种典型的encoder-decoder</h2><p>参考<a href="https://zhuanlan.zhihu.com/p/70880679" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70880679</a></p>
<h3 id="4-1-1-第一种-语义编码C作为Decoder的初始输入"><a href="#4-1-1-第一种-语义编码C作为Decoder的初始输入" class="headerlink" title="4.1.1. 第一种:语义编码C作为Decoder的初始输入"></a>4.1.1. 第一种:语义编码C作为Decoder的初始输入</h3><p><a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">https://arxiv.org/abs/1409.3215</a> Sequence to Sequence Learning with Neural Networks<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/encoder-decoder-%E6%A1%86%E6%9E%B61.jpg" alt=""></p>
<center>图4.4 Encoder-Decoder框架1-C作为Decoder的初始输入</center>

<p>Encoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},x_t]+b)\\
c&=h_T\\
\tag{4-2}
\end{align*}</script><p>Decoder:</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},y_{t-1}]+b),h_0=c\\
o_t&=softmax(Vh_t+d)\\
\tag{4-3}
\end{align*}</script><h3 id="4-1-2-第一种-语义编码C作为Decoder的每一步输入"><a href="#4-1-2-第一种-语义编码C作为Decoder的每一步输入" class="headerlink" title="4.1.2. 第一种:语义编码C作为Decoder的每一步输入"></a>4.1.2. 第一种:语义编码C作为Decoder的每一步输入</h3><p><a href="https://arxiv.org/pdf/1406.1078" target="_blank" rel="noopener">https://arxiv.org/pdf/1406.1078</a> Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/encoder-decoder-%E6%A1%86%E6%9E%B62.jpg" alt=""></p>
<center>图4.5 Encoder-Decoder框架1-C作为Decoder的每一步输入</center>

<p>Encoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},x_t]+b)\\
c&=(Vh_T)\\
\tag{4-4}
\end{align*}</script><p>Decoder:</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},y_{t-1},c]+b)\\
o_t&=softmax(Vh_t+c)\\
\tag{4-5}
\end{align*}</script><h2 id="4-2-Encoder-Decoder的缺点"><a href="#4-2-Encoder-Decoder的缺点" class="headerlink" title="4.2. Encoder-Decoder的缺点"></a>4.2. Encoder-Decoder的缺点</h2><ol>
<li>对于输入序列的每个分量的重要程度没有区分，这和人的思考过程是不相符的，例如人在翻译的时候，对于某个一词多义的词，可能会结合上下文中某些关键词进行辅助判断。</li>
<li>如果在Decoder阶段，仅仅将C作为初始状态，随着时间往后推进，C的作用会越来越微弱。</li>
</ol>
<p>事实上，Attention机制的提出，主要就是为了解决上述问题。</p>
<h1 id="5-Attention机制详解"><a href="#5-Attention机制详解" class="headerlink" title="5. Attention机制详解"></a>5. Attention机制详解</h1><p>前面讲到，在一般形式的encoder-decoder中，输入信息先经过encoder编码保存在C中，C再被decoder使用。这种“直接粗暴”的方式，可能会导致输入信息没有被合理的利用，尤其是当输入信息过长的时候。为了解决这个问题，Attention机制被提出，解决的思路是：在decoder阶段，每个时间点输入的C是不同的(示意图如下图所示)，需要根据当前时刻要输出的y去合理地选择输入x中的上下文信息。<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/attention%E6%9C%BA%E5%88%B6%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Attention机制示意图"></p>
<center>图5.1 Attention机制示意图</center>

<p>具体来讲，就是对encoder的隐藏状态进行加权求和，以便得到不同的C，以中文翻译英文为例，示意图如下（图片来源<a href="https://www.jianshu.com/p/d2ae158fc9e5" target="_blank" rel="noopener">https://www.jianshu.com/p/d2ae158fc9e5</a>）：<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/Attention-%E5%AF%B9encoder%E9%9A%90%E8%97%8F%E7%8A%B6%E6%80%81%E8%BF%9B%E8%A1%8C%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C%E5%BE%97%E5%88%B0%E4%B8%8D%E5%90%8C%E7%9A%84C.png" alt="Attention-对encoder隐藏状态进行加权求和得到不同的C"></p>
<center>图5.2 Attention-对encoder隐藏状态进行加权求和得到不同的C</center>

<p>记$a_{ij}$为encoder中第$j$个隐藏状态$h_j$到decoder中第$i$个隐藏状态$h_i’$对应的$c_i$的权重，可以通过训练确定的，具体计算方法见后文。attention机制的核心思想可以概括为”对输入信息加权求和得到编码信息c”，也即如下公式：</p>
<script type="math/tex; mode=display">
c_i=\sum_{j=1}^{n_x}a_{ij}h_j\tag{5-1}</script><h2 id="5-1-attention机制中权重系数的计算过程"><a href="#5-1-attention机制中权重系数的计算过程" class="headerlink" title="5.1. attention机制中权重系数的计算过程"></a>5.1. attention机制中权重系数的计算过程</h2><p>attention机制中权重系数有多种计算过程，对应于不同种类的attention机制。但是大部分的attention机制，都能表示为下文提到的三个抽象阶段。这里先引入几个概念。<br>我们将模型输入内容记为source，输出内容记为target。<br>source可以表示为一个一个的（key,value），target则表示为一个一个的query。在机器翻译中，key和value合并为一个，就是输入句子中每个单词对应的隐藏层状态。<br>通过计算Query和各个Key的相似性或者相关性（需要进行softmax归一化），得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。</p>
<script type="math/tex; mode=display">
Attention(Query_i,Source)=\sum_{j=1}^{L_x}Similarity(Query,key_j)*value_j\tag{5-2}</script><p>具体来说可以分为三个阶段，如下图所示：</p>
<p><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/attention%E4%B8%AD%E6%9D%83%E9%87%8D%E7%B3%BB%E6%95%B0%E8%AE%A1%E7%AE%97%E7%9A%84%E4%B8%89%E4%B8%AA%E9%98%B6%E6%AE%B5.jpg" alt="attention机制中计算权重系数的三个阶段"></p>
<center>图5.3 attention机制中计算权重系数的三个阶段</center>

<p>其中第一阶段计算相似性时有多种方法，例如向量点积、余弦相似度，甚至可以用一个小的神经网络来通过学习的方式计算。<br>第二阶段softmax归一化的公式如下：</p>
<script type="math/tex; mode=display">
a_{ij}=softmax(S_{ij})=\frac{exp(Sim_{ij})}{\sum_{j=1}^{L_x}exp(Sim_{ij})}\tag{5-3}</script><h2 id="5-2-几种典型的attention机制"><a href="#5-2-几种典型的attention机制" class="headerlink" title="5.2. 几种典型的attention机制"></a>5.2. 几种典型的attention机制</h2><p><a href="https://zhuanlan.zhihu.com/p/70905983" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70905983</a></p>
<h3 id="5-2-1-第一种-Bahdanau-Attention"><a href="#5-2-1-第一种-Bahdanau-Attention" class="headerlink" title="5.2.1. 第一种:Bahdanau Attention"></a>5.2.1. 第一种:<strong>Bahdanau Attention</strong></h3><p><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">https://arxiv.org/abs/1409.0473</a><br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/encoder-decoder-%E6%A1%86%E6%9E%B62.jpg" alt="attention机制框架1"></p>
<center>图5.4 attention机制框架1</center>

<p>Encoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
h_i&=tanh(W[h_{i-1},x_i]+b)\\
\tag{5-4}
\end{align*}</script><p>语义向量：</p>
<script type="math/tex; mode=display">
\begin{align*}
e_{ti}&=v_a^T tanh(W_a [s_{i-1,h_i}])\\
\alpha_{it}&=\frac{exp(e_{ti})}{\sum_{k=1}^Texp(e_{tk})}\\
c_t&=\sum_{i=1}^T\alpha_{ti}h_i\\
\tag{5-5}
\end{align*}</script><p>其中$e_{ti}$是Encoder中$i$时刻隐藏层状态$h_i$对Decoder中$t$时刻以仓储状态$s_t$的影响程度；$\alpha_{ti}$是对$e_{ti}$进行softmax归一化成的概率，也即前文提到的$a_{ij}$或者叫attention权重；$c_t$是$t$时刻的语义向量。<br>Decoder:</p>
<script type="math/tex; mode=display">
\begin{align*}
s_t&=tanh(W[s_{t-1},y_{t-1},c_t])\\
o_t&=softmax(Vs_t)\\
\tag{5-6}
\end{align*}</script><h3 id="5-2-2-第二种：-Global-attentional-model"><a href="#5-2-2-第二种：-Global-attentional-model" class="headerlink" title="5.2.2. 第二种： Global attentional model"></a>5.2.2. 第二种： <strong>Global attentional model</strong></h3><p><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/Attention/attention%E6%A1%86%E6%9E%B62-global-attention.JPG" alt="attention框架2-global-attention"></p>
<center>图5.5 attention框架2-global attention</center>

<p>Encoder与第一种attention是一样的。<br>语义向量：</p>
<script type="math/tex; mode=display">
\begin{align*}
s_t&=tanh(W[s_{t-1},y_{t-1}])\\
e_{ti}&=s_t^TW_ah_i\\
\alpha_{it}&=\frac{exp(e_{ti})}{\sum_{k=1}^Texp(e_{tk})}\\
c_t&=\sum_{i=1}^T\alpha_{ti}h_i\\
\tag{5-7}
\end{align*}</script><p>Decoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde s_t&=tanh(W[s_t,c_t])\\
o_t&=softmax(V\tilde s_t)\\
\tag{5-8}
\end{align*}</script><p>第一种attention机制和第二种的区别主要在于计算影响程度$e_{ti}$的函数（一般称之为对齐函数）不同，第一种依赖于$h_i$和$s_{t-1}$，第二种依赖于$h_i$和$s_t$。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/16/统计学之t检验详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/16/统计学之t检验详解/" itemprop="url">统计学之t检验详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-16T00:31:29+08:00">
                2020-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://blog.csdn.net/Tonywu2018/article/details/83897806" target="_blank" rel="noopener">https://blog.csdn.net/Tonywu2018/article/details/83897806</a></p>
<h2 id="0-背景故事"><a href="#0-背景故事" class="headerlink" title="0. 背景故事"></a>0. 背景故事</h2><p>t检验又叫学生t检验（Student‘s t test），它是由20世纪爱尔兰的一家啤酒厂-健力士酒厂的一名员工（戈斯特）采用笔名“Student”发表的学术文章而得名。</p>
<h2 id="1-从一个例子引入t检验的思路"><a href="#1-从一个例子引入t检验的思路" class="headerlink" title="1. 从一个例子引入t检验的思路"></a>1. 从一个例子引入t检验的思路</h2><p>健力士公司是酿啤酒的，啤酒的原材料是麦子，因此公司种了很多麦田。假设有两片麦田，一块采用A工艺（旧）种植，另一块采用B工艺（新）种植。A工艺的麦田平均每株麦子可以结100粒穗子。公司想知道B工艺是否相比A工艺提高了产量。为了节约成本、小小损耗，抠门的资本家老板从B工艺的麦田里随机摘了5株大麦，每株麦子的平均穗子数量为120粒，看起来似乎产量提高了，因为每株麦子的麦穗粒数均值增加了20%。如何确定这样的结论是否可信呢？</p>
<blockquote>
<p>原假设：B工艺没有提高产量，即AB工艺下的每株麦子麦穗数量服从同一个分布<br>备选假设：B工艺提高了产量</p>
</blockquote>
<p>由中心极限定理，A攻一下每株麦穗的粒数服从均值为100，方差未知的正态分布：</p>
<script type="math/tex; mode=display">
X\sim N(\mu,\sigma^2)\tag{1-1}</script><p>B工艺的单株麦穗粒数也可以认为服从正态分布。如果原假设正确的话，B和A服从同样的正态分布。那么这时候我们可以去评估出现5株均值为120的麦穗的概率是否很极端，来判断原假设是否合理。可以对B的每株麦穗数的分布归一化为标准正态分布，再去查表评估其概率值。也即要计算$\frac{\bar x-\mu_0}{\delta_0}$，其中$\bar x$是B工艺的麦穗粒数均值，$\mu_0$为A工艺的麦穗粒数均值，$\delta_0$为A工艺的麦穗粒数均值。由于B工艺是抽取出一定的样本数来计算均值$\bar x$的，因此不能代表总体均值。当样本数很大时，根据大数定理可以直接认为B工艺提高了产量；当样本数很小时，可能是随机误差。因此，不妨对前面的式子再除以一个n相关的数。为此，戈斯特构造了一个新的统计量：</p>
<script type="math/tex; mode=display">
t=\frac{\bar x-\mu_0}{\delta_0/\sqrt n}\tag{1-2}</script><p>该统计量越大，寿命AB工艺导致的差别越大，越有可能说明B工艺提高了产量。</p>
<h2 id="3-t分布"><a href="#3-t分布" class="headerlink" title="3. t分布"></a>3. t分布</h2><p>对于t统计量：<script type="math/tex">t=\frac{\bar x-\mu_0}{\delta_0/\sqrt n}</script>,其对应的概率密度函数也即t分布为：</p>
<script type="math/tex; mode=display">
f(x)=\frac{\Gamma((\nu+1)/2)}{\sqrt(\nu \pi)\Gamma(\nu/2)}(1+t^2/\nu)^{-(\nu+1)/2}\tag{3-1}</script><p>其中$\nu=n-1$称为自由度，$\Gamma(x)=\int_0^{+\infty}t^{x-1}e^{-t}dt(x&gt;0)$是伽马函数。<br>t分布的函数图像与正态分布有点像，给定t值和自由度，可以通过查表的方式去找到对应的P值。t分布表如下：<br><img src="https://img-blog.csdnimg.cn/20200320000514397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="t分布表"></p>
<p>以本文中的例子为例，假设置信水平wie$\alpha=0.05$，查表得T值为2.132（单侧检验）。假设A工艺的标准差为$5\sqrt5$，可计算得出t=4，大于T。因此可以拒绝原假设。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/16/统计学之样本方差与总体方差/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/16/统计学之样本方差与总体方差/" itemprop="url">统计学之样本方差与总体方差</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-16T00:23:00+08:00">
                2020-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考资料：<a href="https://www.cnblogs.com/zzdbullet/p/10087196.html" target="_blank" rel="noopener">https://www.cnblogs.com/zzdbullet/p/10087196.html</a></p>
<h2 id="1-方差（variance）的定义"><a href="#1-方差（variance）的定义" class="headerlink" title="1. 方差（variance）的定义"></a>1. 方差（variance）的定义</h2><p>方差是用来度量随机变量和其数学期望（均值）之间的偏离程度的一个统计量。</p>
<p>统计学中（所有样本）的总体方差公式：</p>
<script type="math/tex; mode=display">
\sigma^2=\frac{\sum(X-\mu)^2}{N} \tag{1-1}</script><p>其中$\sigma^2$是总体方差，$X$是随机变量，$\mu$是总体均值（有时也用$\bar X$表示），$N$是总体样本数。这里提到的样本，是基于样本数量$N$（几乎）无限的假设。对应的各个统计量，也是所有的样本所服从的分布的真实参数，是客观正真实的。</p>
<h2 id="2-样本方差"><a href="#2-样本方差" class="headerlink" title="2. 样本方差"></a>2. 样本方差</h2><p>现实情况中，我们往往得不到所有的无限样本，而只能抽样出一定数量的有限样本。通过有限的样本来计算的方差，称为样本方差，公式如下：</p>
<script type="math/tex; mode=display">
S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar X)^2\tag{2-1}</script><p>注意上式的系数和总体方差公式里面的系数不一样，分母是$n-1$。为什么不用$n$作为分母呢？这是因为如果沿用总体方差的公式得到的样本方差，是对方差的一个有偏估计。用$n$作为分母的样本方差公式，才是对方差的无偏估计。</p>
<h2 id="3-总体方差公式的有偏性证明"><a href="#3-总体方差公式的有偏性证明" class="headerlink" title="3. 总体方差公式的有偏性证明"></a>3. 总体方差公式的有偏性证明</h2><script type="math/tex; mode=display">
\begin{align*}
\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2&=\frac{1}{n}\sum_{i=1}^{n}\left[(X_i-\mu)+(\mu-\bar X)\right]^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2+\frac{2}{n}\sum_{i=1}^{n}(X_i-\mu)(\mu-\bar X)+\frac{1}{n}\sum_{i=1}^{n}(\mu-\bar X)^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2+2(\bar X-\mu)(\mu-\bar X)+(\mu-\bar X)^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2-(\mu-\bar X)^2\\
\tag{3-1}
\end{align*}</script><p>换言之，除非正好有$\bar X=\mu$，否则一定会有</p>
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2<\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2\tag{3-2}</script><p>上式的右边是对方差的正确估计，左边是有偏估计。<br>产生这一偏差的本质是因为均值用的是样本均值$\bar X$。这将导致采样出来的样本之间不是完全相互独立的，自由度从$n$降为了$n-1$。（注意，一个好的采样有两点要求：随机采样，并且样本之间是相互独立的）这是因为，给定$\bar X$和任意$n-1$个样本，就能确定剩下的一个样本，也即只有$n-1$个样本是完全相互独立的，自由度为$n-1$。</p>
<h2 id="4-样本方差公式分母为n-1的推导"><a href="#4-样本方差公式分母为n-1的推导" class="headerlink" title="4. 样本方差公式分母为n-1的推导"></a>4. 样本方差公式分母为n-1的推导</h2><p>在正式推导之前，先给几个公式作为铺垫：</p>
<ol>
<li>方差计算公式：<script type="math/tex; mode=display">
D(X)=E(X^2)-[E(X)]^2\tag{4-1}</script></li>
<li>均值的均值：<script type="math/tex; mode=display">
\begin{align*}
E(\bar X)&=E\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)\\
&=\frac{1}{n}E(\sum_{i=1}^{n}X_i)\\
&=E(X_i)\\
&=\bar X\tag{4-4}
\end{align*}</script></li>
<li>均值的方差<script type="math/tex; mode=display">
\begin{align*}
D(\bar X)&=D\left(\frac{1}{n}\sum_{i=1}^nX_i\right)\\
&=\frac{1}{n^2}D(\sum_{i=1}^{n}X_i)\\
&=\frac{1}{n}D(X_i)\\
\tag{4-5}
\end{align*}</script></li>
</ol>
<p>对于没有修正的方差计算公式，计算其期望：</p>
<script type="math/tex; mode=display">
\begin{align*}
E(S^2)&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-\frac{2}{n}(X_i)(\bar X)+\frac{1}{n}\sum_{i=1}^{n}(\bar X)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-2(\bar X)^2+(\bar X)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-(\bar X)^2\right)\\
&=E((X_i)^2)-E((\bar X)^2)\\
&=D(X_i)+\left(E(X_i)\right)^2-\left(D(\bar X)+\left(E(\bar X)\right)^2\right)
\tag{4-6}
\end{align*}</script><p>结合{4-4}和{4-5}，可将{4-6}化简为</p>
<script type="math/tex; mode=display">
\begin{align*}
E(S^2)&=D(X_i)-\frac{1}{n}D(X_i)\\
&=\frac{n-1}{n}D(X_i)\\
&=\frac{n-1}{n}\sigma^2\\
\tag{4-7}
\end{align*}</script><p>要使样本方差的期望等于总体方差，就需要进行修正，也即给样本方差乘上$\frac{n}{n-1}$<br>因此得到修正后的样本方差公式:</p>
<script type="math/tex; mode=display">
\begin{align*}
S^2&=\frac{n}{n-1}\left(\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2\right)\\
&=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2\\
\tag{4-8}
\end{align*}</script><p>推导完毕！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/15/统计学之假设检验详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/15/统计学之假设检验详解/" itemprop="url">统计学之假设检验详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-15T23:37:41+08:00">
                2020-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://cosx.org/2010/11/hypotheses-testing/" target="_blank" rel="noopener">https://cosx.org/2010/11/hypotheses-testing/</a></p>
<h2 id="0-背景"><a href="#0-背景" class="headerlink" title="0. 背景"></a>0. 背景</h2><p>在实际生产生活中，我们经常需要对一些逻辑推理进行真假判断，例如</p>
<blockquote>
<p>如果你打了某种疫苗P，就不会得某种流行病Q<br>如果一个疑似病人隔离了14天还没确诊，那他就没有被感染新冠肺炎</p>
</blockquote>
<p>在统计学里面，不会像上面那样说，而是会说：</p>
<blockquote>
<p>如果你打了某种疫苗 ，就有95%的把握不会得流行病Q<br>如果一个疑似病人隔离了14天还没确诊，那他就有95%的把握没有被感染新冠肺炎</p>
</blockquote>
<p>其中的把握水平，在统计推断中用“置信水平”来代替。置信水平是可以人为选取的。</p>
<h2 id="1-从一个硬币的例子来引入假设检验"><a href="#1-从一个硬币的例子来引入假设检验" class="headerlink" title="1. 从一个硬币的例子来引入假设检验"></a>1. 从一个硬币的例子来引入假设检验</h2><p>如何从统计推断的角度来判断一个逻辑推理是否正确呢？通常，我们会给定一个置信水平，然后判断该逻辑推理是否在这个置信水平下成立。这里重新举一个硬币的例子，来引入置信水平的概念。<br>假设有如下命题：</p>
<blockquote>
<p>if P then Q<br>P: 在 100 次投掷中，得到 90 次正面，10 次反面。<br>Q: 硬币不是均匀的。</p>
</blockquote>
<p>我们想知道，如果P成立，判断Q成立的把握有多大。很多时候（但不是所有时候），在统计推断里面，要证明的结论都是直觉上可能性比较大的，直接证明可能不太方便，可以反其道行之，证明Q的反面是否成立，来推断出Q是否成立。为此，列出如下原假设和备择假设：</p>
<blockquote>
<p>H0: 硬币是均匀的（P）<br>Ha: 硬币是不均匀的（not P）</p>
</blockquote>
<p>如果原假设为真，即硬币是均匀的，就不可能会发生这样极端的事情比如：在 100 次投掷中，得到 90 次正面，10 次反面。如果真的观察到了这么极端的事情，就有把握认为硬币不是均匀的，则拒绝原假设，选择备选假设。如果观察到的是60次正面，40个反面，则没有特别大的把握拒绝原假设，这枚硬币是否有偏，需要更多的证据来证明（这通常意味着做更多的实验，比如再投1000次）。</p>
<p>即使观测到100次投掷中90次正面10次反面，也不能说硬币一定是不均匀的（也即不能百分之百的把握拒绝原假设）。如果原假设为真，但是拒绝了原假设，这种情况称为<strong>第一类错误</strong>。发生第一类错误的概率，称为<strong>显著性水平</strong>，用$\alpha$表示。$1-\alpha$称为<strong>置信度</strong>或者<strong>置信水平</strong>，它表示我们根据抽样样本对总体参数的估计的可靠性。$\alpha$一般是人为定的，如0.05，0.01.给定置信水平后，就可以去利用一些统计学的知识去检验原假设是否需要拒绝。<br>如果原假设是错误的，但是没有拒绝原假设，则称为<strong>第二类错误</strong>。如果要求犯第一类错误的概率尽可能小，就会导致第二类错误的概率增大；反之，如果要求第二类错误的Giallo极可能小，就会导致第一类错误的概率增大。在实际中需要权衡。权衡的方式就是调节$\alpha$。在实际中，我们通常认为犯第一类错误的后果比犯第二类错误的后果更为严重。例如，关于打疫苗会后会不会得病的命题，我们通常会将原假设写成：会得病，然后去搜集数据试图拒绝原假设。此时犯第一类错误的后果是比较严重的（实际会得病却认为不会得病，会放松警惕造成大流行），而犯第二类错误的后果不是很严重（实际不糊得病，却没有拒绝原假设，只是会将打疫苗的部分人隔离起来造成一定的不便）</p>
<p>再强调一下，一般都是先提出需要建议的假设，再搜集数据，这是统计推断的原则之一。因为如果现有了数据再提出假设，容易有主观干扰。<br>到这里，我们还是没有解答如何去检验原假设是否需要被拒绝。别急，接着往下看。</p>
<h2 id="2-P值"><a href="#2-P值" class="headerlink" title="2. P值"></a>2. P值</h2><p>如何去定义一个事件是否“极端”呢？首先我们引入“更极端”的概念。更极端，意味着概率更小。例如，91次正面9次反面，比90次正面10次反面，更为极端。因此，很自然地，我们只需要描述出原假设为真，第一类错误恰好为$\alpha$时的事件，然后判断出当前样本集合里面的事件是否比它更极端，就能判断是否要在当前显著性水平下拒绝原假设了。当然，直接这样比较麻烦，可以转换一下思路：计算出发生比当前事件（90次正面，10次反面）更极端的事件的概率P，判断P与$\alpha$的大小，如果$P&lt;\alpha$，则说明如果原假设为真时，发生当前事件的概率很极端（比我们给定的显著性水平$\alpha$还低），因此说明原假设不合理，于是可以拒绝原假设了。此时发生第一类错误的概率小于$\alpha$。这里的概率P，称为<strong>P值</strong>。<br>在硬币投掷实验中，正面出现的次数服$X$服从一个二项分布：$X\sim B(n,p)$，其中$n=100,p-0.5$。根据中心极限定理，二项分布的极限分布是正态分布，因此可以由均值为$np=50$，方差为$np(1-p)=25$的正态分布来近似。我们用这个近似的正态分布的两端去考察所谓“更极端”的事件。取$\alpha=0.05$，由正态分布的性质不难得到，$P$值等于$X<10$或$x>90$的概率值，等于$2\times P(X&lt;10)=1.2442e-15$。这个小于我们给定的$\alpha$，因此该事件很极端，原假设不合理，拒绝原假设。</10$或$x></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/28/卷积神经网络入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/28/卷积神经网络入门/" itemprop="url">卷积神经网络入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-28T22:30:38+08:00">
                2018-06-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-计算机视觉（Computer-Vision）领域介绍"><a href="#1-计算机视觉（Computer-Vision）领域介绍" class="headerlink" title="1. 计算机视觉（Computer Vision）领域介绍"></a>1. 计算机视觉（Computer Vision）领域介绍</h2><p>图片分类(Image Classification)、目标检测(Object detection)、神经风格转换(Neural Style Transfer)。<br>计算机视觉的一大挑战就是输入样本的尺寸可以任意大，进一步导致神经网络的参数很多，容易过拟合并且对计算机的内存和运算速度要求极高。为了解决这个问题，可以引入卷积运算。</p>
<h2 id="2-卷积运算"><a href="#2-卷积运算" class="headerlink" title="2. 卷积运算"></a>2. 卷积运算</h2><h3 id="2-1-一维场合"><a href="#2-1-一维场合" class="headerlink" title="2.1. 一维场合"></a>2.1. 一维场合</h3><p>卷积的一个重要物理意义是：一个函数（如：单位响应）在另一个函数（如：输入信号）上的<strong>加权叠加</strong>。  对于线性时不变系统，如果知道该系统的单位响应，那么将单位响应和输入信号求卷积，就相当于把输入信号的各个时间点的单位响应 加权叠加，就直接得到了输出信号。  形象的物理含义见<a href="https://www.zhihu.com/question/22298352/answer/34267457" target="_blank" rel="noopener">怎样通俗易懂地解释卷积？ - 知乎</a><br>给定一个输入信号序列$x(t), t=1,···,n$，和单位响应序列（有时也称滤波器）$f(t), t=1,···,m$，一般情况下单位响应的长度$m$远小于输入信号长度$n$。  则卷积输出：  </p>
<script type="math/tex; mode=display">
\begin{align*} 
y(t) &=(f*x)(t)\\  &=\sum_{k=1}^{\infty}f(t-k+1)·x(k)) \\ 
&=\sum_{k=1}^{\infty}f(k)·x(t-k+1)) \\  
&=\sum_{k=1}^{m}f(k)·x(t-k+1))\\
\tag{2-1}  
\end{align*}</script><p>在卷积神经网络中，对于不在$[1,n]$范围之内的$x(t)$用零补齐(zero-padding)，输出长度一般为$n+m-1$。此时也称为<strong>宽卷积</strong>。另一类是<strong>窄卷积</strong>，输出长度为$n-m+1$  </p>
<h3 id="2-2-二维场合"><a href="#2-2-二维场合" class="headerlink" title="2.2. 二维场合"></a>2.2. 二维场合</h3><p>二维卷积经常用在图像处理中。给定一个图像$x_{ij}, 1\le i\le M,1\le j\le N$，和滤波器$f_{ij}, 1\le i\le m,1\le j\le n$，一般$m &lt;&lt; M,n &lt;&lt; N$。<br>卷积的输出为：  </p>
<script type="math/tex; mode=display">
\begin{align*} y_{ij}&=\sum_{u=1}^{+\infty}\sum_{v=1}^{+\infty}f_{i-u+1,i-v+1}·x_{u,v}\\  
&=\sum_{u=1}^{m}\sum_{v=1}^{n}f_{u,v}·x_{i-u+1,i-v+1}\\
\tag{2-2}  
\end{align*}</script><p>在图像处理中，常用的均值滤波（mean filter）就是当前位置的像素值设为滤波器窗口中素有像素的平均值，也就是$f_{uv}=\frac{1}{mn}$。<br>上面的运算是信号与系统里面的定义，在实际的操作中通常要将卷积核进行翻转（水平和竖直方向上分别进行一次翻转）再与输入信号进行逐元素相乘（再相加）。但是在深度学习中，简化了翻转的操作，因此其实不适用于上述的公式。深度学习里面的卷积，更严谨的称呼是交叉相关（cross-correlation），但是由于习惯，还是叫做卷积。</p>
<h2 id="3-卷积操作的作用和优点"><a href="#3-卷积操作的作用和优点" class="headerlink" title="3. 卷积操作的作用和优点"></a>3. 卷积操作的作用和优点</h2><h3 id="3-1-参数共享和连接的稀疏性"><a href="#3-1-参数共享和连接的稀疏性" class="headerlink" title="3.1. 参数共享和连接的稀疏性"></a>3.1. 参数共享和连接的稀疏性</h3><p>假设输入图像形状为$32\times 32\times 3$，卷积核形状为$5\times 5\times 6$，则卷积后的图像大小为$28\times 28\times 6$。如果是传统的神经网络，那么参数个数为：$3072\times 4704\approx 14M$，而在卷积神经网络中参数个数为$(5\times 5+1)\times 6=156$<br>卷积核在图像上移动时，参数不变（参数共享）<br>输出图像上的每个像素只来源于上一层图像的一个局部（连接的稀疏性）</p>
<h3 id="3-2-平移不变性"><a href="#3-2-平移不变性" class="headerlink" title="3.2. 平移不变性"></a>3.2. 平移不变性</h3><h3 id="3-2-边缘检测"><a href="#3-2-边缘检测" class="headerlink" title="3.2. 边缘检测"></a>3.2. 边缘检测</h3><h2 id="4-Padding（填充）"><a href="#4-Padding（填充）" class="headerlink" title="4. Padding（填充）"></a>4. Padding（填充）</h2><p>常规卷积操作的后果：  </p>
<ul>
<li>shrinking image没经过一次卷积操作，图片都会缩小 </li>
<li>throw away info from edge（忽视边界信息） ，角落或者边界上的像素被使用的次数比中间的像素少很多</li>
</ul>
<p>记输入图片尺寸为$n\times n$，滤波器大小为$f\times f$，填充的数量为$p$，下面是两种常用的填充方式：</p>
<ul>
<li>“valid”: no padding $n\times n\ *\ f\times f \rightarrow n-f+1\ \times\ n-f+1$  </li>
<li>“Same”: Pad so that output size is the same as the input size $(n+2p-f+1)\times (n+2p-f+1)$， 其中$p=\frac{f-1}{2}$。</li>
</ul>
<p>在计算机视觉领域，f基本上是奇数。因为如果是偶数，需要不对称的填充。而且奇数的滤波器有一个中心，这样可以描述滤波器的位置。$3\times3$的滤波器最常见</p>
<h2 id="5-Strided-Convolutions-带步长的卷积"><a href="#5-Strided-Convolutions-带步长的卷积" class="headerlink" title="5. Strided Convolutions(带步长的卷积)"></a>5. Strided Convolutions(带步长的卷积)</h2><p>假设padding p, stride S，则卷积操作的尺寸运算为：$(n\times n)*(f\times f)\ \rightarrow\ \left(\frac{n+2p-f}{S}+1\right)\times \left(\frac{n+2p-f}{S}+1\right)$<br>如果不能整除，则向下取整：$\lfloor\frac{n+2p-f}{S}+1\rfloor\times \lfloor\frac{n+2p-f}{S}+1\rfloor$ ，这意味着滤波器必须全部落在（填充后的）图像上。</p>
<h2 id="6-对三维图片（RGB）的卷积操作"><a href="#6-对三维图片（RGB）的卷积操作" class="headerlink" title="6. 对三维图片（RGB）的卷积操作"></a>6. 对三维图片（RGB）的卷积操作</h2><p>RGB图像有三个通道（channel），或者叫做深度（depth），因此卷积核也应该有三个通道。卷积核的三个通道分别与RGB图像的三个通道逐元素相乘，再将乘积结果相加，得到卷积后的图像。下图是检测红色垂直边缘（上）和整体图像的垂直边缘（下）的例子：</p>
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Convolutions%20on%20RGB%20image.JPG"></center>
<center>图6.1 RGB图像卷积操作例子</center>
有时为了检测多种类型的边缘，可以用同时多个滤波器对图像进行卷积操作，具体的尺寸运算总结如下：
$n\times n\times n_c\ *\ f\times f\times n_c\ \rightarrow\ n-f+1\times n-f+1\times n_c'$
其中$n_c$代表通道数，通常图像的通道数与滤波器的通道数相等；$n$和$f$分别代表图像和滤波器每个通道的尺寸；$n_c'$代表滤波器的个数
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Multiple%20Filters.JPG"></center>
<center>图6.2 同时用多个卷积核对图像进行卷积操作</center>

<h2 id="7-一层卷积层的例子"><a href="#7-一层卷积层的例子" class="headerlink" title="7. 一层卷积层的例子"></a>7. 一层卷积层的例子</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Example%20of%20a%20convolutional%20layer.JPG"></center></p>
<p><center>图7.1 一层卷积层的例子</center><br>卷积核相关说明：</p>
<script type="math/tex; mode=display">
\begin{align*} f^{[l]}&=filter\ size,\\  
p^{[l]}&=padding\\  s^{[l]}&=stride\\ 
\tag{7-1}
\end{align*}</script><p>卷积核的形状为: </p>
<script type="math/tex; mode=display">f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\tag{7-2}</script><p>输入输出的形状：</p>
<script type="math/tex; mode=display">
\begin{align*} Input&: n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}\\  
Output&: n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}\\  
\tag{7-3}
\end{align*}</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{align*} n_H^{[l]}&=\lfloor\frac{n_H^{[l-1]}+2p^{[l]}-f^{l}}{S^{[l]}}+1\rfloor\\  n_W^{[l]}&=\lfloor\frac{n_W^{[l-1]}+2p^{[l]}-f^{l}}{S^{[l]}}+1\rfloor\\  
\tag{7-4}
\end{align*}</script><p>Activations: $a^{[l]}=n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$<br>Batch or mini batch: $A^{[l]}=m\times n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$<br>Weights: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{l}$<br>bias: $n_c^{[l]}\rightarrow (1,1,1,n_c^{[l]})$</p>
<h2 id="8-卷积神经网络的一个完整例子"><a href="#8-卷积神经网络的一个完整例子" class="headerlink" title="8. 卷积神经网络的一个完整例子"></a>8. 卷积神经网络的一个完整例子</h2><p>一个完整的卷积神经网络模型：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Example%20Convnet.JPG"></center></p>
<p><center>图8.1 一个完整卷积神经网络例子</center><br>在设计卷积神经网络中的许多工作是选择合适的超参数，例如总单元数多少？步长多少？padding多少？使用了多少滤波器等。<br>Types of layer in a convolutional network:  </p>
<ul>
<li>Convolution (CONV)  </li>
<li>Pooling (POOL )  </li>
<li>Fully connected (FC)</li>
</ul>
<h2 id="9-Pooling-layer（池化层）"><a href="#9-Pooling-layer（池化层）" class="headerlink" title="9. Pooling layer（池化层）"></a>9. Pooling layer（池化层）</h2><p>Max pooling  没有参数，因此不需要通过反向传播来学习参数<br>多通道图片时，对每个通道分别进行max pooling<br>pooling层的作用：平移不变性，减少图片尺寸<a href="https://www.zhihu.com/question/36686900/answer/130890492" target="_blank" rel="noopener">pooling层的作用-知乎</a>  </p>
<p>The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input.<br>max pooling比较常用，average pooling不常用<br>hyperparameters:  </p>
<ul>
<li>f: filter size (f=2,s=2,used quite often)  </li>
<li>s: stride  </li>
<li>Max or average pooling<h2 id="10-卷积神经网络经典模型"><a href="#10-卷积神经网络经典模型" class="headerlink" title="10. 卷积神经网络经典模型"></a>10. 卷积神经网络经典模型</h2><h3 id="10-1-Outline"><a href="#10-1-Outline" class="headerlink" title="10.1 Outline"></a>10.1 Outline</h3>经典的网络结构：</li>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG</li>
<li>ResNet</li>
<li>Inception<h3 id="10-1-LeNet-5"><a href="#10-1-LeNet-5" class="headerlink" title="10.1 LeNet-5"></a>10.1 LeNet-5</h3>在计算层数时通常把有参数的算作一层，因为pooling层没有参数，因此conv层和pooling层放在一起当作一层.<br>LetNet-5结构图：<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-5.png"></center>
<center>图10.1 LeNet-5模型结构图</center>
LetNet-5参数表
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-5%E5%8F%82%E6%95%B0.JPG"></center>
<center>图10.2 LetNet-5参数说明</center></li>
<li>用的是sigmoid/tanh激活函数</li>
<li>由于那时的计算机性能比较差，采用了比较复杂的训练技巧<br>注意：</li>
<li>最大池化没有任何参数</li>
<li>卷积层趋向于拥有越来越少的参数，多数参数集中在神经网络的全连接层上</li>
<li>随着神经网络的深入，激活输入大小也逐渐变小。如果减少得太快，通常不利于网络性能<h3 id="10-2-AlexNet"><a href="#10-2-AlexNet" class="headerlink" title="10.2 AlexNet"></a>10.2 AlexNet</h3><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AlexNet.png"></center>
<center>图10.3 AlexNet模型结构图</center></li>
<li>与LSNet-5架构相似，但是规模大许多，约60M个参数</li>
<li>用了ReLU激活函数</li>
<li>由于计算机性能仍然不是很好，采用复杂的训练技巧（将不同的层放在两个GPU上分别训练）</li>
<li>用了Local Response Normalization，但是后面被发现不太有效<h3 id="10-3-VGG-16"><a href="#10-3-VGG-16" class="headerlink" title="10.3 VGG-16"></a>10.3 VGG-16</h3><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/VGG-16.png"></center><br><center>图10.4 VGG-16模型结构图</center><br>138M个参数<br>16是指有16层含有参数的层。<br>前面两层都是64个$3\times 3$的卷积核进行卷积操作。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/25/神经网络之将二分类问题推广到多分类问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/25/神经网络之将二分类问题推广到多分类问题/" itemprop="url">神经网络之将二分类问题推广到多分类问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T16:02:54+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>将神经网络应用到多类分类问题中时，输出层的形式不能用logistic函数（sigmoid激活函数），而应该推广到softmax函数。二分类问题与多分类问题的神经网络模型的最大区别就是输出层。因此下面重点讲解softmax函数的原理。</p>
<h2 id="1-Softmax回归详解"><a href="#1-Softmax回归详解" class="headerlink" title="1. Softmax回归详解"></a>1. Softmax回归详解</h2><p>在softmax回归中，我们解决的是多分类问题（相对于logistic回归解决的二分类问题），标记$y$可以取$k$个不同的值。对于训练集$\{(x^{(1)},y^{(1)}),\cdots,(x^{(m)},y^{(m)})\}$，我们有$y^{(j)}\in \{1,2,\cdots,k\}$。<br>对于给定的测试输入$x$，我们想用假设函数针对每一个类别$j$估算出概率值$P(y=j|x)$。因此，我们的假设函数要输出一个$k$维的向量（向量元素的和为1）来表示$k$个估计的概率值。我们采用如下形式的假设函数$h_{\theta}(x)$：  </p>
<script type="math/tex; mode=display">
\begin{align*}
h_{\theta}(x^{(i)})&=  \begin{bmatrix} P(y^{(i)}=1|x^{(i)};\theta) \\  P(y^{(i)}=2|x^{(i)};\theta) \\  \vdots \\  P(y^{(i)}=10|x^{(i)};\theta)  \end{bmatrix} \\
&=\frac{1}{\sum_{j=1}^ke^{\theta_j^Tx^{(i)}}}  \begin{bmatrix} e^{\theta_1^Tx^{(i)}} \\  e^{\theta_2^Tx^{(i)}} \\  \vdots \\  e^{\theta_k^Tx^{(i)}}  \end{bmatrix} \\
\tag{1-1}  
\end{align*}</script><p>假设输入向量$x$的维数为$n$，则参数$\theta$是一个$k\times (n+1)$的参数矩阵，之所以是$n+1$是因为把截距项$b$表示成了$\theta_0\times x_0$，其中$x_0=1$是一个人工辅助变量。<br>利用极大似然估计的方法，可以得到每一类的后验概率表达式：</p>
<script type="math/tex; mode=display">P(y^{(i)}|x^{(i)};\theta)=\prod_{j=1}^k\left\{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}\right\}^{1(y^{(i)}=j)} \tag{1-2}</script><p>似然函数为：  </p>
<script type="math/tex; mode=display">
\begin{align*} 
L(\theta) &=P(\boldsymbol{Y}|\boldsymbol{X};\theta) \\ 
&=\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta) \\ 
&=\prod_{i=1}^{m}\prod_{j=1}^k\left\{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}\right\}^{1(y^{(i)}=j)}\\
\tag{1-3}  \end{align*}</script><p>对数似然函数为：  </p>
<script type="math/tex; mode=display">
\begin{align*} l(\theta) &=\log L(\theta) \\  
&=\sum_{i=1}^{m}\sum_{j=1}^k1(y^{(i)}=j)\log{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}}\\ 
\tag{1-4}  
\end{align*}</script><p>上面的$(1-4)$就是loss function。<br>cost function为： </p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{j=1}^k1(y^{(i)}=j)\log{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}}\right] \tag{1-5}</script><p>多分类问题的目标就是利用训练数据来训练模型参数$\theta$使其能够最小化$(1-5)$。$(1-5)$是一个凸函数，可以利用梯度下降法得到全局最小值。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/25/创建机器学习项目的注意事项/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/25/创建机器学习项目的注意事项/" itemprop="url">创建机器学习项目的注意事项</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T14:48:06+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Orthogonalization-正交化"><a href="#1-Orthogonalization-正交化" class="headerlink" title="1. Orthogonalization(正交化)"></a>1. Orthogonalization(正交化)</h2><p>正交化的思想就是，每个输入单独控制一个属性，不要让一个输入同时控制多个属性。不同属性的控制是独立的，这样便于调试。<br>如果模型在training set上表现好，在dev set上表现不好，则可以正则化；<br>如果在dev set上表现好，在test set上表现不好，则可以增大dev set的规模；<br>如果在test set上表现好，在真实世界表现不好，则可以改变dev set或者cost function。<br>在神经网络中一般不用early stopping</p>
<h2 id="2-使用单一的量化评价指标"><a href="#2-使用单一的量化评价指标" class="headerlink" title="2. 使用单一的量化评价指标"></a>2. 使用单一的量化评价指标</h2><h3 id="2-1-状态与决策"><a href="#2-1-状态与决策" class="headerlink" title="2.1. 状态与决策"></a>2.1. 状态与决策</h3><ul>
<li>True: 预测正确的样本数  </li>
<li>False: 预测错误的样本数  </li>
<li>Positive: 预测为正样本的样本数  </li>
<li>Negative: 预测为负样本的样本数  <h3 id="2-2-状态与决策的组合"><a href="#2-2-状态与决策的组合" class="headerlink" title="2.2. 状态与决策的组合"></a>2.2. 状态与决策的组合</h3></li>
<li>TP: 将正样本预测为正样本的样本数 （真阳性）  </li>
<li>FP: 将负样本预测为正样本的样本数 （假阳性）  </li>
<li>TN: 将负样本预测为负样本的样本数 （真阴性)  </li>
<li>FN: 将负样本预测为负样本的样本数 （假阳性）<h3 id="2-3-评价指标"><a href="#2-3-评价指标" class="headerlink" title="2.3. 评价指标"></a>2.3. 评价指标</h3></li>
<li>Precision（精确率）：P=TP/(TP+FP)，反映了被分类器判定的正例中，真正的正例样本的比重；</li>
<li>Accuracy（准确率）：A=(TP+TN)/(P+N)=(TP+TN)/(TP+FN+FP+TN)，反映了分类器对整个样本集的判定能力——能将正的判定为正，负的判定为负的能力；</li>
<li>Recall（召回率）：R=TP(TP+FN)=1-FN/T，反映了呗正确判定的正例占总的正例的比重；<br>在实际中， 一般同时用Precision和Recall来综合评价一个分类器，我们希望分类器的这两个指标都很高。<br>采用$F_1$ score来衡量分类器的性能，可以兼顾Precision和Recall。它是Precision和Recall的调和平均数：<script type="math/tex; mode=display">
\begin{align*}
F_1&=\frac{2}{\frac{1}{P}+\frac{1}{R}}\\
&=2\cdot \frac{P\cdot R}{P+R}\\
\tag{2-1}
\end{align*}</script>更为一般地，定义$F_{\beta}$ score：<script type="math/tex; mode=display">
F_{\beta}=(1+\beta^2)\cdot \frac{P\cdot R}{(\beta^2\cdot P)+R} \tag{2-2}</script>一个好的验证机和单一量化评估指标可以提高迭代的效率<br>如果有多个指标（例如准确率和运行时间），选择其中一个加以优化，并使其他指标满足一个阈值，有时候是一个合理的策略。<h3 id="2-3-关于train-dec-test-set"><a href="#2-3-关于train-dec-test-set" class="headerlink" title="2.3. 关于train/dec/test set"></a>2.3. 关于train/dec/test set</h3>三者的分布应该一样，并且都应该对未来的实际数据的分布基本一样。<br>关于三者的尺寸，在传统的机器学习时代，尤其是数据规模不大（万级以下的时候，如果只有train和test，则70%：30%比较合适；如果三者都有，则60%：20%：20%比较合适。在现代机器学习中，数据规模很大，一般都是百万级，那么98%：1%：1%比较合适。<h3 id="2-4-与人类表现比较"><a href="#2-4-与人类表现比较" class="headerlink" title="2.4. 与人类表现比较"></a>2.4. 与人类表现比较</h3>当训练集准确率与人类表现相差比较大时（高偏差），应该优先考虑消除偏差；当偏差小，但是测试集与训练集准确率相差大（高方差）时，应该优先考虑消除高方差。<br>贝叶斯误差是理论上的最小误差，人类表的误差略大于贝叶斯误差，一把可以用人类误差来估计贝叶斯误差。除非过拟合，否则机器学习模型在训练集上的误差不会小于人类误差（贝叶斯误差）<h2 id="3-错误分析"><a href="#3-错误分析" class="headerlink" title="3. 错误分析"></a>3. 错误分析</h2>在实际应用中，人工对分类错误的样本进行统计（记录错误的类别），然后制定相应的改进策略，能够提高模型迭代效率。<br>训练集比较大时，有少量标签错误的样本影响不大，因为深度学习算法对随机错误很稳健，但是对系统误差不那么稳健；在开发集（dev set）中，如果由于标签错误导致的错误率占比较大，则应该考虑去纠正标签错误。<br>dev set和test set必须严格服从相同的分布，而train set的分布可以有稍微的不同。<br>如果你要构建机器学习应用系统，一般可以先建立一个简单的系统，然后采取前面提到的各种方法进行迭代，除非是你经验特别丰富的领域或者学术界有很多参考文献的领域（这个时候你可以直接建立一个复杂的系统）<h2 id="4-train-set和dev-test-set不匹配的问题"><a href="#4-train-set和dev-test-set不匹配的问题" class="headerlink" title="4. train set和dev/test set不匹配的问题"></a>4. train set和dev/test set不匹配的问题</h2>如果你获得了两个分布不同的数据集，一种看似可行的方案是可以把它们混合后随即排列，再用来进行train/dev/test划分。这种方案可能会出现问题，例如当我们关心的那个分布的数据集（记为A，相应地不太关心的数据集记为B）比较小时，会出现dev set中A的比例很小的情况。而dev set是用来选择、迭代模型的，应该对我们关心的数据集更加侧重。因此可以考虑，train set中包含所有的B以及少量的A，而dev set和test set包含剩余的A。<br>当train set与dev set的分布不一样，而两者的错误率相差很大时，不能贸然地下结论说模型的variance很大。此时可以将train set进一步划分为train set和train-dev set，这两个set分布一样，可以用来检验模型的泛化能力。<h2 id="5-从多个任务中学习"><a href="#5-从多个任务中学习" class="headerlink" title="5. 从多个任务中学习"></a>5. 从多个任务中学习</h2>如果你训练好了一个识别猫的网络，想用这个网络进一步得到识别医学影像的网络，可以用少量医学影像的数据来训练该网络的最后一层和输出层，或者用很多医学影像的数据训练网络的所有参数，此时训练识别猫的阶段成为预训练（pre-training），更新网络参数的阶段叫做微调（fine tuning）。<br>迁移学习（transfer learning）可以把一个拥有大量数据的问题模型，迁移到一个先对之下仅有很少数据的问题模型中。<br>迁移学习是有先后顺序的，而多任务学习（multi-task learning）在一开始就尝试让一个神经网络同时做几件事。比如在一张图片中识别是否有车、行人、红绿灯、交通标示牌等。这要比训练多个单独的网络（每个网络解决一个问题）的效果要好。损失函数的形式：<script type="math/tex; mode=display">
\begin{align*}
\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^4L(\hat y_j^{(i)},y_j^{(i)})\\
\tag{5-1}
\end{align*}</script>而且你在标注图片的时候，哪怕图片里面只标了部分任务的标签,多任务学习也可以正常进行下去，因为$(5-1)$只对0或者1的标签进行统计，没有标记的任务可以不统计。<br>目标检测就是多任务学习的一个例子。总体而言，多任务学习比迁移学习的场景要少一些。<h2 id="6-端到端（end-to-end-学习"><a href="#6-端到端（end-to-end-学习" class="headerlink" title="6. 端到端（end-to-end)学习"></a>6. 端到端（end-to-end)学习</h2>端到端学习就是省略了传统的多阶段过程，但是需要大量的数据。如果只有中等的数据，可以考虑折中一下，分阶段进行。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/15/神经网络中的优化方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/15/神经网络中的优化方法/" itemprop="url">神经网络中的优化方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-15T22:25:43+08:00">
                2018-06-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Mini-batch-decent方法"><a href="#1-Mini-batch-decent方法" class="headerlink" title="1. Mini-batch decent方法"></a>1. Mini-batch decent方法</h2><h3 id="1-1-Batch-vs-mini-batch"><a href="#1-1-Batch-vs-mini-batch" class="headerlink" title="1.1. Batch vs. mini-batch"></a>1.1. Batch vs. mini-batch</h3><p>Batch：利用矢量化编程的方法，对整个训练集运用梯度下降法。梯度每下降一小步，都要处理整个训练集。这样的效率比较慢。<br>Mini-batch：将训练集拆分为更小的训练集，成为小批量训练集（mini-batch)<br>Mini-batch t: $X^{\{t\}},Y^{\{t\}}$<br>对每个mini-batch都进行一次完整的前向和反向传播过程，当对所有的mini-batch都进行了前向和反向过程后，我们称完成了对训练集的一次遍历<strong>（epoch）</strong>。<br>Batch gradient descent，原则上cost应该是单调下降（除非learning rate太大了）；Mini-batch gradient descent，整体趋势下降，但是局部是振荡的。</p>
<h3 id="1-2-Choosing-mini-batch-size"><a href="#1-2-Choosing-mini-batch-size" class="headerlink" title="1.2. Choosing mini-batch size"></a>1.2. Choosing mini-batch size</h3><ul>
<li>如果mini-batch size=m： 等价于batch gradient descent，一般可以收敛到全局最小值点；</li>
<li>如果mini-batch size=1：等价于stochastic gradient descent，不一定收敛到全局最小值点，一般会在该点处振荡。<br>如果训练集较小（&lt;2000），就使用batch gradient descent；否则，可以选择64到512之间（2的幂数）的mini-batch size。确保可以放入CPU/GPU的内存中</li>
</ul>
<h2 id="2-指数加权平均方法（exponentially-weighted-averages）"><a href="#2-指数加权平均方法（exponentially-weighted-averages）" class="headerlink" title="2. 指数加权平均方法（exponentially weighted averages）"></a>2. 指数加权平均方法（exponentially weighted averages）</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90-%E5%AF%BB%E6%89%BE%E6%B8%A9%E5%BA%A6%E8%B6%8B%E5%8A%BF.png"></center></p>
<p><center>图2.1 指数加权平均例子-寻找温度趋势</center></p>
<script type="math/tex; mode=display">
\begin{align*}
v_0&=0\\
v_1&=0.9v_0+0.1\theta_1\\
v_2&=0.9v_1+0.1\theta_2\\
v_3&=0.9v_2+0.1\theta_3\\
\vdots\\
\tag{2-1}
\end{align*}</script><p>第t天的指数平均值的通项公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
v_t&=\beta v_{t-1}+(1-\beta)\theta_t \\
&=(1-\beta)\left(\theta_t+\beta\theta_{t-1}+\cdots+\beta^{k}\theta_{t-k}+\cdots+\beta^{t-1}\theta_{1}\right) \\
\tag{2-2}
\end{align*}</script><p>近似公式：</p>
<script type="math/tex; mode=display">
v_t\approx \frac{1}{1-\beta}\ days'\ temperature\tag{2-3}</script><p>如图2.2所示，当$\beta$增大时，曲线向右平移（绿线）；$\beta$减小时，曲线振荡加剧（黄线），</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%CE%B2%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%9B%B2%E7%BA%BF%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D.png"></center></p>
<p><center>图2.2 β大小对曲线形状的影响</center></p>
<h3 id="2-1-Bias-Correction（偏差修正）"><a href="#2-1-Bias-Correction（偏差修正）" class="headerlink" title="2.1. Bias Correction（偏差修正）"></a>2.1. Bias Correction（偏差修正）</h3><p>原因：$v_0=0$导致初始阶段的点估计不准<br>解决方法：用$\frac{v_t}{1-\beta^t}$代替$v_t$</p>
<h2 id="3-Gradient-descent-with-momentum（动量梯度下降）"><a href="#3-Gradient-descent-with-momentum（动量梯度下降）" class="headerlink" title="3. Gradient descent with momentum（动量梯度下降）"></a>3. Gradient descent with momentum（动量梯度下降）</h2><p>背景问题：当目标函数的等高线为图3.1所示时，梯度下降的过程中可能会发生振荡：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%97%B6%E6%8C%AF%E8%8D%A1%E7%9A%84%E4%BE%8B%E5%AD%90.JPG"></center></p>
<p><center>图3.1 梯度下降振荡的例子></center><br>Momentum：<br>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
v_{dw}&=\beta v_{dw}+(1-\beta)dw \\
v_{db}&=\beta v_{db}+(1-\beta)db \\
w&:=w-\alpha v_{dw};\\
b&:=b-\alpha v_{db}\\
\tag{3-1}
\end{align*}</script><p>采用前面提到的指数加权平均可以使梯度的下降过程更平滑。<br>一般$\beta$取0.9就好，而且实际中一般不用修正偏差，因为迭代几步后偏差就自动减小很多了。</p>
<h2 id="4-RMSprop（Root-Mean-Square-prop，均方根传递）"><a href="#4-RMSprop（Root-Mean-Square-prop，均方根传递）" class="headerlink" title="4. RMSprop（Root Mean Square prop，均方根传递）"></a>4. RMSprop（Root Mean Square prop，均方根传递）</h2><p>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
s_{dw}&=\beta s_{dw}+(1-\beta)dw^2 \\
s_{db}&=\beta s_{db}+(1-\beta)db^2 \\
w&:=w-\alpha \frac{dw}{\sqrt{ s_{dw}}};\\
b&:=b-\alpha \frac{db}{\sqrt{ s_{db}}}\\
\tag{4-1}
\end{align*}</script><p>垂直方向除以一个较大的数，水平方向除以一个较小的数（假设b是垂直方向，w是水平方向）。为了防止分母出现零的情况，可以在分母加上一个小的$\epsilon$</p>
<h2 id="5-Adam优化算法"><a href="#5-Adam优化算法" class="headerlink" title="5. Adam优化算法"></a>5. Adam优化算法</h2><p>Adam的本质是将动量和RMSprop结合起来。<br>$v_{dw}=0,s_{dw}=0.v_{db}=0,s_{db}=0.$<br>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
v_{dw}&=\beta_1 v_{dw}+(1-\beta_1)dw \\
v_{db}&=\beta_1 v_{db}+(1-\beta_1)db \\
s_{dw}&=\beta_2 s_{dw}+(1-\beta_2)dw^2 \\
s_{db}&=\beta_2 s_{db}+(1-\beta_2)db^2 \\
V_{dw}^{corrected}&=v_{dw}/\left(1-\beta_1^t\right),V_{db}^{corrected}=v_{db}/\left(1-\beta_1^t\right)\\
S_{dw}^{corrected}&=s_{dw}/\left(1-\beta_2^t\right),S_{db}^{corrected}=s_{db}/\left(1-\beta_2^t\right)\\
w&:=w-\alpha \frac{V_{dw}^{corrected}}{\sqrt{ S_{dw}^{corrected}}};\\
b&:=b-\alpha \frac{V_{db}^{corrected}}{\sqrt{ S_{db}^{corrected}}}\\
\tag{5-1}
\end{align*}</script><p>超参数：<br>$\alpha$:人工调整<br>$\beta_1:0.9$，$(dw)$<br>$\beta_2:0.999$，$(dw^2)$<br>$\epsilon$:$10^{-8}$</p>
<h2 id="6-学习率衰减（learning-rate-decay）"><a href="#6-学习率衰减（learning-rate-decay）" class="headerlink" title="6. 学习率衰减（learning rate decay）"></a>6. 学习率衰减（learning rate decay）</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E9%9A%BE%E4%BB%A5%E7%9C%9F%E6%AD%A3%E6%94%B6%E6%95%9B%E7%9A%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png"></center></p>
<p><center>图6.1 固定学习率导致不能完全收敛的示意图</center><br>解决方法：让学习率$\alpha$逐渐下降。<br>下降的形式：</p>
<ul>
<li>$\alpha=\frac{1}{1+decay-rate\ *\ epoch-num}$</li>
<li>$\alpha=0.95^{epoch-num}\cdot\alpha_0$</li>
<li>$\alpha=\frac{k}{\sqrt epoch-num}\alpha_0$</li>
<li>…</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><a class="extend next" rel="next" href="page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="images/avatar.jpg"
                alt="Guoxing Lan" />
            
              <p class="site-author-name" itemprop="name">Guoxing Lan</p>
              <p class="site-description motion-element" itemprop="description">On The Journey To Truth</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://lankuohsing.github.io/blog/archives">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async="" src="busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guoxing Lan</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
