<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="On The Journey To Truth">
<meta property="og:type" content="website">
<meta property="og:title" content="Guoxing Lan">
<meta property="og:url" content="https://lankuohsing.github.io/blog/index.html">
<meta property="og:site_name" content="Guoxing Lan">
<meta property="og:description" content="On The Journey To Truth">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Guoxing Lan">
<meta name="twitter:description" content="On The Journey To Truth">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lankuohsing.github.io/blog/"/>





  <title>Guoxing Lan</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
	<a href="https://github.com/lankuohsing"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Guoxing Lan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Journey To Truth</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://lankuohsing.github.io/blog" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://lankuohsing.github.io/blog/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://lankuohsing.github.io/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://lankuohsing.github.io/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://lankuohsing.github.io/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/16/统计学之t检验详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/16/统计学之t检验详解/" itemprop="url">统计学之t检验详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-16T00:31:29+08:00">
                2020-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://blog.csdn.net/Tonywu2018/article/details/83897806" target="_blank" rel="noopener">https://blog.csdn.net/Tonywu2018/article/details/83897806</a></p>
<h2 id="0-背景故事"><a href="#0-背景故事" class="headerlink" title="0. 背景故事"></a>0. 背景故事</h2><p>t检验又叫学生t检验（Student‘s t test），它是由20世纪爱尔兰的一家啤酒厂-健力士酒厂的一名员工（戈斯特）采用笔名“Student”发表的学术文章而得名。</p>
<h2 id="1-从一个例子引入t检验的思路"><a href="#1-从一个例子引入t检验的思路" class="headerlink" title="1. 从一个例子引入t检验的思路"></a>1. 从一个例子引入t检验的思路</h2><p>健力士公司是酿啤酒的，啤酒的原材料是麦子，因此公司种了很多麦田。假设有两片麦田，一块采用A工艺（旧）种植，另一块采用B工艺（新）种植。A工艺的麦田平均每株麦子可以结100粒穗子。公司想知道B工艺是否相比A工艺提高了产量。为了节约成本、小小损耗，抠门的资本家老板从B工艺的麦田里随机摘了5株大麦，每株麦子的平均穗子数量为120粒，看起来似乎产量提高了，因为每株麦子的麦穗粒数均值增加了20%。如何确定这样的结论是否可信呢？</p>
<blockquote>
<p>原假设：B工艺没有提高产量，即AB工艺下的每株麦子麦穗数量服从同一个分布<br>备选假设：B工艺提高了产量</p>
</blockquote>
<p>由中心极限定理，A攻一下每株麦穗的粒数服从均值为100，方差未知的正态分布：</p>
<script type="math/tex; mode=display">
X\sim N(\mu,\sigma^2)\tag{1-1}</script><p>B工艺的单株麦穗粒数也可以认为服从正态分布。如果原假设正确的话，B和A服从同样的正态分布。那么这时候我们可以去评估出现5株均值为120的麦穗的概率是否很极端，来判断原假设是否合理。可以对B的每株麦穗数的分布归一化为标准正态分布，再去查表评估其概率值。也即要计算$\frac{\bar x-\mu_0}{\delta_0}$，其中$\bar x$是B工艺的麦穗粒数均值，$\mu_0$为A工艺的麦穗粒数均值，$\delta_0$为A工艺的麦穗粒数均值。由于B工艺是抽取出一定的样本数来计算均值$\bar x$的，因此不能代表总体均值。当样本数很大时，根据大数定理可以直接认为B工艺提高了产量；当样本数很小时，可能是随机误差。因此，不妨对前面的式子再除以一个n相关的数。为此，戈斯特构造了一个新的统计量：</p>
<script type="math/tex; mode=display">
t=\frac{\bar x-\mu_0}{\delta_0/\sqrt n}\tag{1-2}</script><p>该统计量越大，寿命AB工艺导致的差别越大，越有可能说明B工艺提高了产量。</p>
<h2 id="3-t分布"><a href="#3-t分布" class="headerlink" title="3. t分布"></a>3. t分布</h2><p>对于t统计量：<script type="math/tex">t=\frac{\bar x-\mu_0}{\delta_0/\sqrt n}</script>,其对应的概率密度函数也即t分布为：</p>
<script type="math/tex; mode=display">
f(x)=\frac{\Gamma((\nu+1)/2)}{\sqrt(\nu \pi)\Gamma(\nu/2)}(1+t^2/\nu)^{-(\nu+1)/2}\tag{3-1}</script><p>其中$\nu=n-1$称为自由度，$\Gamma(x)=\int_0^{+\infty}t^{x-1}e^{-t}dt(x&gt;0)$是伽马函数。<br>t分布的函数图像与正态分布有点像，给定t值和自由度，可以通过查表的方式去找到对应的P值。t分布表如下：<br><img src="https://img-blog.csdnimg.cn/20200320000514397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="t分布表"></p>
<p>以本文中的例子为例，假设置信水平wie$\alpha=0.05$，查表得T值为2.132（单侧检验）。假设A工艺的标准差为$5\sqrt5$，可计算得出t=4，大于T。因此可以拒绝原假设。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/16/统计学之样本方差与总体方差/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/16/统计学之样本方差与总体方差/" itemprop="url">统计学之样本方差与总体方差</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-16T00:23:00+08:00">
                2020-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考资料：<a href="https://www.cnblogs.com/zzdbullet/p/10087196.html" target="_blank" rel="noopener">https://www.cnblogs.com/zzdbullet/p/10087196.html</a></p>
<h2 id="1-方差（variance）的定义"><a href="#1-方差（variance）的定义" class="headerlink" title="1. 方差（variance）的定义"></a>1. 方差（variance）的定义</h2><p>方差是用来度量随机变量和其数学期望（均值）之间的偏离程度的一个统计量。</p>
<p>统计学中（所有样本）的总体方差公式：</p>
<script type="math/tex; mode=display">
\sigma^2=\frac{\sum(X-\mu)^2}{N} \tag{1-1}</script><p>其中$\sigma^2$是总体方差，$X$是随机变量，$\mu$是总体均值（有时也用$\bar X$表示），$N$是总体样本数。这里提到的样本，是基于样本数量$N$（几乎）无限的假设。对应的各个统计量，也是所有的样本所服从的分布的真实参数，是客观正真实的。</p>
<h2 id="2-样本方差"><a href="#2-样本方差" class="headerlink" title="2. 样本方差"></a>2. 样本方差</h2><p>现实情况中，我们往往得不到所有的无限样本，而只能抽样出一定数量的有限样本。通过有限的样本来计算的方差，称为样本方差，公式如下：</p>
<script type="math/tex; mode=display">
S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar X)^2\tag{2-1}</script><p>注意上式的系数和总体方差公式里面的系数不一样，分母是$n-1$。为什么不用$n$作为分母呢？这是因为如果沿用总体方差的公式得到的样本方差，是对方差的一个有偏估计。用$n$作为分母的样本方差公式，才是对方差的无偏估计。</p>
<h2 id="3-总体方差公式的有偏性证明"><a href="#3-总体方差公式的有偏性证明" class="headerlink" title="3. 总体方差公式的有偏性证明"></a>3. 总体方差公式的有偏性证明</h2><script type="math/tex; mode=display">
\begin{align*}
\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2&=\frac{1}{n}\sum_{i=1}^{n}\left[(X_i-\mu)+(\mu-\bar X)\right]^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2+\frac{2}{n}\sum_{i=1}^{n}(X_i-\mu)(\mu-\bar X)+\frac{1}{n}\sum_{i=1}^{n}(\mu-\bar X)^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2+2(\bar X-\mu)(\mu-\bar X)+(\mu-\bar X)^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2-(\mu-\bar X)^2\\
\tag{3-1}
\end{align*}</script><p>换言之，除非正好有$\bar X=\mu$，否则一定会有</p>
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2<\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2\tag{3-2}</script><p>上式的右边是对方差的正确估计，左边是有偏估计。<br>产生这一偏差的本质是因为均值用的是样本均值$\bar X$。这将导致采样出来的样本之间不是完全相互独立的，自由度从$n$降为了$n-1$。（注意，一个好的采样有两点要求：随机采样，并且样本之间是相互独立的）这是因为，给定$\bar X$和任意$n-1$个样本，就能确定剩下的一个样本，也即只有$n-1$个样本是完全相互独立的，自由度为$n-1$。</p>
<h2 id="4-样本方差公式分母为n-1的推导"><a href="#4-样本方差公式分母为n-1的推导" class="headerlink" title="4. 样本方差公式分母为n-1的推导"></a>4. 样本方差公式分母为n-1的推导</h2><p>在正式推导之前，先给几个公式作为铺垫：</p>
<ol>
<li>方差计算公式：<script type="math/tex; mode=display">
D(X)=E(X^2)-[E(X)]^2\tag{4-1}</script></li>
<li>均值的均值：<script type="math/tex; mode=display">
\begin{align*}
E(\bar X)&=E\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)\\
&=\frac{1}{n}E(\sum_{i=1}^{n}X_i)\\
&=E(X_i)\\
&=\bar X\tag{4-4}
\end{align*}</script></li>
<li>均值的方差<script type="math/tex; mode=display">
\begin{align*}
D(\bar X)&=D\left(\frac{1}{n}\sum_{i=1}^nX_i\right)\\
&=\frac{1}{n^2}D(\sum_{i=1}^{n}X_i)\\
&=\frac{1}{n}D(X_i)\\
\tag{4-5}
\end{align*}</script></li>
</ol>
<p>对于没有修正的方差计算公式，计算其期望：</p>
<script type="math/tex; mode=display">
\begin{align*}
E(S^2)&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-\frac{2}{n}(X_i)(\bar X)+\frac{1}{n}\sum_{i=1}^{n}(\bar X)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-2(\bar X)^2+(\bar X)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-(\bar X)^2\right)\\
&=E((X_i)^2)-E((\bar X)^2)\\
&=D(X_i)+\left(E(X_i)\right)^2-\left(D(\bar X)+\left(E(\bar X)\right)^2\right)
\tag{4-6}
\end{align*}</script><p>结合{4-4}和{4-5}，可将{4-6}化简为</p>
<script type="math/tex; mode=display">
\begin{align*}
E(S^2)&=D(X_i)-\frac{1}{n}D(X_i)\\
&=\frac{n-1}{n}D(X_i)\\
&=\frac{n-1}{n}\sigma^2\\
\tag{4-7}
\end{align*}</script><p>要使样本方差的期望等于总体方差，就需要进行修正，也即给样本方差乘上$\frac{n}{n-1}$<br>因此得到修正后的样本方差公式:</p>
<script type="math/tex; mode=display">
\begin{align*}
S^2&=\frac{n}{n-1}\left(\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2\right)\\
&=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2\\
\tag{4-8}
\end{align*}</script><p>推导完毕！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/15/统计学之假设检验详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/15/统计学之假设检验详解/" itemprop="url">统计学之假设检验详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-15T23:37:41+08:00">
                2020-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://cosx.org/2010/11/hypotheses-testing/" target="_blank" rel="noopener">https://cosx.org/2010/11/hypotheses-testing/</a></p>
<h2 id="0-背景"><a href="#0-背景" class="headerlink" title="0. 背景"></a>0. 背景</h2><p>在实际生产生活中，我们经常需要对一些逻辑推理进行真假判断，例如</p>
<blockquote>
<p>如果你打了某种疫苗P，就不会得某种流行病Q<br>如果一个疑似病人隔离了14天还没确诊，那他就没有被感染新冠肺炎</p>
</blockquote>
<p>在统计学里面，不会像上面那样说，而是会说：</p>
<blockquote>
<p>如果你打了某种疫苗 ，就有95%的把握不会得流行病Q<br>如果一个疑似病人隔离了14天还没确诊，那他就有95%的把握没有被感染新冠肺炎</p>
</blockquote>
<p>其中的把握水平，在统计推断中用“置信水平”来代替。置信水平是可以人为选取的。</p>
<h2 id="1-从一个硬币的例子来引入假设检验"><a href="#1-从一个硬币的例子来引入假设检验" class="headerlink" title="1. 从一个硬币的例子来引入假设检验"></a>1. 从一个硬币的例子来引入假设检验</h2><p>如何从统计推断的角度来判断一个逻辑推理是否正确呢？通常，我们会给定一个置信水平，然后判断该逻辑推理是否在这个置信水平下成立。这里重新举一个硬币的例子，来引入置信水平的概念。<br>假设有如下命题：</p>
<blockquote>
<p>if P then Q<br>P: 在 100 次投掷中，得到 90 次正面，10 次反面。<br>Q: 硬币不是均匀的。</p>
</blockquote>
<p>我们想知道，如果P成立，判断Q成立的把握有多大。很多时候（但不是所有时候），在统计推断里面，要证明的结论都是直觉上可能性比较大的，直接证明可能不太方便，可以反其道行之，证明Q的反面是否成立，来推断出Q是否成立。为此，列出如下原假设和备择假设：</p>
<blockquote>
<p>H0: 硬币是均匀的（P）<br>Ha: 硬币是不均匀的（not P）</p>
</blockquote>
<p>如果原假设为真，即硬币是均匀的，就不可能会发生这样极端的事情比如：在 100 次投掷中，得到 90 次正面，10 次反面。如果真的观察到了这么极端的事情，就有把握认为硬币不是均匀的，则拒绝原假设，选择备选假设。如果观察到的是60次正面，40个反面，则没有特别大的把握拒绝原假设，这枚硬币是否有偏，需要更多的证据来证明（这通常意味着做更多的实验，比如再投1000次）。</p>
<p>即使观测到100次投掷中90次正面10次反面，也不能说硬币一定是不均匀的（也即不能百分之百的把握拒绝原假设）。如果原假设为真，但是拒绝了原假设，这种情况称为<strong>第一类错误</strong>。发生第一类错误的概率，称为<strong>显著性水平</strong>，用$\alpha$表示。$1-\alpha$称为<strong>置信度</strong>或者<strong>置信水平</strong>，它表示我们根据抽样样本对总体参数的估计的可靠性。$\alpha$一般是人为定的，如0.05，0.01.给定置信水平后，就可以去利用一些统计学的知识去检验原假设是否需要拒绝。<br>如果原假设是错误的，但是没有拒绝原假设，则称为<strong>第二类错误</strong>。如果要求犯第一类错误的概率尽可能小，就会导致第二类错误的概率增大；反之，如果要求第二类错误的Giallo极可能小，就会导致第一类错误的概率增大。在实际中需要权衡。权衡的方式就是调节$\alpha$。在实际中，我们通常认为犯第一类错误的后果比犯第二类错误的后果更为严重。例如，关于打疫苗会后会不会得病的命题，我们通常会将原假设写成：会得病，然后去搜集数据试图拒绝原假设。此时犯第一类错误的后果是比较严重的（实际会得病却认为不会得病，会放松警惕造成大流行），而犯第二类错误的后果不是很严重（实际不糊得病，却没有拒绝原假设，只是会将打疫苗的部分人隔离起来造成一定的不便）</p>
<p>再强调一下，一般都是先提出需要建议的假设，再搜集数据，这是统计推断的原则之一。因为如果现有了数据再提出假设，容易有主观干扰。<br>到这里，我们还是没有解答如何去检验原假设是否需要被拒绝。别急，接着往下看。</p>
<h2 id="2-P值"><a href="#2-P值" class="headerlink" title="2. P值"></a>2. P值</h2><p>如何去定义一个事件是否“极端”呢？首先我们引入“更极端”的概念。更极端，意味着概率更小。例如，91次正面9次反面，比90次正面10次反面，更为极端。因此，很自然地，我们只需要描述出原假设为真，第一类错误恰好为$\alpha$时的事件，然后判断出当前样本集合里面的事件是否比它更极端，就能判断是否要在当前显著性水平下拒绝原假设了。当然，直接这样比较麻烦，可以转换一下思路：计算出发生比当前事件（90次正面，10次反面）更极端的事件的概率P，判断P与$\alpha$的大小，如果$P&lt;\alpha$，则说明如果原假设为真时，发生当前事件的概率很极端（比我们给定的显著性水平$\alpha$还低），因此说明原假设不合理，于是可以拒绝原假设了。此时发生第一类错误的概率小于$\alpha$。这里的概率P，称为<strong>P值</strong>。<br>在硬币投掷实验中，正面出现的次数服$X$服从一个二项分布：$X\sim B(n,p)$，其中$n=100,p-0.5$。根据中心极限定理，二项分布的极限分布是正态分布，因此可以由均值为$np=50$，方差为$np(1-p)=25$的正态分布来近似。我们用这个近似的正态分布的两端去考察所谓“更极端”的事件。取$\alpha=0.05$，由正态分布的性质不难得到，$P$值等于$X<10$或$x>90$的概率值，等于$2\times P(X&lt;10)=1.2442e-15$。这个小于我们给定的$\alpha$，因此该事件很极端，原假设不合理，拒绝原假设。</10$或$x></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/28/卷积神经网络入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/28/卷积神经网络入门/" itemprop="url">卷积神经网络入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-28T22:30:38+08:00">
                2018-06-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-计算机视觉（Computer-Vision）领域介绍"><a href="#1-计算机视觉（Computer-Vision）领域介绍" class="headerlink" title="1. 计算机视觉（Computer Vision）领域介绍"></a>1. 计算机视觉（Computer Vision）领域介绍</h2><p>图片分类(Image Classification)、目标检测(Object detection)、神经风格转换(Neural Style Transfer)。<br>计算机视觉的一大挑战就是输入样本的尺寸可以任意大，进一步导致神经网络的参数很多，容易过拟合并且对计算机的内存和运算速度要求极高。为了解决这个问题，可以引入卷积运算。</p>
<h2 id="2-卷积运算"><a href="#2-卷积运算" class="headerlink" title="2. 卷积运算"></a>2. 卷积运算</h2><h3 id="2-1-一维场合"><a href="#2-1-一维场合" class="headerlink" title="2.1. 一维场合"></a>2.1. 一维场合</h3><p>卷积的一个重要物理意义是：一个函数（如：单位响应）在另一个函数（如：输入信号）上的<strong>加权叠加</strong>。  对于线性时不变系统，如果知道该系统的单位响应，那么将单位响应和输入信号求卷积，就相当于把输入信号的各个时间点的单位响应 加权叠加，就直接得到了输出信号。  形象的物理含义见<a href="https://www.zhihu.com/question/22298352/answer/34267457" target="_blank" rel="noopener">怎样通俗易懂地解释卷积？ - 知乎</a><br>给定一个输入信号序列$x(t), t=1,···,n$，和单位响应序列（有时也称滤波器）$f(t), t=1,···,m$，一般情况下单位响应的长度$m$远小于输入信号长度$n$。  则卷积输出：  </p>
<script type="math/tex; mode=display">
\begin{align*} 
y(t) &=(f*x)(t)\\  &=\sum_{k=1}^{\infty}f(t-k+1)·x(k)) \\ 
&=\sum_{k=1}^{\infty}f(k)·x(t-k+1)) \\  
&=\sum_{k=1}^{m}f(k)·x(t-k+1))\\
\tag{2-1}  
\end{align*}</script><p>在卷积神经网络中，对于不在$[1,n]$范围之内的$x(t)$用零补齐(zero-padding)，输出长度一般为$n+m-1$。此时也称为<strong>宽卷积</strong>。另一类是<strong>窄卷积</strong>，输出长度为$n-m+1$  </p>
<h3 id="2-2-二维场合"><a href="#2-2-二维场合" class="headerlink" title="2.2. 二维场合"></a>2.2. 二维场合</h3><p>二维卷积经常用在图像处理中。给定一个图像$x_{ij}, 1\le i\le M,1\le j\le N$，和滤波器$f_{ij}, 1\le i\le m,1\le j\le n$，一般$m &lt;&lt; M,n &lt;&lt; N$。<br>卷积的输出为：  </p>
<script type="math/tex; mode=display">
\begin{align*} y_{ij}&=\sum_{u=1}^{+\infty}\sum_{v=1}^{+\infty}f_{i-u+1,i-v+1}·x_{u,v}\\  
&=\sum_{u=1}^{m}\sum_{v=1}^{n}f_{u,v}·x_{i-u+1,i-v+1}\\
\tag{2-2}  
\end{align*}</script><p>在图像处理中，常用的均值滤波（mean filter）就是当前位置的像素值设为滤波器窗口中素有像素的平均值，也就是$f_{uv}=\frac{1}{mn}$。<br>上面的运算是信号与系统里面的定义，在实际的操作中通常要将卷积核进行翻转（水平和竖直方向上分别进行一次翻转）再与输入信号进行逐元素相乘（再相加）。但是在深度学习中，简化了翻转的操作，因此其实不适用于上述的公式。深度学习里面的卷积，更严谨的称呼是交叉相关（cross-correlation），但是由于习惯，还是叫做卷积。</p>
<h2 id="3-卷积操作的作用和优点"><a href="#3-卷积操作的作用和优点" class="headerlink" title="3. 卷积操作的作用和优点"></a>3. 卷积操作的作用和优点</h2><h3 id="3-1-参数共享和连接的稀疏性"><a href="#3-1-参数共享和连接的稀疏性" class="headerlink" title="3.1. 参数共享和连接的稀疏性"></a>3.1. 参数共享和连接的稀疏性</h3><p>假设输入图像形状为$32\times 32\times 3$，卷积核形状为$5\times 5\times 6$，则卷积后的图像大小为$28\times 28\times 6$。如果是传统的神经网络，那么参数个数为：$3072\times 4704\approx 14M$，而在卷积神经网络中参数个数为$(5\times 5+1)\times 6=156$<br>卷积核在图像上移动时，参数不变（参数共享）<br>输出图像上的每个像素只来源于上一层图像的一个局部（连接的稀疏性）</p>
<h3 id="3-2-平移不变性"><a href="#3-2-平移不变性" class="headerlink" title="3.2. 平移不变性"></a>3.2. 平移不变性</h3><h3 id="3-2-边缘检测"><a href="#3-2-边缘检测" class="headerlink" title="3.2. 边缘检测"></a>3.2. 边缘检测</h3><h2 id="4-Padding（填充）"><a href="#4-Padding（填充）" class="headerlink" title="4. Padding（填充）"></a>4. Padding（填充）</h2><p>常规卷积操作的后果：  </p>
<ul>
<li>shrinking image没经过一次卷积操作，图片都会缩小 </li>
<li>throw away info from edge（忽视边界信息） ，角落或者边界上的像素被使用的次数比中间的像素少很多</li>
</ul>
<p>记输入图片尺寸为$n\times n$，滤波器大小为$f\times f$，填充的数量为$p$，下面是两种常用的填充方式：</p>
<ul>
<li>“valid”: no padding $n\times n\ *\ f\times f \rightarrow n-f+1\ \times\ n-f+1$  </li>
<li>“Same”: Pad so that output size is the same as the input size $(n+2p-f+1)\times (n+2p-f+1)$， 其中$p=\frac{f-1}{2}$。</li>
</ul>
<p>在计算机视觉领域，f基本上是奇数。因为如果是偶数，需要不对称的填充。而且奇数的滤波器有一个中心，这样可以描述滤波器的位置。$3\times3$的滤波器最常见</p>
<h2 id="5-Strided-Convolutions-带步长的卷积"><a href="#5-Strided-Convolutions-带步长的卷积" class="headerlink" title="5. Strided Convolutions(带步长的卷积)"></a>5. Strided Convolutions(带步长的卷积)</h2><p>假设padding p, stride S，则卷积操作的尺寸运算为：$(n\times n)*(f\times f)\ \rightarrow\ \left(\frac{n+2p-f}{S}+1\right)\times \left(\frac{n+2p-f}{S}+1\right)$<br>如果不能整除，则向下取整：$\lfloor\frac{n+2p-f}{S}+1\rfloor\times \lfloor\frac{n+2p-f}{S}+1\rfloor$ ，这意味着滤波器必须全部落在（填充后的）图像上。</p>
<h2 id="6-对三维图片（RGB）的卷积操作"><a href="#6-对三维图片（RGB）的卷积操作" class="headerlink" title="6. 对三维图片（RGB）的卷积操作"></a>6. 对三维图片（RGB）的卷积操作</h2><p>RGB图像有三个通道（channel），或者叫做深度（depth），因此卷积核也应该有三个通道。卷积核的三个通道分别与RGB图像的三个通道逐元素相乘，再将乘积结果相加，得到卷积后的图像。下图是检测红色垂直边缘（上）和整体图像的垂直边缘（下）的例子：</p>
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Convolutions%20on%20RGB%20image.JPG"></center>
<center>图6.1 RGB图像卷积操作例子</center>
有时为了检测多种类型的边缘，可以用同时多个滤波器对图像进行卷积操作，具体的尺寸运算总结如下：
$n\times n\times n_c\ *\ f\times f\times n_c\ \rightarrow\ n-f+1\times n-f+1\times n_c'$
其中$n_c$代表通道数，通常图像的通道数与滤波器的通道数相等；$n$和$f$分别代表图像和滤波器每个通道的尺寸；$n_c'$代表滤波器的个数
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Multiple%20Filters.JPG"></center>
<center>图6.2 同时用多个卷积核对图像进行卷积操作</center>

<h2 id="7-一层卷积层的例子"><a href="#7-一层卷积层的例子" class="headerlink" title="7. 一层卷积层的例子"></a>7. 一层卷积层的例子</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Example%20of%20a%20convolutional%20layer.JPG"></center></p>
<p><center>图7.1 一层卷积层的例子</center><br>卷积核相关说明：</p>
<script type="math/tex; mode=display">
\begin{align*} f^{[l]}&=filter\ size,\\  
p^{[l]}&=padding\\  s^{[l]}&=stride\\ 
\tag{7-1}
\end{align*}</script><p>卷积核的形状为: </p>
<script type="math/tex; mode=display">f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\tag{7-2}</script><p>输入输出的形状：</p>
<script type="math/tex; mode=display">
\begin{align*} Input&: n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}\\  
Output&: n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}\\  
\tag{7-3}
\end{align*}</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{align*} n_H^{[l]}&=\lfloor\frac{n_H^{[l-1]}+2p^{[l]}-f^{l}}{S^{[l]}}+1\rfloor\\  n_W^{[l]}&=\lfloor\frac{n_W^{[l-1]}+2p^{[l]}-f^{l}}{S^{[l]}}+1\rfloor\\  
\tag{7-4}
\end{align*}</script><p>Activations: $a^{[l]}=n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$<br>Batch or mini batch: $A^{[l]}=m\times n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$<br>Weights: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{l}$<br>bias: $n_c^{[l]}\rightarrow (1,1,1,n_c^{[l]})$</p>
<h2 id="8-卷积神经网络的一个完整例子"><a href="#8-卷积神经网络的一个完整例子" class="headerlink" title="8. 卷积神经网络的一个完整例子"></a>8. 卷积神经网络的一个完整例子</h2><p>一个完整的卷积神经网络模型：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Example%20Convnet.JPG"></center></p>
<p><center>图8.1 一个完整卷积神经网络例子</center><br>在设计卷积神经网络中的许多工作是选择合适的超参数，例如总单元数多少？步长多少？padding多少？使用了多少滤波器等。<br>Types of layer in a convolutional network:  </p>
<ul>
<li>Convolution (CONV)  </li>
<li>Pooling (POOL )  </li>
<li>Fully connected (FC)</li>
</ul>
<h2 id="9-Pooling-layer（池化层）"><a href="#9-Pooling-layer（池化层）" class="headerlink" title="9. Pooling layer（池化层）"></a>9. Pooling layer（池化层）</h2><p>Max pooling  没有参数，因此不需要通过反向传播来学习参数<br>多通道图片时，对每个通道分别进行max pooling<br>pooling层的作用：平移不变性，减少图片尺寸<a href="https://www.zhihu.com/question/36686900/answer/130890492" target="_blank" rel="noopener">pooling层的作用-知乎</a>  </p>
<p>The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input.<br>max pooling比较常用，average pooling不常用<br>hyperparameters:  </p>
<ul>
<li>f: filter size (f=2,s=2,used quite often)  </li>
<li>s: stride  </li>
<li>Max or average pooling<h2 id="10-卷积神经网络经典模型"><a href="#10-卷积神经网络经典模型" class="headerlink" title="10. 卷积神经网络经典模型"></a>10. 卷积神经网络经典模型</h2><h3 id="10-1-Outline"><a href="#10-1-Outline" class="headerlink" title="10.1 Outline"></a>10.1 Outline</h3>经典的网络结构：</li>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG</li>
<li>ResNet</li>
<li>Inception<h3 id="10-1-LeNet-5"><a href="#10-1-LeNet-5" class="headerlink" title="10.1 LeNet-5"></a>10.1 LeNet-5</h3>在计算层数时通常把有参数的算作一层，因为pooling层没有参数，因此conv层和pooling层放在一起当作一层.<br>LetNet-5结构图：<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-5.png"></center>
<center>图10.1 LeNet-5模型结构图</center>
LetNet-5参数表
<center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LeNet-5%E5%8F%82%E6%95%B0.JPG"></center>
<center>图10.2 LetNet-5参数说明</center></li>
<li>用的是sigmoid/tanh激活函数</li>
<li>由于那时的计算机性能比较差，采用了比较复杂的训练技巧<br>注意：</li>
<li>最大池化没有任何参数</li>
<li>卷积层趋向于拥有越来越少的参数，多数参数集中在神经网络的全连接层上</li>
<li>随着神经网络的深入，激活输入大小也逐渐变小。如果减少得太快，通常不利于网络性能<h3 id="10-2-AlexNet"><a href="#10-2-AlexNet" class="headerlink" title="10.2 AlexNet"></a>10.2 AlexNet</h3><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AlexNet.png"></center>
<center>图10.3 AlexNet模型结构图</center></li>
<li>与LSNet-5架构相似，但是规模大许多，约60M个参数</li>
<li>用了ReLU激活函数</li>
<li>由于计算机性能仍然不是很好，采用复杂的训练技巧（将不同的层放在两个GPU上分别训练）</li>
<li>用了Local Response Normalization，但是后面被发现不太有效<h3 id="10-3-VGG-16"><a href="#10-3-VGG-16" class="headerlink" title="10.3 VGG-16"></a>10.3 VGG-16</h3><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/VGG-16.png"></center><br><center>图10.4 VGG-16模型结构图</center><br>138M个参数<br>16是指有16层含有参数的层。<br>前面两层都是64个$3\times 3$的卷积核进行卷积操作。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/25/神经网络之将二分类问题推广到多分类问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/25/神经网络之将二分类问题推广到多分类问题/" itemprop="url">神经网络之将二分类问题推广到多分类问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T16:02:54+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>将神经网络应用到多类分类问题中时，输出层的形式不能用logistic函数（sigmoid激活函数），而应该推广到softmax函数。二分类问题与多分类问题的神经网络模型的最大区别就是输出层。因此下面重点讲解softmax函数的原理。</p>
<h2 id="1-Softmax回归详解"><a href="#1-Softmax回归详解" class="headerlink" title="1. Softmax回归详解"></a>1. Softmax回归详解</h2><p>在softmax回归中，我们解决的是多分类问题（相对于logistic回归解决的二分类问题），标记$y$可以取$k$个不同的值。对于训练集$\{(x^{(1)},y^{(1)}),\cdots,(x^{(m)},y^{(m)})\}$，我们有$y^{(j)}\in \{1,2,\cdots,k\}$。<br>对于给定的测试输入$x$，我们想用假设函数针对每一个类别$j$估算出概率值$P(y=j|x)$。因此，我们的假设函数要输出一个$k$维的向量（向量元素的和为1）来表示$k$个估计的概率值。我们采用如下形式的假设函数$h_{\theta}(x)$：  </p>
<script type="math/tex; mode=display">
\begin{align*}
h_{\theta}(x^{(i)})&=  \begin{bmatrix} P(y^{(i)}=1|x^{(i)};\theta) \\  P(y^{(i)}=2|x^{(i)};\theta) \\  \vdots \\  P(y^{(i)}=10|x^{(i)};\theta)  \end{bmatrix} \\
&=\frac{1}{\sum_{j=1}^ke^{\theta_j^Tx^{(i)}}}  \begin{bmatrix} e^{\theta_1^Tx^{(i)}} \\  e^{\theta_2^Tx^{(i)}} \\  \vdots \\  e^{\theta_k^Tx^{(i)}}  \end{bmatrix} \\
\tag{1-1}  
\end{align*}</script><p>假设输入向量$x$的维数为$n$，则参数$\theta$是一个$k\times (n+1)$的参数矩阵，之所以是$n+1$是因为把截距项$b$表示成了$\theta_0\times x_0$，其中$x_0=1$是一个人工辅助变量。<br>利用极大似然估计的方法，可以得到每一类的后验概率表达式：</p>
<script type="math/tex; mode=display">P(y^{(i)}|x^{(i)};\theta)=\prod_{j=1}^k\left\{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}\right\}^{1(y^{(i)}=j)} \tag{1-2}</script><p>似然函数为：  </p>
<script type="math/tex; mode=display">
\begin{align*} 
L(\theta) &=P(\boldsymbol{Y}|\boldsymbol{X};\theta) \\ 
&=\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta) \\ 
&=\prod_{i=1}^{m}\prod_{j=1}^k\left\{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}\right\}^{1(y^{(i)}=j)}\\
\tag{1-3}  \end{align*}</script><p>对数似然函数为：  </p>
<script type="math/tex; mode=display">
\begin{align*} l(\theta) &=\log L(\theta) \\  
&=\sum_{i=1}^{m}\sum_{j=1}^k1(y^{(i)}=j)\log{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}}\\ 
\tag{1-4}  
\end{align*}</script><p>上面的$(1-4)$就是loss function。<br>cost function为： </p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{j=1}^k1(y^{(i)}=j)\log{\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{l=1}^ke^{\theta_l^Tx^{(i)}}}}\right] \tag{1-5}</script><p>多分类问题的目标就是利用训练数据来训练模型参数$\theta$使其能够最小化$(1-5)$。$(1-5)$是一个凸函数，可以利用梯度下降法得到全局最小值。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/25/创建机器学习项目的注意事项/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/25/创建机器学习项目的注意事项/" itemprop="url">创建机器学习项目的注意事项</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T14:48:06+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Orthogonalization-正交化"><a href="#1-Orthogonalization-正交化" class="headerlink" title="1. Orthogonalization(正交化)"></a>1. Orthogonalization(正交化)</h2><p>正交化的思想就是，每个输入单独控制一个属性，不要让一个输入同时控制多个属性。不同属性的控制是独立的，这样便于调试。<br>如果模型在training set上表现好，在dev set上表现不好，则可以正则化；<br>如果在dev set上表现好，在test set上表现不好，则可以增大dev set的规模；<br>如果在test set上表现好，在真实世界表现不好，则可以改变dev set或者cost function。<br>在神经网络中一般不用early stopping</p>
<h2 id="2-使用单一的量化评价指标"><a href="#2-使用单一的量化评价指标" class="headerlink" title="2. 使用单一的量化评价指标"></a>2. 使用单一的量化评价指标</h2><h3 id="2-1-状态与决策"><a href="#2-1-状态与决策" class="headerlink" title="2.1. 状态与决策"></a>2.1. 状态与决策</h3><ul>
<li>True: 预测正确的样本数  </li>
<li>False: 预测错误的样本数  </li>
<li>Positive: 预测为正样本的样本数  </li>
<li>Negative: 预测为负样本的样本数  <h3 id="2-2-状态与决策的组合"><a href="#2-2-状态与决策的组合" class="headerlink" title="2.2. 状态与决策的组合"></a>2.2. 状态与决策的组合</h3></li>
<li>TP: 将正样本预测为正样本的样本数 （真阳性）  </li>
<li>FP: 将负样本预测为正样本的样本数 （假阳性）  </li>
<li>TN: 将负样本预测为负样本的样本数 （真阴性)  </li>
<li>FN: 将负样本预测为负样本的样本数 （假阳性）<h3 id="2-3-评价指标"><a href="#2-3-评价指标" class="headerlink" title="2.3. 评价指标"></a>2.3. 评价指标</h3></li>
<li>Precision（精确率）：P=TP/(TP+FP)，反映了被分类器判定的正例中，真正的正例样本的比重；</li>
<li>Accuracy（准确率）：A=(TP+TN)/(P+N)=(TP+TN)/(TP+FN+FP+TN)，反映了分类器对整个样本集的判定能力——能将正的判定为正，负的判定为负的能力；</li>
<li>Recall（召回率）：R=TP(TP+FN)=1-FN/T，反映了呗正确判定的正例占总的正例的比重；<br>在实际中， 一般同时用Precision和Recall来综合评价一个分类器，我们希望分类器的这两个指标都很高。<br>采用$F_1$ score来衡量分类器的性能，可以兼顾Precision和Recall。它是Precision和Recall的调和平均数：<script type="math/tex; mode=display">
\begin{align*}
F_1&=\frac{2}{\frac{1}{P}+\frac{1}{R}}\\
&=2\cdot \frac{P\cdot R}{P+R}\\
\tag{2-1}
\end{align*}</script>更为一般地，定义$F_{\beta}$ score：<script type="math/tex; mode=display">
F_{\beta}=(1+\beta^2)\cdot \frac{P\cdot R}{(\beta^2\cdot P)+R} \tag{2-2}</script>一个好的验证机和单一量化评估指标可以提高迭代的效率<br>如果有多个指标（例如准确率和运行时间），选择其中一个加以优化，并使其他指标满足一个阈值，有时候是一个合理的策略。<h3 id="2-3-关于train-dec-test-set"><a href="#2-3-关于train-dec-test-set" class="headerlink" title="2.3. 关于train/dec/test set"></a>2.3. 关于train/dec/test set</h3>三者的分布应该一样，并且都应该对未来的实际数据的分布基本一样。<br>关于三者的尺寸，在传统的机器学习时代，尤其是数据规模不大（万级以下的时候，如果只有train和test，则70%：30%比较合适；如果三者都有，则60%：20%：20%比较合适。在现代机器学习中，数据规模很大，一般都是百万级，那么98%：1%：1%比较合适。<h3 id="2-4-与人类表现比较"><a href="#2-4-与人类表现比较" class="headerlink" title="2.4. 与人类表现比较"></a>2.4. 与人类表现比较</h3>当训练集准确率与人类表现相差比较大时（高偏差），应该优先考虑消除偏差；当偏差小，但是测试集与训练集准确率相差大（高方差）时，应该优先考虑消除高方差。<br>贝叶斯误差是理论上的最小误差，人类表的误差略大于贝叶斯误差，一把可以用人类误差来估计贝叶斯误差。除非过拟合，否则机器学习模型在训练集上的误差不会小于人类误差（贝叶斯误差）<h2 id="3-错误分析"><a href="#3-错误分析" class="headerlink" title="3. 错误分析"></a>3. 错误分析</h2>在实际应用中，人工对分类错误的样本进行统计（记录错误的类别），然后制定相应的改进策略，能够提高模型迭代效率。<br>训练集比较大时，有少量标签错误的样本影响不大，因为深度学习算法对随机错误很稳健，但是对系统误差不那么稳健；在开发集（dev set）中，如果由于标签错误导致的错误率占比较大，则应该考虑去纠正标签错误。<br>dev set和test set必须严格服从相同的分布，而train set的分布可以有稍微的不同。<br>如果你要构建机器学习应用系统，一般可以先建立一个简单的系统，然后采取前面提到的各种方法进行迭代，除非是你经验特别丰富的领域或者学术界有很多参考文献的领域（这个时候你可以直接建立一个复杂的系统）<h2 id="4-train-set和dev-test-set不匹配的问题"><a href="#4-train-set和dev-test-set不匹配的问题" class="headerlink" title="4. train set和dev/test set不匹配的问题"></a>4. train set和dev/test set不匹配的问题</h2>如果你获得了两个分布不同的数据集，一种看似可行的方案是可以把它们混合后随即排列，再用来进行train/dev/test划分。这种方案可能会出现问题，例如当我们关心的那个分布的数据集（记为A，相应地不太关心的数据集记为B）比较小时，会出现dev set中A的比例很小的情况。而dev set是用来选择、迭代模型的，应该对我们关心的数据集更加侧重。因此可以考虑，train set中包含所有的B以及少量的A，而dev set和test set包含剩余的A。<br>当train set与dev set的分布不一样，而两者的错误率相差很大时，不能贸然地下结论说模型的variance很大。此时可以将train set进一步划分为train set和train-dev set，这两个set分布一样，可以用来检验模型的泛化能力。<h2 id="5-从多个任务中学习"><a href="#5-从多个任务中学习" class="headerlink" title="5. 从多个任务中学习"></a>5. 从多个任务中学习</h2>如果你训练好了一个识别猫的网络，想用这个网络进一步得到识别医学影像的网络，可以用少量医学影像的数据来训练该网络的最后一层和输出层，或者用很多医学影像的数据训练网络的所有参数，此时训练识别猫的阶段成为预训练（pre-training），更新网络参数的阶段叫做微调（fine tuning）。<br>迁移学习（transfer learning）可以把一个拥有大量数据的问题模型，迁移到一个先对之下仅有很少数据的问题模型中。<br>迁移学习是有先后顺序的，而多任务学习（multi-task learning）在一开始就尝试让一个神经网络同时做几件事。比如在一张图片中识别是否有车、行人、红绿灯、交通标示牌等。这要比训练多个单独的网络（每个网络解决一个问题）的效果要好。损失函数的形式：<script type="math/tex; mode=display">
\begin{align*}
\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^4L(\hat y_j^{(i)},y_j^{(i)})\\
\tag{5-1}
\end{align*}</script>而且你在标注图片的时候，哪怕图片里面只标了部分任务的标签,多任务学习也可以正常进行下去，因为$(5-1)$只对0或者1的标签进行统计，没有标记的任务可以不统计。<br>目标检测就是多任务学习的一个例子。总体而言，多任务学习比迁移学习的场景要少一些。<h2 id="6-端到端（end-to-end-学习"><a href="#6-端到端（end-to-end-学习" class="headerlink" title="6. 端到端（end-to-end)学习"></a>6. 端到端（end-to-end)学习</h2>端到端学习就是省略了传统的多阶段过程，但是需要大量的数据。如果只有中等的数据，可以考虑折中一下，分阶段进行。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/15/神经网络中的优化方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/15/神经网络中的优化方法/" itemprop="url">神经网络中的优化方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-15T22:25:43+08:00">
                2018-06-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Mini-batch-decent方法"><a href="#1-Mini-batch-decent方法" class="headerlink" title="1. Mini-batch decent方法"></a>1. Mini-batch decent方法</h2><h3 id="1-1-Batch-vs-mini-batch"><a href="#1-1-Batch-vs-mini-batch" class="headerlink" title="1.1. Batch vs. mini-batch"></a>1.1. Batch vs. mini-batch</h3><p>Batch：利用矢量化编程的方法，对整个训练集运用梯度下降法。梯度每下降一小步，都要处理整个训练集。这样的效率比较慢。<br>Mini-batch：将训练集拆分为更小的训练集，成为小批量训练集（mini-batch)<br>Mini-batch t: $X^{\{t\}},Y^{\{t\}}$<br>对每个mini-batch都进行一次完整的前向和反向传播过程，当对所有的mini-batch都进行了前向和反向过程后，我们称完成了对训练集的一次遍历<strong>（epoch）</strong>。<br>Batch gradient descent，原则上cost应该是单调下降（除非learning rate太大了）；Mini-batch gradient descent，整体趋势下降，但是局部是振荡的。</p>
<h3 id="1-2-Choosing-mini-batch-size"><a href="#1-2-Choosing-mini-batch-size" class="headerlink" title="1.2. Choosing mini-batch size"></a>1.2. Choosing mini-batch size</h3><ul>
<li>如果mini-batch size=m： 等价于batch gradient descent，一般可以收敛到全局最小值点；</li>
<li>如果mini-batch size=1：等价于stochastic gradient descent，不一定收敛到全局最小值点，一般会在该点处振荡。<br>如果训练集较小（&lt;2000），就使用batch gradient descent；否则，可以选择64到512之间（2的幂数）的mini-batch size。确保可以放入CPU/GPU的内存中</li>
</ul>
<h2 id="2-指数加权平均方法（exponentially-weighted-averages）"><a href="#2-指数加权平均方法（exponentially-weighted-averages）" class="headerlink" title="2. 指数加权平均方法（exponentially weighted averages）"></a>2. 指数加权平均方法（exponentially weighted averages）</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E4%BE%8B%E5%AD%90-%E5%AF%BB%E6%89%BE%E6%B8%A9%E5%BA%A6%E8%B6%8B%E5%8A%BF.png"></center></p>
<p><center>图2.1 指数加权平均例子-寻找温度趋势</center></p>
<script type="math/tex; mode=display">
\begin{align*}
v_0&=0\\
v_1&=0.9v_0+0.1\theta_1\\
v_2&=0.9v_1+0.1\theta_2\\
v_3&=0.9v_2+0.1\theta_3\\
\vdots\\
\tag{2-1}
\end{align*}</script><p>第t天的指数平均值的通项公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
v_t&=\beta v_{t-1}+(1-\beta)\theta_t \\
&=(1-\beta)\left(\theta_t+\beta\theta_{t-1}+\cdots+\beta^{k}\theta_{t-k}+\cdots+\beta^{t-1}\theta_{1}\right) \\
\tag{2-2}
\end{align*}</script><p>近似公式：</p>
<script type="math/tex; mode=display">
v_t\approx \frac{1}{1-\beta}\ days'\ temperature\tag{2-3}</script><p>如图2.2所示，当$\beta$增大时，曲线向右平移（绿线）；$\beta$减小时，曲线振荡加剧（黄线），</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%CE%B2%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%9B%B2%E7%BA%BF%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BD%B1%E5%93%8D.png"></center></p>
<p><center>图2.2 β大小对曲线形状的影响</center></p>
<h3 id="2-1-Bias-Correction（偏差修正）"><a href="#2-1-Bias-Correction（偏差修正）" class="headerlink" title="2.1. Bias Correction（偏差修正）"></a>2.1. Bias Correction（偏差修正）</h3><p>原因：$v_0=0$导致初始阶段的点估计不准<br>解决方法：用$\frac{v_t}{1-\beta^t}$代替$v_t$</p>
<h2 id="3-Gradient-descent-with-momentum（动量梯度下降）"><a href="#3-Gradient-descent-with-momentum（动量梯度下降）" class="headerlink" title="3. Gradient descent with momentum（动量梯度下降）"></a>3. Gradient descent with momentum（动量梯度下降）</h2><p>背景问题：当目标函数的等高线为图3.1所示时，梯度下降的过程中可能会发生振荡：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%97%B6%E6%8C%AF%E8%8D%A1%E7%9A%84%E4%BE%8B%E5%AD%90.JPG"></center></p>
<p><center>图3.1 梯度下降振荡的例子></center><br>Momentum：<br>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
v_{dw}&=\beta v_{dw}+(1-\beta)dw \\
v_{db}&=\beta v_{db}+(1-\beta)db \\
w&:=w-\alpha v_{dw};\\
b&:=b-\alpha v_{db}\\
\tag{3-1}
\end{align*}</script><p>采用前面提到的指数加权平均可以使梯度的下降过程更平滑。<br>一般$\beta$取0.9就好，而且实际中一般不用修正偏差，因为迭代几步后偏差就自动减小很多了。</p>
<h2 id="4-RMSprop（Root-Mean-Square-prop，均方根传递）"><a href="#4-RMSprop（Root-Mean-Square-prop，均方根传递）" class="headerlink" title="4. RMSprop（Root Mean Square prop，均方根传递）"></a>4. RMSprop（Root Mean Square prop，均方根传递）</h2><p>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
s_{dw}&=\beta s_{dw}+(1-\beta)dw^2 \\
s_{db}&=\beta s_{db}+(1-\beta)db^2 \\
w&:=w-\alpha \frac{dw}{\sqrt{ s_{dw}}};\\
b&:=b-\alpha \frac{db}{\sqrt{ s_{db}}}\\
\tag{4-1}
\end{align*}</script><p>垂直方向除以一个较大的数，水平方向除以一个较小的数（假设b是垂直方向，w是水平方向）。为了防止分母出现零的情况，可以在分母加上一个小的$\epsilon$</p>
<h2 id="5-Adam优化算法"><a href="#5-Adam优化算法" class="headerlink" title="5. Adam优化算法"></a>5. Adam优化算法</h2><p>Adam的本质是将动量和RMSprop结合起来。<br>$v_{dw}=0,s_{dw}=0.v_{db}=0,s_{db}=0.$<br>On iteration t:<br>&emsp;&emsp;Compute $dw,db$ on current  mini-batch.</p>
<script type="math/tex; mode=display">
\begin{align*}
v_{dw}&=\beta_1 v_{dw}+(1-\beta_1)dw \\
v_{db}&=\beta_1 v_{db}+(1-\beta_1)db \\
s_{dw}&=\beta_2 s_{dw}+(1-\beta_2)dw^2 \\
s_{db}&=\beta_2 s_{db}+(1-\beta_2)db^2 \\
V_{dw}^{corrected}&=v_{dw}/\left(1-\beta_1^t\right),V_{db}^{corrected}=v_{db}/\left(1-\beta_1^t\right)\\
S_{dw}^{corrected}&=s_{dw}/\left(1-\beta_2^t\right),S_{db}^{corrected}=s_{db}/\left(1-\beta_2^t\right)\\
w&:=w-\alpha \frac{V_{dw}^{corrected}}{\sqrt{ S_{dw}^{corrected}}};\\
b&:=b-\alpha \frac{V_{db}^{corrected}}{\sqrt{ S_{db}^{corrected}}}\\
\tag{5-1}
\end{align*}</script><p>超参数：<br>$\alpha$:人工调整<br>$\beta_1:0.9$，$(dw)$<br>$\beta_2:0.999$，$(dw^2)$<br>$\epsilon$:$10^{-8}$</p>
<h2 id="6-学习率衰减（learning-rate-decay）"><a href="#6-学习率衰减（learning-rate-decay）" class="headerlink" title="6. 学习率衰减（learning rate decay）"></a>6. 学习率衰减（learning rate decay）</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E9%9A%BE%E4%BB%A5%E7%9C%9F%E6%AD%A3%E6%94%B6%E6%95%9B%E7%9A%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png"></center></p>
<p><center>图6.1 固定学习率导致不能完全收敛的示意图</center><br>解决方法：让学习率$\alpha$逐渐下降。<br>下降的形式：</p>
<ul>
<li>$\alpha=\frac{1}{1+decay-rate\ *\ epoch-num}$</li>
<li>$\alpha=0.95^{epoch-num}\cdot\alpha_0$</li>
<li>$\alpha=\frac{k}{\sqrt epoch-num}\alpha_0$</li>
<li>…</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/12/神经网络中的正则化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/12/神经网络中的正则化/" itemprop="url">神经网络中的正则化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T21:39:58+08:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Adding regularization will often help To prevent overfitting problem (high variance problem ).  </p>
<h2 id="1-Logistic-regression"><a href="#1-Logistic-regression" class="headerlink" title="1. Logistic regression"></a>1. Logistic regression</h2><p>回忆一下训练时的优化目标函数</p>
<script type="math/tex; mode=display">
\min \limits_{w,b}J\left(w,b\right), \ \ \ \ w\in\mathbb{R}^{n_x},b\in\mathbb{R} \tag{1-1}</script><p>其中</p>
<script type="math/tex; mode=display">
J\left(w,b\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)\\  \tag{1-2}</script><p>$L_2 \ \ regularization $ (most commonly used)：  </p>
<script type="math/tex; mode=display">J\left(w,b\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)+\frac{\lambda}{2m}\left\lVert w \right\rVert_2^2\\  \tag{1-3}</script><p>其中</p>
<script type="math/tex; mode=display">\left\lVert w \right\rVert_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw\tag{1-4}</script><p>Why do we regularize just the parameter w? Because w Is usually a high dimensional parameter vector while b is A scalar. Almost all The parameters are in w rather than b.<br>$L_1 \ \ regularization $  </p>
<script type="math/tex; mode=display">
J\left(w,b\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)+\frac{\lambda}{m}\left\lvert w \right\rvert_1\tag{1-5}</script><p>其中</p>
<script type="math/tex; mode=display">
\left\lvert w \right\rvert_1=\sum_j^{n_x}\left\lvert w_j \right\rvert \tag{1-6}</script><p>w will end up being sparse. In other words the w vector will have a lot of zeros in it. This can help with compressing the model a little.  </p>
<h2 id="2-Neural-network-“Frobenius-norm”"><a href="#2-Neural-network-“Frobenius-norm”" class="headerlink" title="2. Neural network “Frobenius norm”"></a>2. Neural network “Frobenius norm”</h2><script type="math/tex; mode=display">J\left(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]}\right)=\frac{1}{m}\sum_{i=1}^{m}L\left(\hat y^{(i)},y^{(i)}\right)+\frac{\lambda}{2m}\sum_{l=1}^{L}{\left\lVert w \right\rVert_2^2 }\tag{2-1}</script><p>其中</p>
<script type="math/tex; mode=display">
\left\lVert w^{[l]} \right\rVert_F^2=\sum_i^{n^{[l-1]}}\sum_j^{n^{[l]}}\left(w_{ij}\right)^2  \tag{2-2}</script><p>$L_2$ regulation is also called <strong>Weight decay</strong>:  </p>
<script type="math/tex; mode=display">
\begin{align*} dw^{[l]}&=\left(from\ backprop\right)+\frac{\lambda}{m}w^{[l]}\\  w^{l}:&=w^{[l]}-\alpha dw^{[l]}\\  &=\left(1-\frac{\alpha\lambda}{m}\right)w^{[l]}-\alpha(from\ backprop)\\
\tag{2-3}
\end{align*}</script><p>能够防止权重$w$过大，从而避免过拟合</p>
<h2 id="3-inverted-dropout"><a href="#3-inverted-dropout" class="headerlink" title="3. inverted dropout"></a>3. inverted dropout</h2><p>对于不同的训练样本都可以随机消除一部分结点<br><strong>反向随机失活</strong>（前向和后向都需要dropout）：</p>
<script type="math/tex; mode=display">
\begin{align*}  d^3&=np.random.rand(a_3.shape[0],a_3.shape[1]) < keep.prob\\  a^3&=np.multiply(a_3,d_3)\ \ \ \#a3*d3, element\ wise\ multiplication\\  a^3/&=keep.prob\ \ \ \#in\ order\ to\ not\ reduce\ the\ expected\ value\ of\ a^3\ \ inverted\ dropout\\  z^{[4]}&=w^{[4]}a^{[3]}+b^{[4]}\\  z^{[4]}/&=keep.prob\\
\tag{3-1}
\end{align*}</script><p>this inverted dropout technique by dividing by the keep.prob, it ensures that the expected value of a3 remains the same. This makes test time easier because you have less of a scaling problem.<br><strong>测试时不需要使用drop out</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/12/神经网络中的激活函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/12/神经网络中的激活函数/" itemprop="url">神经网络中的激活函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-12T21:37:51+08:00">
                2018-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>$tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$效果严格地比$sigmoid$函数好，因为该函数的对称中心在$(0,0)$，具有将数据归一化为0均值的效果。当然，二分类的输出层的激活函数还是一般用$sigmoid(z)$，因为$sigmod$函数能将输出值映射到$0\sim1$之间（概率值）<br>$Relu(z)=max(0,z)$出现后，神经网络默认都用$Relu$函数（rectified linear）来作为激活函数。此时一般默认$z&gt;0$<br>$leaky(z)=max(0.01z,z)$可以避免$z&lt;0$时斜率为零的情况  输出层有时也用线性激活函数（房价预测）  </p>
<h2 id="1-Sigmoid-activation-function"><a href="#1-Sigmoid-activation-function" class="headerlink" title="1. Sigmoid activation function"></a>1. Sigmoid activation function</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid.png"></center></p>
<p><center>图1.1 激活函数-sigmoid</center></p>
<script type="math/tex; mode=display">
\begin{align*}  a&=g(z) \\  &=\frac{1}{1+e^{-z}}\\  \tag{1-1}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\frac{e^{-z}}{1+e^{-z}}\\  &=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)\\  &=g(z)\left(1-g(z)\right)\\  &=a(1-a)\\  \tag{1-2}  \end{align*}</script><h2 id="2-Tanh-activation-function"><a href="#2-Tanh-activation-function" class="headerlink" title="2. Tanh activation function"></a>2. Tanh activation function</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh.png"></center></p>
<p><center>图2.1 激活函数-tanh</center></p>
<script type="math/tex; mode=display">
\begin{align*}  
a&=g(z) \\  &=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\  \tag{2-1}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\frac{e^{-z}}{1+e^{-z}}\\  &=\frac{\left(e^{z}+e^{-z}\right)^2-\left(e^z-e^{-z}\right)^2}{\left(e^z+e^{-z}\right)^2}\\  &=1-\left(g(z)\right)^2\\  &=1-a^2\\  \tag{2-2}  \end{align*}</script><h2 id="3-ReLU-and-Leaky-ReLU"><a href="#3-ReLU-and-Leaky-ReLU" class="headerlink" title="3. ReLU and Leaky ReLU"></a>3. ReLU and Leaky ReLU</h2><p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/ReLU.png"></center></p>
<p><center>图3.1 激活函数-ReLU</center><br><strong>ReLU:</strong>  </p>
<script type="math/tex; mode=display">
\begin{align*}  a&=g(z) \\  &=max(0,z)\\  \tag{3-1}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\left\{  \begin{aligned}  0\quad if\ z<0\\  1\quad if\ z\geq0  \end{aligned}  \right.  \tag{3-2}  \end{align*}</script><p><strong>Leaky ReLU:</strong>  </p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Leaky-ReLU.png"></center></p>
<p><center>图3.2 激活函数-Leaky ReLU</center></p>
<script type="math/tex; mode=display">
\begin{align*}  a&=g(z) \\  &=max(0.01z,z)\\  \tag{3-3}  \end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  g'(z)&=\frac{d}{dz}g(z)\\  &=\left\{  \begin{aligned}  0.01\quad if\ z<0\\  1\quad if\ z\geq0  \end{aligned}  \right.  \tag{3-4}  \end{align*}</script><h2 id="4-选择激活函数的准则"><a href="#4-选择激活函数的准则" class="headerlink" title="4.选择激活函数的准则"></a>4.选择激活函数的准则</h2><ul>
<li>如果处理的问题是二分类问题，输出为0和1，那么输出层选择sigmoid函数，其他神经元选择ReLU(有时也可用tanh)，理论上Leaky ReLU比ReLU好，但是实践中差不多。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2018/06/05/从logistic回归到神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2018/06/05/从logistic回归到神经网络/" itemprop="url">从logistic回归到神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T23:59:23+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>有任何问题请联系 <a href="mailto:languoxing@126.com" target="_blank" rel="noopener">languoxing@126.com</a></p>
<p>本文是深度学习入门教程序列的第一篇，通过介绍logistic回归的原理，进而引入一层神经网络的前向传播和后向传播公式，并提供了示例代码。</p>
<h2 id="1-logistic回归详解"><a href="#1-logistic回归详解" class="headerlink" title="1.logistic回归详解"></a>1.logistic回归详解</h2><p>logistic回归模型是用来解决二分类问题的，因此我们将首先在概率的框架下描述什么是分类问题。分类问题的一般描述如下图所示：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0.png" width="500" height="150" alt="分类问题的一般描述"></center></p>
<p><center>图1.1 分类问题的一般描述</center><br>用X代表输入空间，是输入向量的所有可能取值的集合，也叫特征空间；Y代表输出空间，是输出的所有可能取值的集合。$x^{(i)}\in X$代表特征空间的一个样本，$y^{(i)}\in Y$代表其类别（标签）。分类问题的一般描述可总结为：利用已知标签的训练集$X_{train}$训练出一个模型，该模型包含一个映射关系$h:X\rightarrow Y$，$h$应该能够对新的数据点$x^{(m+1)}$预测其类别$y^{(m+1)}$，并且预测结果应该尽可能好。通常情况下，这种“好”的标准为正确率尽可能高。</p>
<p>以二分类问题为例，用0和1代表可能的类别，也即$Y=\{0,1\}$。我们从概率的框架下来讨论二分类问题：给定输入特征向量$x$，我们希望估计出它分别属于两类的概率$P(y=0|x;\theta)$和$P(y=1|x;\theta)$。因为现在讨论的是二分类问题，可令$\hat y=P(y=1|x;\theta)$，那么只要估计出$\hat y$就可以了。在估计之前，需要选择合适形式的函数对各类别的后验概率建模，一种最简单也是最笨的方法就是令$\hat y=w ^T x+b$，也即线性回归。但是这样可能会导致$\hat y$的值大于1或者大于0，这和概率的定义相违背。因此，我们可以在线性回归的表达式前面加上一层sigmoid函数，也即$\hat y=\sigma\left(\omega ^T x+b\right)$。sigmoid函数的表达式为：</p>
<script type="math/tex; mode=display">\sigma (z)=\frac{1}{1+e^{-z}}\\ \tag{1-1}</script><p>其函数图像为：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/600px-Logistic-curve.png" width="300" height="200" alt="sigmoid函数图像"></center></p>
<p><center>图1.2 logistic函数图像</center><br>到此，我们得到了logistic回归模型：</p>
<script type="math/tex; mode=display">\hat y=\frac{1}{1+e^{(-\theta^Tx)}} \tag{1-2}</script><p>注意，在$(1-2)$中，我们将参数项b用$x_0$表示，这样能够写成更紧凑的形式。为了突出参数$\theta$，我们用$h_\theta$表示公式$(2)$中的函数，它代表了在已知特征$x$的情况下，类别为$y=1$的概率，也即$P(y=1|x;\theta)=h_\theta$；显然,$P(y=0|x;\theta)=1-h_\theta$<br>假设我们的目标是让分类的错误率最小（即最小错误率决策准则，这是最普遍的一种分类准则）。不难证明，最小化错误等价于最大化各类的后验概率。因此，若$(1-2)$的值大于0.5，则判定为类别1；否则判定为类别0。</p>
<p>注意，logistic模型仍然是一个线性分类模型，因为它的决策面是$0=w ^T x+b$，我一个线性决策面。 </p>
<h2 id="2-损失函数的选取"><a href="#2-损失函数的选取" class="headerlink" title="2.损失函数的选取"></a>2.损失函数的选取</h2><p>选取好了模型后，接下来要选取损失函数（Loss function），然后在训练集上利用一定的算法（例如梯度下降法）最小化损失函数，从而确定$h_{\theta}$中的参数$\theta$。一种很自然的想法是选取$L\left(\hat y,y\right)=\frac{1}{2}\left(\hat y-y\right)^2$作为损失函数，但是这会导致在后面学习参数的过程中，最优问题不是一个凸问题。我们在这里采用极大似然估计的方法来推导出一个更合理的损失函数，并且该损失函数是凸函数。</p>
<h3 id="2-1-最大化后验概率与极大似然估计"><a href="#2-1-最大化后验概率与极大似然估计" class="headerlink" title="2.1.最大化后验概率与极大似然估计"></a>2.1.最大化后验概率与极大似然估计</h3><p>回忆一下极大似然估计的思想：对于可观测的样本$X$及其观测值$Y$，写出该观测值的概率表达式（记为$L\left(\theta\right)$)，该概率表达式一般依赖于参数$\theta$，极大似然估计的目标是寻找$\theta$的估计值$\hat \theta$使得$L\left(\theta\right)$最大。<br>对于某一个观测样本$x^{(i)}$和观测值$y^{(i)}$,有：</p>
<script type="math/tex; mode=display">
\begin{align*}
L(\theta)&=P(y^{(i)}|x;\theta)\\
&=(h_\theta(x))^{1\left(y^{(i)}=1\right)}(1-h_\theta(x))^{1\left(y^{(i)}=0\right)}\\
&=(h_\theta(x))^y(1-h_\theta(x))^{1-y}\\
\tag{2-1}  
\end{align*}</script><p>$L(\theta)$也叫似然函数。<br>对数似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align*} l(\theta) &=\log L(\theta) \\  
&=y\log\left(h_\theta(x)\right)+(1-y)\log (1-h_\theta(x)) \tag{2-2} \end{align*}</script><p>$(2-2)$就是单个样本的损失函数。下面讨论训练集上的代价函数（cost function）。<br>对于多个观测样本$X$和观测值$Y$，似然函数可写成：</p>
<script type="math/tex; mode=display">
\begin{align*} L(\theta) &=P(\boldsymbol{Y}|\boldsymbol{X};\theta) \\ &=\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta) \\ &=\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\\ \tag{2-3}  
\end{align*}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align*} l(\theta) &=\log L(\theta) \\ &=\sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta(x^{(i)}))\\ \tag{2-4} \end{align*}</script><p>因为对数似然函数需要最大化，而损失函数需要最小化，因此我们选择如下表达式作为损失函数：</p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta(x^{(i)}))\right] \tag{2-5}</script><p>$(2-5)$就是代价函数（cost function），它等价于对所有的$(2-2)$取平均。<br>损失函数（loss function）是定义在单个样本上的，代价函数（cost function）是定义在多个样本上的。</p>
<h2 id="3-梯度下降方法求解最优的参数-w-和-b-（一层神经网络）"><a href="#3-梯度下降方法求解最优的参数-w-和-b-（一层神经网络）" class="headerlink" title="3.梯度下降方法求解最优的参数$w$和$b$（一层神经网络）"></a>3.梯度下降方法求解最优的参数$w$和$b$（一层神经网络）</h2><p>下面我们讨论只有输入层和输出层（激活函数为sigmoid函数）的简单神经网络的前向传播和反向传播过程，也即logistic回归。</p>
<h3 id="3-1-前向传播"><a href="#3-1-前向传播" class="headerlink" title="3.1.前向传播"></a>3.1.前向传播</h3><script type="math/tex; mode=display">
\begin{align*}  X&=\left[x^{(1)},x^{(2)},\cdots,x^{(m)}\right]\\  Z&=\left[z^{(1)},z^{(2)},\cdots,z^{(m)}\right]\\  
&=[w^Tx^{(1)}+b,w^Tx^{(2)}+b,\cdots,w^Tx^{(m)}+b]\\  A&=\left[a^{(1)},a^{(2)},\cdots,a^{(m)}\right]\\
&=\left[\sigma\left(a^{(1)}\right),\sigma\left(a^{(2)}\right),\cdots,\sigma\left(a^{(m)}\right)\right]\\
Y&=\left[y^{(1)},y^{(2)},\cdots,y^{(m)}\right]\\    \tag{3-2}  \end{align*}</script><p>$(3-2)$中从上到下的顺序可以代表前向传播过程。</p>
<h3 id="3-2-反向传播"><a href="#3-2-反向传播" class="headerlink" title="3.2.反向传播"></a>3.2.反向传播</h3><p>先对求导公式进行一些化简：</p>
<script type="math/tex; mode=display">
\begin{align*}  
\rm d\it a  &= \frac{\rm{d}\it{L(a,y)}}{\rm{d}\it{a}}\\  
&= -\frac{y}{a}+\frac{1-y}{1-a}\\  
\tag{3-3}  
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}  
\rm d\it z&= \frac{\rm{d}\it{L(a,y)}}{\rm{d}\it{z}}\\  
&= \frac{\rm{d}\it{L(a,y)}}{\rm{d}\it{a}}\cdot  
\frac{\rm d \it a}{\rm d \it z}\\  &= \left(-\frac{y}{a}+\frac{1-y}{1-a}\right)\cdot a(1-a)\\  
&=a-y \\  
\tag{3-4}  
\end{align*}</script><p>有了$(3-4)$，我们就能得到$\rm{d}\it{Z}=A-Y$。因此反向传播过程如下：</p>
<script type="math/tex; mode=display">
\begin{align*} 
\rm{d}\it{Z}&=A-Y\\
\rm d\it w&= \frac{1}{m}\cdot X\cdot \rm{d}\it{Z}^T\\  &=\frac{1}{m}X\left(A-Y\right)^T \\  \rm d\it b&= \frac{1}{m}\cdot\sum_{i=1}^{m}\left(\rm{d}\it{Z^{(i)}}\right) \\  \tag{3-5}  \end{align*}</script><p>注意，$(3-5)$的流程可以理解为先对每一个样本求损失函数关于$z$的梯度，并对每个样本求出$\rm{d}\it{w}$，在对所有样本的$\rm{d}\it{w}$球平均，因此有$1/m$。</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/logistic%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.JPG" alt="logistic回归梯度下降"></center></p>
<p><center>图3.2 logistic回归梯度下降</center><br>也可以对$m$个样本的代价函数一次性直接求偏导，这需要一定的向量微分和复合函数微分的知识。这样的话，在$\rm{d}\it{Z}$中就会有一个$1/m$，那么在求$\rm{d}\it{w}$时就不用另外再加一个$1/m$了，并且形状可能互为转置，最终的结果是一样的。具体过程这里省略，有兴趣的读者可以自行推导。</p>
<h2 id="4-单层神经网络示例代码"><a href="#4-单层神经网络示例代码" class="headerlink" title="4.单层神经网络示例代码"></a>4.单层神经网络示例代码</h2><p>网络结构见图3.1<br>请点击下面的超链接跳转至github页面：<br><a href="https://github.com/lankuohsing/Coursera-Deep-Learning-Specialization/tree/master/Neural%20Networs%20and%20Deep%20Learning" target="_blank" rel="noopener">用logistic回归实现一层神经网络用来识别猫的图片</a><br>其中Logistic_Regression_with_a_Neural_Network_mindset.ipynb文件包含详细的注释和图示，但需要在jupyternotebook中运行；Logistic_Regression_with_a_Neural_Network_mindset.py文件可直接用“python”命令运行，两者的功能是一样的。</p>
<h2 id="5-两层神经网络"><a href="#5-两层神经网络" class="headerlink" title="5.两层神经网络"></a>5.两层神经网络</h2><h3 id="5-1-前向传播"><a href="#5-1-前向传播" class="headerlink" title="5.1.前向传播"></a>5.1.前向传播</h3><script type="math/tex; mode=display">
\begin{align*}  X&=\left[x^{(1)},x^{(2)},\cdots,x^{(m)}\right]\\  
W^{[1]}&=\left[w_1^{[1]},w_2^{[1]},\cdots,w_{n_1}^{[1]}\right]^{T}\\
b^{[1]}&=\left[b_1^{[1]},b_2^{[1]},\cdots,b_{n_1}^{[1]}\right]\\
Z^{[1]}&=W^{[1]}X+b^{[1]}\\
A^{[1]}&=g^{[1]}\left(Z^{[1]}\right)\\
W^{[2]}&=\left[w_1^{[2]},w_2^{[2]},\cdots,w_{n_2}^{[2]}\right]^{T}\\
b^{[2]}&=\left[b_1^{[2]},b_2^{[2]},\cdots,b_{n_2}^{[2]}\right]\\
Z^{[2]}&=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}&=g^{[2]}\left(Z^{[2]}\right)\\
Y&=\left[y^{(1)},y^{(2)},\cdots,y^{(m)}\right]\\    \tag{5-1}  \end{align*}</script><p>$(5-1)$中从上到下的顺序可以代表前向传播过程。</p>
<h3 id="5-2-反向传播"><a href="#5-2-反向传播" class="headerlink" title="5.2.反向传播"></a>5.2.反向传播</h3><script type="math/tex; mode=display">
\begin{align*} 
{\rm d}Z^{[2]}&=A^{[2]}-Y\\
{\rm d}W^{[2]}&=\frac{1}{m}{\rm d}Z^{[2]}A^{[1]\ T}\\  
{\rm d}b^{[2]}&=\frac{1}{m}np.sum\left({\rm d}Z^{[2]},axis=1,keepdims=True\right)\\  
{\rm d}A^{[1]}&={W^{[2]}}^T{\rm d}Z^{[2]}\\
{\rm d}Z^{[1]}&={\rm d}A^{[1]}*{g^{[1]}}'\left(Z^{[1]}\right)\\
{\rm d}W^{[1]}&=\frac{1}{m}{\rm d}Z^{[1]}A^{[0]\ T}\\  
{\rm d}b^{[1]}&=\frac{1}{m}np.sum\left({\rm d}Z^{[1]},axis=1,keepdims=True\right)\\  \tag{5-2}
\end{align*}</script><p>$(5-2)$中从上到下的顺序可以代表前向传播过程。</p>
<h3 id="5-3-关于参数的初始化"><a href="#5-3-关于参数的初始化" class="headerlink" title="5.3.关于参数的初始化"></a>5.3.关于参数的初始化</h3><p>在单层神经网络（logistic回归）中，可以将参数初始化wie全零值；但是在多层神经网络中，$W$不能初始化为0，否则每一层的各个节点对应的$W$会训练成一样的值；$b$可以初始化为0  $W$不能初始化为特别大，浅层网络一般是0.01量级；因为太大了的话，激活函数的自变量要么正向很大，要么负向很大，导致斜率趋近于0 ，收敛速度很慢。尤其是你使用$sigmoid$或$tanh$激活函数。如果不使用这些激活函数，但是你是二分类问题（输出层是sigmoid函数），也会出现这个问题.</p>
<h3 id="5-4-2层神经网络示例代码"><a href="#5-4-2层神经网络示例代码" class="headerlink" title="5.4.2层神经网络示例代码"></a>5.4.2层神经网络示例代码</h3><p>对如下二维平面上的点进行分类：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/planar.png"></center></p>
<p><center>图5.1 二维平面上的非线性二分类数据集例子</center><br>网络结构如下：</p>
<p><center><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E4%BB%8Elogistic%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2_layer_NN.png"></center></p>
<p><center>图5.2 2层神经网络结构例子</center><br>点击下面的超链接查看完整代码：<br><a href="https://github.com/lankuohsing/Coursera-Deep-Learning-Specialization/tree/master/Neural%20Networs%20and%20Deep%20Learning" target="_blank" rel="noopener">2层神经网络的示例代码</a><br>其中Palnar_data_clf_with_one_hidden_layer.ipynb文件包含详细的注释和图示，但需要在jupyternotebook中运行；Palnar_data_clf_with_one_hidden_layer.py文件可直接用“python”命令运行，两者的功能是一样的。</p>
<h2 id="6-深层神经网络"><a href="#6-深层神经网络" class="headerlink" title="6.深层神经网络"></a>6.深层神经网络</h2><h3 id="6-1-每层的参数及变量的尺寸"><a href="#6-1-每层的参数及变量的尺寸" class="headerlink" title="6.1.每层的参数及变量的尺寸"></a>6.1.每层的参数及变量的尺寸</h3><script type="math/tex; mode=display">\begin{align*}  z^{[l]}&:\left(n^{[l]},1\right)\\  a^{[l]}&:\left(n^{[l]},1\right)\\  W^{[l]}&:\left(n^{[l]},n^{[l-1]}\right)\\  b^{[l]}&:\left(n^{[l]},1\right)\\  dW^{[l]}&:\left(n^{[l]},n^{[l-1]}\right)\\  db^{[l]}&:\left(n^{[l]},1\right)\\  \tag{6-1}
\end{align*}</script><h3 id="6-2-前向传播递推公式"><a href="#6-2-前向传播递推公式" class="headerlink" title="6.2.前向传播递推公式"></a>6.2.前向传播递推公式</h3><script type="math/tex; mode=display">
\begin{align*}  X&=\left[x^{(1)},x^{(2)},\cdots,x^{(m)}\right]\\  
W^{[l]}&=\left[w_1^{[l]},w_2^{[l]},\cdots,w_{n_l}^{[l]}\right]^{T}\\
b^{[l]}&=\left[b_1^{[l]},b_2^{[l]},\cdots,b_{n_l}^{[l]}\right]\\
Z^{[l]}&=W^{[l]}A^{[l-1]}+b^{[l]}\\
A^{[l]}&=g^{[l]}\left(Z^{[l]}\right)\\
W^{[l+1]}&=\left[w_1^{[l+1]},w_2^{[l+1]},\cdots,w_{n_l+1}^{[l+1]}\right]^{T}\\
b^{[l+1]}&=\left[b_1^{[l+1]},b_2^{[l+1]},\cdots,b_{n_2}^{[l+1]}\right]\\
Z^{[l+1]}&=W^{[l+1]}A^{[l]}+b^{[l+1]}\\
A^{[l+1]}&=g^{[l+1]}\left(Z^{[l+1]}\right)\\
Y&=\left[y^{(1)},y^{(2)},\cdots,y^{(m)}\right]\\    \tag{6-2}  \end{align*}</script><h3 id="6-3-反向传播递推公式"><a href="#6-3-反向传播递推公式" class="headerlink" title="6.3.反向传播递推公式"></a>6.3.反向传播递推公式</h3><script type="math/tex; mode=display">
\begin{align*} {\rm d}Z^{[l]}&={\rm d}A^{[l]}*{g^{[l]}}'\left(Z^{[l]}\right)\\  
{\rm d}W^{[l]}&=\frac{1}{m}{\rm d}Z^{[l]}A^{[l-1]\ T}\\  
{\rm d}b^{[l]}&=\frac{1}{m}np.sum\left({\rm d}Z^{[l]},axis=1,keepdims=True\right)\\  
{\rm d}A^{[l-1]}&={W^{[l]}}^T{\rm d}Z^{[l]}\\  
\tag{6-3}
\end{align*}</script><h3 id="6-4-多层神经网络的示例代码"><a href="#6-4-多层神经网络的示例代码" class="headerlink" title="6.4.多层神经网络的示例代码"></a>6.4.多层神经网络的示例代码</h3><p>点击如下超链接。<br><a href="https://github.com/lankuohsing/Coursera-Deep-Learning-Specialization/tree/master/Neural%20Networs%20and%20Deep%20Learning" target="_blank" rel="noopener">对比了2层神经网络和5层神经网络在对猫分类时的性能</a><br>Deep Neural Network - Application.py<br>Deep Neural Network - Application.ipynb</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><a class="extend next" rel="next" href="page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="images/avatar.jpg"
                alt="Guoxing Lan" />
            
              <p class="site-author-name" itemprop="name">Guoxing Lan</p>
              <p class="site-description motion-element" itemprop="description">On The Journey To Truth</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://lankuohsing.github.io/blog/archives">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async="" src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
  <i class="fa fa-user-md"></i>
  <span id="busuanzi_container_site_uv">
    visitor count:<span id="busuanzi_value_site_uv"></span>
  </span>
  <span id="busuanzi_value_site_pv">
	visit count:<span id="busuanzi_value_site_pv"></span>
  </span>
</div>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guoxing Lan</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
