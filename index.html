<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="On The Journey To Truth">
<meta property="og:type" content="website">
<meta property="og:title" content="Guoxing Lan">
<meta property="og:url" content="https://lankuohsing.github.io/blog/index.html">
<meta property="og:site_name" content="Guoxing Lan">
<meta property="og:description" content="On The Journey To Truth">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Guoxing Lan">
<meta name="twitter:description" content="On The Journey To Truth">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lankuohsing.github.io/blog/"/>





  <title>Guoxing Lan</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
	<a href="https://github.com/lankuohsing"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Guoxing Lan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Journey To Truth</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://lankuohsing.github.io/blog" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://lankuohsing.github.io/blog/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://lankuohsing.github.io/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://lankuohsing.github.io/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://lankuohsing.github.io/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2021/02/15/神经网络量化压缩笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2021/02/15/神经网络量化压缩笔记/" itemprop="url">神经网络量化压缩笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-02-15T22:32:00+08:00">
                2021-02-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>@[toc]<br><a href="https://jackwish.net/2019/neural-network-quantization-introduction-chn.html" target="_blank" rel="noopener">https://jackwish.net/2019/neural-network-quantization-introduction-chn.html</a><br><a href="https://zhuanlan.zhihu.com/p/149659607" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/149659607</a></p>
<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h1><p>近年来，基于神经网络的深度学习在图像处理、自然语言处理、语音识别等领域取得了显著效果。一般情况下，一个神经网络模型越大（一般指参数量越大），模型的拟合能力更强，准确度越高。这将进一步导致运行模型时需要的内存（包内存容量和内存带宽）、磁盘存储消耗增大，推理时延和功耗也会变大，从而限制模型的工业化落地。<br>为了解决这个问题，可以从两方面来下手：</p>
<ul>
<li>从原理上设计更高效地网络结构，即参数量更少的模型，并保持精度下降可以接受。</li>
<li>不改变原有模型结构，而是对已有的模型进行参数压缩。</li>
</ul>
<h1 id="1-量化压缩方法简介"><a href="#1-量化压缩方法简介" class="headerlink" title="1. 量化压缩方法简介"></a>1. 量化压缩方法简介</h1><p>量化压缩就是上一章节中的第二种解决方案，即对已有的模型进行参数压缩。所谓量化，就是用精度更低的类型来存储权重参数。例如，通常情况下模型默认以float32类型变量来存储权重，低精度就是以float16，int8甚至更低精度的类型来存储。最极端的情况下，可以采用一个bit来存储，也即二值神经网络。工业界中常采用的是int8类型。<br>在训练过程中，一般还是全精度（float32）类型，到了推理阶段，则有两种方案：一种是所有算子都支持量化后的类型的数据运算（以int8为例），因此模型的全过程中数据流都是int8；另一种是数据流仍是float32，每个算子前后分别有quantize层和dequantize层用以将float32转换为int8或者反过来。</p>
<h1 id="2-量化压缩原理"><a href="#2-量化压缩原理" class="headerlink" title="2. 量化压缩原理"></a>2. 量化压缩原理</h1><h2 id="2-1-定点数与浮点数"><a href="#2-1-定点数与浮点数" class="headerlink" title="2.1. 定点数与浮点数"></a>2.1. 定点数与浮点数</h2><p>在计算机的存储中，int属于定点数，float和double属于浮点数。定点数与浮点数的介绍见<a href="https://www.jianshu.com/p/d39fb5792ac8" target="_blank" rel="noopener">https://www.jianshu.com/p/d39fb5792ac8</a><br>浮点数的存储与转换方式详解见<a href="https://www.cnblogs.com/lan0725/p/11515584.html" target="_blank" rel="noopener">https://www.cnblogs.com/lan0725/p/11515584.html</a><br>int8的值域为[-128,127],取值个数为$2^{8};$float32的值域为$[(2-2^{-23})\times 2^{127},(2^{-23}-2)\times 2^{127}]$,取值个数约为$2^{32}$。int8在整个值域上的精度分布是均匀的，而float32不是均匀的,0附近的精度越高，越往值域区间两边精度越低。这是因为，在给定指数时，float32在此指数对应的区间内数值个数是一定的，如下图所示（图片来源<a href="https://jackwish.net/2019/neural-network-quantization-introduction-chn.html" target="_blank" rel="noopener">https://jackwish.net/2019/neural-network-quantization-introduction-chn.html</a>）<br><img src="https://img-blog.csdnimg.cn/20210215223213448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><center>图2.1 float类型的数值个数分布</center></p>
<h2 id="2-2-将浮点数量化为定点数"><a href="#2-2-将浮点数量化为定点数" class="headerlink" title="2.2. 将浮点数量化为定点数"></a>2.2. 将浮点数量化为定点数</h2><p>量化压缩的过程本质上是一个一个区间放缩到另一个区间的过程，这里仅讨论线性放缩:</p>
<script type="math/tex; mode=display">
x_{quantized}=round(x_{float}/x_{scale}+x_{zero\_point})\tag{2-1}</script><p>round表示取整<br>首先确定放缩因子：</p>
<script type="math/tex; mode=display">
x_{scale}=\frac{x_{float}^{max}-x_{float}^{min}}{x_{quantized}^{max}-x_{quantized}^{min}}\tag{2-2}</script><p>而</p>
<script type="math/tex; mode=display">
(x_{quantized}^{max}-x_{quantized})\times x_{scale}=(x_{float}^{max}-x_{float})\tag{2-3}</script><p>所以：</p>
<script type="math/tex; mode=display">
x_{quantized}=x_{float}/x_{scale}+x_{quantized}^{max}-x_{float}^{max}/x_{scale}\tag{2-4}</script><p>也即：</p>
<script type="math/tex; mode=display">
x_{zero\_point}=x_{quantized}^{max}-x_{float}^{max}/x_{scale}\tag{2-5}</script><p>在工程中，int8可能会取[0,255] (无符号整数)</p>
<p>有时float类型的极大值和极小值会太极端，从而使得放缩因子太大，导致最后量化后的结果太集中于某一个小的子区间，浪费了其他部分的值域空间。这时，可以读float类型的极大值极小值进行clip。例如，认为设为-1.0和1.0.</p>
<p>这里留一个小彩蛋，为什么（2-3)里面不用$x-x^{min}$来计算缩放因子，而是采用$x-x^{max}$呢？虽然从数学上是等价的，但是工程实现上的效果会有点不一样</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2021/02/03/TensorFlow-PyTorch中张量-Tensor-的底层存储方式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2021/02/03/TensorFlow-PyTorch中张量-Tensor-的底层存储方式/" itemprop="url">TensorFlow/PyTorch中张量(Tensor)的底层存储方式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-02-03T00:39:25+08:00">
                2021-02-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>@[toc]</p>
<h1 id="0-张量（Tensor）基本概念回顾"><a href="#0-张量（Tensor）基本概念回顾" class="headerlink" title="0. 张量（Tensor）基本概念回顾"></a>0. 张量（Tensor）基本概念回顾</h1><p>张量（Tensor）其实就是多维数组，类似于NumPy里面的np.array。<br>这里的维度，更准确的讲法应该叫阶（rank），这是为了跟向量（vector）的维度区分开的。vector其实就是rank为1的张量，我们说一个vector是n维的其实是说它有n个分量（标量）。而如果张量的维度（阶）是n维的，并不是说它有n个标量分量，而是说在表示这个张量时需要用n个坐标轴。每个轴上都可以有多个分量。为了表示每个轴上的分量个数，引入形状（shape）的概念。<br>例如，一个三阶的张量，可以理解为是一个立方体。假设它的shape是$[3,2,5]$,那么下图就是一个具体的例子（下面两张图的来源均为<a href="https://www.tensorflow.org/guide/tensor" target="_blank" rel="noopener">https://www.tensorflow.org/guide/tensor</a>）：<br><img src="https://img-blog.csdnimg.cn/20210203231202217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图0.1 一个rank为3的tensor的例子</center>

<p>而一个四阶张量的例子则用下图表示：<br><img src="https://img-blog.csdnimg.cn/20210203231212171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图0.2 一个rank为4的tensor的例子</center>

<p>TensorFlow/PyTorch中使用比较多的tensor的阶为4，shape为$[Batch, Height, Weight, Features]$.</p>
<p>n阶张量的排列规律如下图所示（图片来源<a href="https://syborg.dev/posts/understanding-tensors-in-pytorch" target="_blank" rel="noopener">https://syborg.dev/posts/understanding-tensors-in-pytorch</a>）：<br><img src="https://img-blog.csdnimg.cn/20210203231222692.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图0.3 1-6阶tensor的shape规律</center>

<p>可以将规律总结为：从shape列表的最右边往左遍历，最开始三个阶按照“下-右-里”的顺序排列，然后打包成一个group，再将整个group按照“下-右-里”的顺序排列，满三次后再打包成一个group，如此往复循环。。</p>
<h1 id="1-tensor在计算机内存中的存储方式"><a href="#1-tensor在计算机内存中的存储方式" class="headerlink" title="1. tensor在计算机内存中的存储方式"></a>1. tensor在计算机内存中的存储方式</h1><p>前面提到的rank和shape，都是数学上的定义，实际在内存中是一维存储的。<br>排列规律为：</p>
<ol>
<li>假设rank为n，即shape列表的size为n.</li>
<li>初始位置为原点</li>
<li>将shape[n-1]方向上的元素按照这个方向上的坐标递增的顺序进行线性排列。</li>
<li>shape[n-1]方向上的元素排完后再将shape[n-2]上的坐标递增，继续排shape[n-1]方向上的元素，直到shape[n-2]方向上的坐标到了最大值，并且排完了这个坐标上的shape[n-1]方向的元素，此时shape[n-2]方向上坐标归零。</li>
<li>将shape[n-3]上的坐标递增，继续3,4，直到shape[n-3]方向上的坐标和shape[n-2]方向上的坐标到了最大值，并且排完了这个坐标上的shape[n-1]方向的元素，此时shape[n-3]和shape[n-2]方向上坐标归零</li>
<li>对shape[n-i] (i=4,5,…,n)上的坐标递增,并重复前面的过程。</li>
</ol>
<p>一般情况下，假设高阶tensor的下标为coordinates, 则转换为一维向量的下标index过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">index = coordinates[0];</span><br><span class="line">for(int i=0; i&lt;coordinates.size(); i++)&#123;</span><br><span class="line">index = index * shape[i] + coordinates[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2021/01/13/pytorch教程之自动求导机制-AUTOGRAD-从梯度和Jacobian矩阵讲起/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2021/01/13/pytorch教程之自动求导机制-AUTOGRAD-从梯度和Jacobian矩阵讲起/" itemprop="url">pytorch教程之自动求导机制(AUTOGRAD)-从梯度和Jacobian矩阵讲起</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-13T23:48:46+08:00">
                2021-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>@[toc]</p>
<h1 id="1-梯度和Jacobian矩阵"><a href="#1-梯度和Jacobian矩阵" class="headerlink" title="1. 梯度和Jacobian矩阵"></a>1. 梯度和Jacobian矩阵</h1><p>设$f(x)\in R^1$是关于向量$x\in R^n$的函数，则它关于$x$的导数定义为：</p>
<script type="math/tex; mode=display">
\frac{df(x)}{dx}:=\left[\frac{\partial f(x)}{\partial x_i}\right]\in R^{n}\tag{1-1}</script><p>函数$f(x)\in R^1$关于向量$x\in R^n$的导数是一个列向量，称之为$f(x)$关于$x$的梯度。</p>
<script type="math/tex; mode=display">
\frac{df(x)^T}{dx}:=\left(\frac{df(x)}{dx}\right)^T=\left[\frac{\partial f(x)}{\partial x_i}\right]^T\in R^{1\times n}\tag{1-2}</script><p>如果$f(x)\in R^M$是关于向量$x\in R^n$的函数向量，则$f(x)$关于$x$的导数定义为：</p>
<script type="math/tex; mode=display">
\frac{df(x)}{dx}:=\frac{df(x)}{dx^T}=\left[\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},\cdots,\frac{\partial f(x)}{\partial x_n}\right]\in R^{m\times n}\tag{1-3}</script><p>称上述矩阵为Jacobian矩阵。<br>一些常用推论：</p>
<ol>
<li>假设$v,x\in R^n$:<script type="math/tex; mode=display">
\frac{d}{dx}(v^Tx)=\frac{d}{dx}(x^Tv)=v\tag{1-4}</script></li>
<li>假设$y\in R^1$,$z\in R^m$,$x\in R^n$,$z=g(x)$,y=f(z)：<script type="math/tex; mode=display">
\frac{dy}{dx}=\left(\frac{dz}{dx}\right)^T\frac{dy}{dz}\tag{1-5}</script>可以从向量矩阵的维度适配上去理解和记忆，因为$\frac{dy}{dx}\in R^n$,$\frac{dy}{dz}\in R^m$,$\frac{dz}{dx}\in R^{m\times n}$，所以必须有上述的公式才能适配。</li>
<li>假设$y\in R^k$,$z\in R^m$,$x\in R^1$,$z=g(x)$,y=f(z)：<script type="math/tex; mode=display">
\frac{dy}{dx}=\frac{dy}{dz}\frac{dz}{dx}\tag{1-6}</script></li>
<li>假设$y\in R^k$,$z\in R^m$,$x\in R^n$,$z=g(x)$,y=f(z)：<script type="math/tex; mode=display">
\frac{dy}{dx}=\frac{dy}{dz}\frac{dz}{dx}\tag{1-7}</script><h1 id="2-pytorch求变量导数的过程"><a href="#2-pytorch求变量导数的过程" class="headerlink" title="2. pytorch求变量导数的过程"></a>2. pytorch求变量导数的过程</h1>在pytorch和TensorFlow中，是不支持张量对张量的求导。这不是因为数学上没法求，而是因为工程实现上比较麻烦。因为向量对向量求导是个矩阵，二阶张量（矩阵）对二阶张量（矩阵）求导得到一个四阶张量，这样很容易会产生阶数爆炸。所以pytorch和TensorFlow（猜测其他深度学习框架也是这样）对外的接口干脆不支持张量对张量求导。如果遇到张量对张量求导的情况，例如向量对向量求导的情况，需要对因变量乘以一个维度一样的向量，转换为标量对向量的求导，这样可以大大减少计算量（具体见后文）。并且，因为pytorch和TensorFlow是为了机器学习/深度学习模型设计的，机器学习/深度模型的求导基本上都是损失函数(标量)对参数的求导，很少直接用到向量对向量求导，因此上述过程是有实际意义和需求的。</li>
</ol>
<p>假设有一个三维tensor $x=[x_1,x_2,x_3]^T=[1,2,3]^T$,另一个三维tensor y:</p>
<script type="math/tex; mode=display">
y=f(x)=
\begin{bmatrix}  
{x_1}^3+2{x_2}^2+3x_3 \\  
3x_1+2{x_2}^2+{x_3}^3\\  
2x_1+{x_2}^3+3{x_3}^2  
\end{bmatrix} \tag{2-1}</script><p>那么在计算y相对于x的导数时，</p>
<script type="math/tex; mode=display">
\frac{dy}{dx}=
\begin{bmatrix}  
&3{x_1}^2,&4x_2,&3 \\  
&3,&4{x_2},&3{x_3}^2\\  
&2,&3{x_2}^2,&6{x_3}
\end{bmatrix} \tag{2-2}</script><p>在pytorch中实际计算时，不能直接用y对x求导，需要先用一个向量$w$左乘y，再转置。例如，$w^T=[3,2,1]$。因此，pytorch算的其实是：</p>
<script type="math/tex; mode=display">
\frac{dy^T}{dx}w=
\left(w^T\frac{dy}{dx}\right)^T
=\begin{bmatrix}  
17\\
52\\  
81\\
\end{bmatrix} \tag{2-3}</script><p>$w$可以理解为是对$[\frac{\partial y_1}{\partial x},\frac{\partial y_2}{\partial x},\frac{\partial y_3}{\partial x}]^T$的权重参数。因此我们得到的是y的各个分量的导数的加权求和。</p>
<p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x1=torch.tensor(1, requires_grad=True, dtype = torch.float)</span><br><span class="line">x2=torch.tensor(2, requires_grad=True, dtype = torch.float)</span><br><span class="line">x3=torch.tensor(3, requires_grad=True, dtype = torch.float)</span><br><span class="line">y=torch.randn(3)</span><br><span class="line">y[0]=x1**3+2*x2**2+3*x3</span><br><span class="line">y[1]=3*x1+2*x2**2+x3**3</span><br><span class="line">y[2]=2*x1+x2**3+3*x3**2</span><br><span class="line">v=torch.tensor([3,2,1],dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line">print(x1.grad)</span><br><span class="line">print(x2.grad)</span><br><span class="line">print(x3.grad)</span><br></pre></td></tr></table></figure></p>
<p>利用链式求导的原理来理解，可以理解为$w$是（远方）某个标量对$y$的导数。pytorch之所以要这么设计，是因为在机器学习/深度学习模型中，求导的最终目的一般是为了让损失函数最小。损失函数一般都是一个标量，因此无论链式求导的过程多么复杂，中间过程也许有很多向量对向量求导的子过程，但是最开始一定会有一个标量（损失函数）对向量的求导过程，这个导数就是前面的$w$。</p>
<p>下面看一个带两个隐藏层的神经网络解决线性回归问题的例子，来进一步说明这点。<br>为了简单起见，考虑batch_size=1的情况。设输入数据为$x=[x_1,x_2]^T$,输入层到第一个隐藏层的权重矩阵为</p>
<script type="math/tex; mode=display">
W=\begin{bmatrix}  
w_1^T\\
w_2^T
\end{bmatrix} =
\begin{bmatrix}  
w_{11},w_{12}\\
w_{21},w_{22}\\
\end{bmatrix} \tag{2-4}</script><p>第一个隐藏层的值为$z=[z_1,z_2]^T$,<br>第一个隐藏层到第二个隐藏层的权重矩阵为</p>
<script type="math/tex; mode=display">
U=\begin{bmatrix}  
u_1^T\\
u_2^T
\end{bmatrix} =
\begin{bmatrix}  
u_{11},u_{12}\\
u_{21},u_{22}\\
\end{bmatrix} \tag{2-5}</script><p>第二个隐藏层的值为$s=[s_1,s_2]^T$,<br>输出层的值为$y$,隐藏层到输出层的权重参数为$v=[v_1,v_2]^T$。则有：</p>
<script type="math/tex; mode=display">
z=\begin{bmatrix}  
z_1\\
z_2\\
\end{bmatrix}=
\begin{bmatrix}  
w_{11},w_{12}\\
w_{21},w_{22}\\
\end{bmatrix}\begin{bmatrix}  
x_1\\
x_2\\
\end{bmatrix}=
\begin{bmatrix}  
w_{11}x_1+w_{12}x_2\\
w_{21}x_1+w_{22}x_2\\
\end{bmatrix}\tag{2-6}</script><script type="math/tex; mode=display">
\begin{align*}
s&=\begin{bmatrix}  
s_1\\
s_2\\
\end{bmatrix}=
\begin{bmatrix}  
u_{11},u_{12}\\
u_{21},u_{22}\\
\end{bmatrix}\begin{bmatrix}  
z_1\\
z_2\\
\end{bmatrix}\\
&=
\begin{bmatrix}  
u_{11}(w_{11}x_1+w_{12}x_2)+u_{12}(w_{21}x_1+w_{22}x_2)\\
u_{21}(w_{11}x_1+w_{12}x_2)+u_{22}(w_{21}x_1+w_{22}x_2)\\
\end{bmatrix}\tag{2-7}
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
y&=
[v_1,v_2]\begin{bmatrix}  
s_1\\
s_2\\
\end{bmatrix}\\
&=(v_1u_{11}x_1+v_2u_{21}x_1)w_{11}\\
&+(v_1u_{11}x_2+v_2u_{21}x_2)w_{12}\\
&+(v_1u_{12}x_1+v_2u_{22}x_1)w_{21}\\
&+(v_1u_{12}x_2+v_2u_{22}x_2)w_{22}
\end{align*}\tag{2-8}</script><p>损失函数为$L=(y-\hat y)^2/2$<br>则损失函数关于权重参数$w_1$的导数为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{dL}{dw_1}&=(y-\hat y)\frac{dy}{dx}\\
&=(y-\hat y)\frac{ds^T}{dx}\frac{dy}{ds}\\
&=(y-\hat y)\frac{dz^T}{dx}\frac{ds^T}{dz}\frac{dy}{ds}\\
&=(y-\hat y)\begin{bmatrix}  
x_1,0\\
x_2,0\\
\end{bmatrix}
\begin{bmatrix}  
u_{11},u_{21}\\
u_{12},u_{22}\\
\end{bmatrix}
\begin{bmatrix}  
v_1\\
v_2
\end{bmatrix}\\
&=(y-\hat y)\begin{bmatrix}  
v_1x_1u_{11}+v_2x_1u_{21}\\
v_1x_2u_{11}+v_2x_2u_{21}
\end{bmatrix}\\
\end{align*}\tag{2-9}</script><p>可以验证$(2-9)$和前面$(2-8)$中直接求得的导数值是一样的。<br>这里发现了一个小彩蛋：<br>假设在pytorch的底层实现中，如果从左往右计算，则需要进行进行大量的矩阵乘法。如果有n个$2\times 2$的方阵相乘，那么需要进行$4\times (n-1)$次内积。如果从又往左计算，只需要进行$2\times n$次内积。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/10/21/凸优化基础知识笔记-凸集、凸函数、凸优化问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/10/21/凸优化基础知识笔记-凸集、凸函数、凸优化问题/" itemprop="url">凸优化基础知识笔记-凸集、凸函数、凸优化问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-21T23:02:07+08:00">
                2020-10-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Mathematical-Fundamentals/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematical Fundamentals</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>[TOC]</p>
<h1 id="1-凸集"><a href="#1-凸集" class="headerlink" title="1. 凸集"></a>1. 凸集</h1><p>集合$C$被称为凸集，如果C中任意两点间的线段仍然在$C$中。即对于任意$x_1,x_2\in C$和满足$0\leq \theta \leq 1$的$\theta$都有</p>
<script type="math/tex; mode=display">
\theta x_1+(1-\theta)x_1\in C\\
\tag{1-1}</script><h1 id="2-凸函数"><a href="#2-凸函数" class="headerlink" title="2. 凸函数"></a>2. 凸函数</h1><p>凸函数的原始定义：</p>
<blockquote>
<p>函数$f:{\rm{R}}^n\rightarrow{\rm{R}}$是凸的，如果${\rm dom}\ f$是凸集，且对于任意$x,y\in {\rm dom}\ f$和任意$0\leq \theta\leq 1$，有</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta)y)\leq \theta f(x)+(1-\theta)f(y)\tag{2-1}</script></blockquote>
<p>严格凸：上式中当$x\not=y$且$0\leq \theta \leq 1$时，不等式严格成立（即取小于号）<br><strong>几何意义</strong>：上述不等式意味着点$(x,f(x))$和$(y,f(y))$之间的线段在函数$f$的图像上方。</p>
<h2 id="2-1-凸函数的一阶条件"><a href="#2-1-凸函数的一阶条件" class="headerlink" title="2.1. 凸函数的一阶条件"></a>2.1. 凸函数的一阶条件</h2><blockquote>
<p>假设$f$可微（即其梯度$\nabla f$在开集${\rm dom}\ f$内处处存在），则函数$f$是凸函数的充要条件是${\rm dom}\ f$是凸集且对于任意$x,y\in {\rm dom}\ f$，下式成立：</p>
<script type="math/tex; mode=display">
f(y)\geq f(x)+\nabla f(x)^T(y-x)\tag{2-2}</script></blockquote>
<p><strong>几何意义</strong>：凸函数的一阶Taylor近似是原函数的一个全局下估计，也即凸函数任意一点处的切线都在原函数图像的下方。反之亦然（充分必要条件）<br>2.2. 凸函数的二阶条件</p>
<blockquote>
<p>假设$f$二阶可微，即对于开集${\rm dom}\ f$内的任意一点，它的Hessian矩阵或者二阶导数$\nabla ^2f$存在，则函数$f$是凸函数的充要条件是其Hessian矩阵是半正定阵：即对于所有的$x\in {\rm dom}\ f$有：</p>
<script type="math/tex; mode=display">
\nabla^2f(x)\succeq 0\tag{2-3}</script></blockquote>
<p><strong>几何意义</strong>：函数图像在点$x$处具有正（向上）的曲率。</p>
<h2 id="2-1-凸函数例子"><a href="#2-1-凸函数例子" class="headerlink" title="2.1. 凸函数例子"></a>2.1. 凸函数例子</h2><p><strong>常见的凸函数：</strong></p>
<ul>
<li><strong>指数函数</strong>：$e^{ax},\forall a \in R$</li>
<li><strong>范数: $\lVert x\rVert_p=\left(\lvert x_1\rvert^p+\lvert x_2\rvert^p+\cdots+\lvert x_n\rvert^p\right)^{1/p},p\geq 1$</strong>。${\rm R}^n上的任意范数均为凸函数$。</li>
<li><strong>负熵函数</strong>：函数$xlog{x}$在其定义域（$R_{++}或者R_X$）上是凸函数。</li>
</ul>
<h1 id="3-凸优化问题"><a href="#3-凸优化问题" class="headerlink" title="3. 凸优化问题"></a>3. 凸优化问题</h1><p>优化问题的<strong>标准形式</strong>：</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ \ &f_0(x)\\
s.t.\ \ &f_i(x)\leq 0,i=1,2,\cdots,m\\
&h_i(x)=0,i=1,2,\cdots,p\\
\tag{3-1}
\end{align*}</script><p>我们称$x\in R^n$为<strong>优化变量</strong>，称函数$f_0:R^n\rightarrow R$为为<strong>目标函数</strong>或代价函数；不等式$f_i(x)\leq 0$称为不等式约束，$h_i:R^n\rightarrow R$称为等式约束。优化问题的定义域是目标函数和约束函数的定义域的交集。满足约束条件的定义域中的点称为可行点；所有可行点的集合称为可行集。<br>问题$(3-1)$的最优值$p^{\star}$定义为:</p>
<script type="math/tex; mode=display">
\begin{align*}
p=\inf\{&f_0(x)|\\
&f_i(x)\leq 0,i=1,2,\cdots,m,h_i(x)=0,i=1,2,\cdots,p\}\\
\tag{3-2}
\end{align*}</script><p>如果问题不可行，则$p^{\star}=\infty$</p>
<p><strong>凸优化问题</strong>的标准形式</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ \ &f_0(x)\\
s.t.\ \ &f_i(x)\leq 0,i=1,2,\cdots,m\\
&a_i^Tx=b_i,i=1,2,\cdots,p\\
\tag{3-3}
\end{align*}</script><p>其中，$f_0,f_1,\cdots,f_m$是凸函数<br>凸优化问题与一般优化问题的标准形式的区别在于以下三点：</p>
<ul>
<li>目标函数必须是凸的</li>
<li>不等式约束函数必须是凸的</li>
<li>等式约束函数必须是仿射函数</li>
</ul>
<p>至于为什么等式约束必须是仿射函数，这里有个直观的解释：等式约束可以看成要同时满足$h_i(x)\leq 0$和$-h_i(x)\leq 0$,为了满足不等式约束的条件，要求$h_i(x)$同时是凸函数和凹函数，这样的函数只能是仿射函数。</p>
<p>凸优化问题有一个很好的性质：任意局部最优解也是全局最优解。<br>对于无约束条件的凸优化问题，$x$是其最优解的充要条件是：</p>
<script type="math/tex; mode=display">
\nabla f_0 (x)=0 \\
\tag{3-2}</script><h1 id="4-对偶"><a href="#4-对偶" class="headerlink" title="4. 对偶"></a>4. 对偶</h1><h2 id="4-1-Lagrange函数与Lagrange对偶"><a href="#4-1-Lagrange函数与Lagrange对偶" class="headerlink" title="4.1. Lagrange函数与Lagrange对偶"></a>4.1. Lagrange函数与Lagrange对偶</h2><p>回到前面提到的标准形式的优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ \ &f_0(x)\\
s.t.\ \ &f_i(x)\leq 0,i=1,2,\cdots,m\\
&h_i(x)=0,i=1,2,\cdots,p\\
\tag{4-1}
\end{align*}</script><p>注意，这里没有要求是凸优化问题。<br><strong>Lagrange对偶</strong>的基本思想是，在目标函数中考虑$(4-1)$的约束条件，即添加约束条件的加权和，得到增广的目标函数，称之为<strong>Lagrange函数：</strong></p>
<script type="math/tex; mode=display">
L(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^{m}{\lambda _if_i(x)} + \sum_{i=1}^{p}{\nu _ih_i(x)}\\
\tag{4-2}</script><p>注意，Lagrange函数的定义域是$D\times R^m\times R^p$，在后面的讨论中，我们会假设$\lambda_i\geq 0$<br>向量$\lambda$和$\nu$成为对偶变量，或者是问题$(4-1)$的Lagrange乘子向量。<br>Lagrange对偶函数定义为Lagrange函数关于x取得的最小值：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda,\nu)&=\mathop{inf}\limits_{x\in D}L(x,\lambda,\nu)\\
&= \mathop{inf}\limits_{x\in D}\left(f_0(x)+\sum_{i=1}^{m}{\lambda _if_i(x)} + \sum_{i=1}^{p}{\nu _ih_i(x)}\right)\\
\tag{4-3}
\end{align*}</script><p>Lagrange对偶函数是Lagrange函数的<strong>逐点下确界有</strong>，有个很重要的性质：无论原问题是不是凸的，Lagrange对偶函数都是凹函数。下面分别从理论上进行证明，以及从几何上形象地解释。<br><strong>理论证明</strong>：<br>不难看出，$g(\lambda,\nu)$是关于$\lambda,\nu$的仿射函数，为了书写简简洁，我们用一个长的向量$\mu$代表$(\lambda,\nu)$<br>要想证明$g(\lambda,\nu)$是凹函数，只需证明$\forall \mu_1,\mu_2$下式都成立：</p>
<script type="math/tex; mode=display">
g(\theta \mu_1+(1-\theta)\mu_2)\geq \theta g(\mu_1)+(1-\theta)g(\mu_2)\\
\tag{4-4}</script><p>下面是证明过程：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\theta \mu_1+(1-\theta)\mu_2)&=\mathop{min}\limits_{x}L(x,\theta \mu_1+(1-\theta)\mu_2)\\
&=\mathop{min}\limits_{x}\left(\theta L(x, \mu_1)+(1-\theta)L(x, \mu_2)\right)\\
&\geq \mathop{min}\limits_{x}\left(\theta L(x, \mu_1)\right)+\mathop{min}\limits_{x}\left((1-\theta)L(x, \mu_2)\right)\\
&=\theta\mathop{min}\limits_{x}\left( L(x, \mu_1)\right)+(1-\theta)\mathop{min}\limits_{x}\left(L(x, \mu_2)\right)\\
&=\theta g(\mu_1)+(1-\theta)g(\mu_2)\\
\tag{4-5}
\end{align*}</script><p>得证！<br>注意，第一步到第二步是因为$L(x,\mu)$是关于$u$的仿射函数；第二步到第三步是因为，地二步中取得最小值时，括号中两项中的$x$是取相同的值的，而第三步中两项分别取最小值不要求$x$一定取相同值（也即能够比第二步涵盖更多情况），因此第三步可能取到的最小值肯定小于或等于第二步的最小值。<br><strong>几何解释</strong>如下：<br>由于$L(x,\mu)$是关于$u$的仿射函数，我们将$\mu$退化为1维来形象地解释。$L(x,\mu)$可以看成是许多的直线簇组成。$g(x,\mu)$可以理解成：当$\mu$取某一个值时，取曲线簇在这个值上的最小值，遍历所有$\mu$，将曲线簇的一些最小值作为$g(x,\mu)$的值域。因此，$g(x,\mu)$可以看成下图中黄色区域的边界线，显然是一个凹函数。<br><img src="https://github.com/lankuohsing/Markdown-Images/raw/master/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/%E5%87%B8%E4%BC%98%E5%8C%96/%E9%80%90%E7%82%B9%E4%B8%8B%E7%A1%AE%E7%95%8C%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A.JPG" alt="逐点下确界几何解释"></p>
<p>此外，Lagrange对偶函数还有如下性质：<br>$\forall \lambda \succeq 0$(每一维都大于0)和$\nu$，都有</p>
<script type="math/tex; mode=display">
g(\lambda,\nu)\leq p^{\star}\\
\tag{4-6}</script><p>其中$p^{\star}$是原问题的最优值。也即，对偶函数构成了原问题的最优值的下界。</p>
<h2 id="4-2-共轭函数"><a href="#4-2-共轭函数" class="headerlink" title="4.2. 共轭函数"></a>4.2. 共轭函数</h2><p>设函数$f:R^n\rightarrow R$，定义$f^{\star}:R^n\rightarrow R$为：</p>
<script type="math/tex; mode=display">
f^{\star}(y)=\mathop{sup}\limits_{x\in dom\ f}\left(y^Tx-f(x)\right)\\
\tag{4-7}</script><p>此函数成为函数$f$的共轭函数。共轭函数是一系列仿射函数的逐点上确界，所以必然是一个凸函数。<br>对于负熵函数$xlog{x}$，它的共轭函数不难推导出是$f^{\star}(y)=e^{y-1}$,这在后面会用到</p>
<h2 id="4-3-Lagrange对偶问题"><a href="#4-3-Lagrange对偶问题" class="headerlink" title="4.3. Lagrange对偶问题"></a>4.3. Lagrange对偶问题</h2><p>由$(4-6)$可以看到，对于任意一组$(\lambda,\nu)$，其中$\lambda \succeq0$,Lagrange对偶函数给出了优化问题$(4-1)$的最优值$p^{\star}$的一个下界。我们来看一下从Lagrange函数得到的<strong>最好下界</strong>。该问题可以表述为如下优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
maximize\ \ g(\lambda,\nu)\\
subject\ to\ \ \lambda\succeq 0
\tag{4-8}
\end{align*}</script><p>上述问题被称为<strong>原问题</strong>的<strong>Lagrange对偶问题</strong>。<br>满足$\lambda \succeq 0$和$g(\lambda,\nu)&gt;0$的一组$(\lambda,\nu)$被称为一组<strong>对偶可行解</strong>。如果一组$(\lambda^{\star},\nu^{\star})$是对偶问题的最优解，那么称它是<strong>对偶最优解</strong>或者<strong>最优Lagrange乘子</strong>。<br>由于$g(\lambda,\nu)&gt;0$必然是凹函数，且约束条件是凸函数，所以问题$(4-8)$必然是一个凸优化问题。<br>因此<strong>Lagrange对偶问题是一个凸优化问题，与原问题的凸性无关</strong></p>
<p>记Lagrange对偶问题的最优值为$d^{\star}$，原问题的最优值为$p^{\star}$。显然有$d^{\star}\leq p^{\star}$，这个性质称为<strong>弱对偶性</strong>。</p>
<h2 id="4-4-强对偶性与Slater约束准则"><a href="#4-4-强对偶性与Slater约束准则" class="headerlink" title="4.4. 强对偶性与Slater约束准则"></a>4.4. 强对偶性与Slater约束准则</h2><p>如果前面的有$d^{\star}=p^{\star}$，则<strong>强对偶性</strong>成立。<br>强对偶性成立的一个简单的约束条件是：存在一点$x\in relint\ D$使得下式成立：</p>
<script type="math/tex; mode=display">
f_i(x)< 0,i1,\cdots,m,Ax=b\\
\tag{4-9}</script><p>如果不等式约束函数中有一些是仿射函数时，Slater条件可以进一步改进为：不是仿射函数的那些不等式约束函数需要满足$(4-9)$。换言之，仿射不等式不需要严格成立。<br>由此可以得到一个推论：当所有约束条件是线性等式或线性不等式且$dom\ f_0$是开集时，改进的Slater条件就是可行性条件。也即只要问题是可行的，强对偶性就成立。<br>Boyd的《Convex Optimization》一书中，证明了<strong>当原问题是凸问题且Slater条件成立时，强对偶性成立。</strong></p>
<h2 id="4-5-最优性条件"><a href="#4-5-最优性条件" class="headerlink" title="4.5. 最优性条件"></a>4.5. 最优性条件</h2><p>注意，此小节讨论的问题并不要求是凸问题。</p>
<h3 id="4-5-1-互补松弛性"><a href="#4-5-1-互补松弛性" class="headerlink" title="4.5.1. 互补松弛性"></a>4.5.1. 互补松弛性</h3><p>如果强对偶性成立，则有：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_0(x^{\star})&=g(\lambda^{\star},\nu^{\star})\\
&=\mathop{inf}\limits_{x}\left(f_0(x)+\sum_{i=1}^{m}{\lambda _i^{\star}f_i(x)} + \sum_{i=1}^{p}{\nu _i^{\star}h_i(x)}\right))\\
&\leq f_0(x^{\star})+\sum_{i=1}^{m}{\lambda _i^{\star}f_i(x^{\star})} + \sum_{i=1}^{p}{\nu _i^{\star}h_i(x^{\star})}\\
&\leq f_0(x^{\star})\\
\tag{4-10}
\end{align*}</script><p>上式可以得到几个有用的结论：</p>
<ul>
<li>由于第三个不等式取等号，说明$L(x,\lambda^{\star},\nu^{\star})$在$x^{\star}$处取得局部最小值，也即该点处导数为0</li>
<li>$\lambda_i^{\star}f_i(x^{\star})=0,i=1,2,\cdots,m$，这个称为<strong>互补松弛条件</strong>，意味着在最优点处，不等式约束要么取等号$(f_i(x^{\star})=0)$，要么它对应的Lagrange乘子为零$\lambda_i^{\star}=0$</li>
</ul>
<h3 id="4-5-2-KKT最优性条件"><a href="#4-5-2-KKT最优性条件" class="headerlink" title="4.5.2. KKT最优性条件"></a>4.5.2. KKT最优性条件</h3><p>这小节讨论的目标函数$f_0$和约束函数$f_1,f_2,\cdots,f_m,h_1,h_2,\cdots,h_p$是可微的，但并不要求它们都是凸函数。<br>结合上一小节的内容，我们可以推出，对于目标函数和约束函数可微的任意优化问题，如果强对偶性成立，则任一对偶问题的最优解和对偶问题的最优解必须满足下列的式子：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_i(x^{\star})&\leq 0 ,i=1,2,\cdots,m\\
h_i(x^{\star})&=0,i=1,2,\cdots,p\\
\lambda_i^{\star}&\geq 0,i=1,2,\cdots,m\\
\lambda_i^{\star}f_i(x^{\star})&=0,i=1,2,\cdots,m\\
\nabla f_0(x^{\star})+\sum_{i=1}^{m}{\lambda _i^{\star}\nabla f_i(x^{\star})} + \sum_{i=1}^{p}{\nu _i^{\star}\nabla h_i(x^{\star})}&=0\\
\tag{4-9}
\end{align*}</script><p>上式被称为<strong>非凸问题的KKT条件</strong>：<br><strong>如果原问题是凸问题，则满足KKT条件的点也是原、对偶问题的最优解</strong>。这个定理很重要！<br>上述定理的证明：前面两个条件说明了$x^{\star}$是原问题的可行解；因为$\lambda^{\star}\geq 0$，所以$L(x,\lambda^{\star},\nu^{\star})$是x的凸函数；最优一个条件说明了Lagrange函数在$x^{\star}$处导数为零，也即Lagrange函数取得全局最小值，因此此时有：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda^{\star},\nu^{\star})&=L(x^{\star},\lambda^{\star},\nu^{\star})\\
&=f_0(x^{\star})+\sum_{i=1}^{m}{\lambda _i^{\star}f_i(x^{\star})} + \sum_{i=1}^{p}{\nu _i^{\star}h_i(x^{\star})}\\
&=f_0(x^{\star})\\
\tag{4-10}
\end{align*}</script><p>上述意味着对偶间隙为0，强对偶性成立，因此得证。</p>
<h3 id="4-5-3-通过解对偶问题求解原问题"><a href="#4-5-3-通过解对偶问题求解原问题" class="headerlink" title="4.5.3. 通过解对偶问题求解原问题"></a>4.5.3. 通过解对偶问题求解原问题</h3><p>由前面可知，如果强对偶性成立，且存在一个对偶最优解$(\lambda^{\star},\nu^{\star})$，那么任意原问题最优点也是$L(x,\lambda^{\star},\nu^{\star})$的最优解。利用这个性质，我们可以从对偶最优方程中去求解原问题最优解。确切的讲，如果强对偶性成立，对偶最优解$(\lambda^{\star},\nu^{\star})$已知，并且下列问题的解唯一：</p>
<script type="math/tex; mode=display">
min\ f_0(x)+\sum_{i=1}^{m}{\lambda _if_i(x)} + \sum_{i=1}^{p}{\nu _ih_i(x)}\tag{4-11}</script><p>（Lagrange函数是严格凸函数时上述最优化问题的解是唯一的），如果上式问题的解是原问题的可行解，那么它就是原问题的最优解；如果它不是原问题的可行解，那么原问题不存在最优解（或者无法达到）。当对偶问题比原问题更容易求解时，上述方法很有意义。</p>
<h1 id="5-利用Lagrange对偶求解最优化问题的例子"><a href="#5-利用Lagrange对偶求解最优化问题的例子" class="headerlink" title="5. 利用Lagrange对偶求解最优化问题的例子"></a>5. 利用Lagrange对偶求解最优化问题的例子</h1><h2 id="5-1-熵的最大化问题"><a href="#5-1-熵的最大化问题" class="headerlink" title="5.1. 熵的最大化问题"></a>5.1. 熵的最大化问题</h2><p>这个例子在机器学习中可能会经常遇到。问题描述如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
min\ f_0(x)&=\sum_{i=1}^{n}{x_ilog{x_i}}\\
s.t.\ Ax&\preceq b\\
1^Tx&=1\\
\tag{5-1}
\end{align*}</script><p>定义域为$R_{++}$<br>记目标函数为$f_0(x)$Lagrange函数为：</p>
<script type="math/tex; mode=display">
\begin{align*}
L(x,\lambda,\nu)=f_0(x)+\lambda^T(Ax-b)+\nu(\vec 1^Tx-1)\\
\tag{5-2}
\end{align*}</script><p>Lagrange对偶函数为：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda,\nu)&=\mathop{inf}\limits_{x}\ \left(f_0(x)+\lambda^T(Ax-b)+\nu(1^Tx-1)\right)\\
&=-b^T\lambda-\nu+\mathop{inf}\limits_{x}\ \left(f_0(x)+(A^T\lambda+\vec 1\nu)^Tx\right)\\
&=-b^T\lambda-\nu-\mathop{sup}\limits_{x}\ \left(-f_0(x)-(A^T\lambda+\vec 1\nu)^Tx\right)\\
&=-b^T\lambda-\nu-f_0^{\star}\left(-(A^T\lambda+\vec 1\nu)\right)\\
\tag{5-3}
\end{align*}</script><p>其中$f_0^{\star}$是$f_0$的共轭函数。对于负熵函数$xlog{x}$，它的共轭函数不难推导出是$f^{\star}(y)=e^{y-1}$，因此不难得出$(5-3)$可进一步化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
g(\lambda,\nu)&=\mathop{inf}\limits_{x}\ \left(f_0(x)+\lambda^T(Ax-b)+\nu(1^Tx-1)\right)\\
&=-b^T\lambda-\nu-\sum_{i=1}^{n}e^{\left(-a_i^T\lambda-\nu-1\right)}\\
\tag{5-3}
\end{align*}</script><p>假设原问题可行，也即Slater条件成立（注意这里的约束条件都是仿射函数），那么此时强对偶性成立。因此对Lagrange函数求最小值即可求得原问题的最小值解。注意到Lagrange函数是严格凸函数，很容易求得最小值点</p>
<script type="math/tex; mode=display">
x_i^{\star}=exp\left(-(a_i^T\lambda^{\star}+\nu^{\star}+1)\right),i=1,2,\cdots,n\\
\tag{5-4}</script><p>其中$a_i$是$A$的列向量，如果$x^{\star}$是原问题的可行解，则必定是原问题的最优解。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/10/03/seq2seq入门详解：从RNN到Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/10/03/seq2seq入门详解：从RNN到Attention/" itemprop="url">seq2seq入门详解：从RNN到Attention</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-03T23:09:18+08:00">
                2020-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>@[toc]</p>
<h1 id="1-前馈神经网络的缺点"><a href="#1-前馈神经网络的缺点" class="headerlink" title="1. 前馈神经网络的缺点"></a>1. 前馈神经网络的缺点</h1><p>对于输入向量中个分量的位置信息不感知，也即无法利用序列型输入特征向量中的位置信息（将个分量调换顺序最后训练出的模型是等价的），但是在实际的任务中，各分量是有先后关系的。例如，我们在理解一段文本时，孤立地理解每个字或者词是不够的，还要将它们作为一个整体的序列来理解。</p>
<h1 id="2-循环神经网络RNN"><a href="#2-循环神经网络RNN" class="headerlink" title="2. 循环神经网络RNN"></a>2. 循环神经网络RNN</h1><h2 id="2-1-RNN的基本结构与数学定义"><a href="#2-1-RNN的基本结构与数学定义" class="headerlink" title="2.1. RNN的基本结构与数学定义"></a>2.1. RNN的基本结构与数学定义</h2><p>RNN的输入数据，一般有三个维度：batch大小，时间长度，特征维数。TensorFlow中的RNN层API的输入数据shape为[batch, timesteps, feature]。因为本节的图片来自Andrew NG的Coursera公开课中的例子，因此这里的RNN输入数据形状将以Andrew NG的习惯为例，这不影响原理的讲解。输入层的维数是$(n_x,m,T_x)$,其中$n_x$是每个训练样本的维数；$m$是一个batch的大小；$T_x$是输入序列的长度。</p>
<p>输出层的维数是$(n_y,m,T_y)$,其中$n_y$是输出预测向量的维数；$m$是一个batch的大小；$T_y$是输出序列的长度。</p>
<p>我们先研究输入向量和输出向量相等，即$n_x=n_y$的情况，结构图如下所示（图片来源<a href="https://www.coursera.org/learn/nlp-sequence-models/notebook/X20PE/building-a-recurrent-neural-network-step-by-step）。" target="_blank" rel="noopener">https://www.coursera.org/learn/nlp-sequence-models/notebook/X20PE/building-a-recurrent-neural-network-step-by-step）。</a><br><img src="https://img-blog.csdnimg.cn/20200915224055966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图2.1 RNN基本结构</center>


<p>上下标说明举例：$a_5^{(2)[3]<4>}$表示第2个训练样本，第3层，第4个时刻，激活函数输出向量的第5维。<br>每个RNN-Cell的内部结构见下图<br><img src="https://img-blog.csdnimg.cn/20200915224125412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></4></p>
<center>图2.2 RNN的一个基本单元</center>

<p>注意，输出$\hat y$是状态向量$a$经过线性变换再经过softmax变换得到的。</p>
<script type="math/tex; mode=display">
\begin{align*}
a^{\langle t\rangle}&=tanh\left(W_{ax}x^{\langle t\rangle}+W_{aa}a^{\langle t-1\rangle}+b_a\right)\\
\hat y^{\langle t\rangle}&=softmax\left(W_{ya}a^{\langle t\rangle}+b_y\right)\\
\tag{2-1}
\end{align*}</script><p>需要注意的是，在不同的RNN-Cell中，上述公式里面的参数$W,b$都是共享的。</p>
<h2 id="2-2-输入输出长度的讨论"><a href="#2-2-输入输出长度的讨论" class="headerlink" title="2.2. 输入输出长度的讨论"></a>2.2. 输入输出长度的讨论</h2><h3 id="2-2-1-n-x-n-y-n"><a href="#2-2-1-n-x-n-y-n" class="headerlink" title="2.2.1. $n_x=n_y=n$"></a>2.2.1. $n_x=n_y=n$</h3><p>第一种情况是输入输出长度相等的情况，如下图所示（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://img-blog.csdnimg.cn/20200915224210915.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图2.3 RNN-输入输出长度相等</center>

<p>常用于序列标注模型，例如命名实体识别模型中。</p>
<h3 id="2-2-2-n-x-n-n-y-1"><a href="#2-2-2-n-x-n-n-y-1" class="headerlink" title="2.2.2. $n_x=n,n_y=1$"></a>2.2.2. $n_x=n,n_y=1$</h3><p>第二种情况是输入长度为N，输出长度为1（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://img-blog.csdnimg.cn/20200915224233564.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图2.4 RNN-输出长度为1</center>

<p>模型只在最后一个时刻输出，常用于文本分类模型<br>利用RNN网络预测sin函数的代码例子：<a href="https://github.com/lankuohsing/TensorFlow-Examples/blob/master/RNN/RNN_sin.py" target="_blank" rel="noopener">https://github.com/lankuohsing/TensorFlow-Examples/blob/master/RNN/RNN_sin.py</a></p>
<h3 id="2-2-3-n-x-1-n-y-n"><a href="#2-2-3-n-x-1-n-y-n" class="headerlink" title="2.2.3. $n_x=1,n_y=n$"></a>2.2.3. $n_x=1,n_y=n$</h3><p>第三种情况是输入长度为1，输出长度为N。uti实现时，可以将输入作为最开始时刻的输入，也可以作为所有时刻的输入（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://img-blog.csdnimg.cn/20200915224253817.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图2.5 RNN-输入长度为1</center>

<p><img src="https://img-blog.csdnimg.cn/20200915224310543.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图2.6 RNN-输入长度为1（输入特征在所有时刻重复使用）</center>

<p>常用于文字生成模型中。</p>
<h3 id="2-2-4-n-x-n-n-y-m-，Encoder-Decoder模型"><a href="#2-2-4-n-x-n-n-y-m-，Encoder-Decoder模型" class="headerlink" title="2.2.4. $n_x=n,n_y=m$，Encoder-Decoder模型"></a>2.2.4. $n_x=n,n_y=m$，Encoder-Decoder模型</h3><p>第四种情况是输入长度为N,输出长度为M的情况，也即Encoder-Decoder模型（图片来源<a href="https://www.jianshu.com/p/c5723c3bb921" target="_blank" rel="noopener">https://www.jianshu.com/p/c5723c3bb921</a>）<br><img src="https://img-blog.csdnimg.cn/2020091522433218.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图2.7 Encoder-Decoder模型，输入输出长度为一般情况的RNN</center>

<p>常用于语音识别、机器翻译等场景。在后面的章节中我们会详细介绍Encoder-Decoder模型</p>
<h1 id="3-RNN的复杂变种"><a href="#3-RNN的复杂变种" class="headerlink" title="3. RNN的复杂变种"></a>3. RNN的复杂变种</h1><h2 id="3-1-GRU-Gated-Recurrent-Unit"><a href="#3-1-GRU-Gated-Recurrent-Unit" class="headerlink" title="3.1. GRU(Gated Recurrent Unit)"></a>3.1. GRU(Gated Recurrent Unit)</h2><p>GRU的提出是为了解决RNN难以学习到输入序列中的长距离信息的问题。<br>GRU引入一个新的变量——记忆单元，简称$C$。$C^{\langle t\rangle}$其实就是$a^{\langle t\rangle}$<br>$C$的表达式不是一步到位的，首先定义$C$的候选值$\tilde C$:</p>
<script type="math/tex; mode=display">
\tilde C^{\langle t\rangle}=tanh\left(W_c[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)</script><p>更新门：</p>
<script type="math/tex; mode=display">
\Gamma_u=\sigma\left(W_u[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)</script><p>在实际训练好的网络中$\Gamma$要么很接近1要么很接近0，对应着输入序列里面有些元素起作用有些元素不起作用。</p>
<script type="math/tex; mode=display">
C^{\langle t\rangle}=\Gamma_u*\tilde C^{\langle t\rangle}+（1-\Gamma_u）* C^{\langle t-1\rangle}</script><p>也即输入序列的有些元素，记忆单元不需要更新，有些元素需要更新。</p>
<blockquote>
<p>The cat, which already ate …, was full</p>
</blockquote>
<p>cat后面的词直到was之前，都不需要更新$C$,直接等于cat对应的$C$<br>可以解决梯度消失的问题.输出层的梯度可以传播到cat处</p>
<p>注：$C$和$\Gamma$都可以是想聊，它们在相乘时采用的是element-wise的乘法。当为向量时，与cat的单复数无关的词对应的$\Gamma$可能有些维度为零，有些维度不为零。为零的维度，是用来保留cat的单复数信息的；不为零的维度可能是保留其他语义信息的，比如是不是food呀之类的<br>目前讨论的是简化版的GRU，结构图如下<br><img src="https://img-blog.csdnimg.cn/20200915224500120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图3.1GRU的一个基本单元</center>

<p>完整的GRU：</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[\Gamma_r*C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_r&=\sigma\left(W_r[C^{\langle t-1\rangle},x^{\langle t\rangle}]+b_r\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+（1-\Gamma_u）* C^{\langle t-1\rangle}\\
a^{\langle t\rangle}&=C^{\langle t\rangle}\\
\tag{3-1}
\end{align*}</script><p>$\Gamma_r$表示了$\tilde C^{\langle t\rangle}$和$C^{\langle t-1\rangle}$之间的相关程度</p>
<h2 id="3-2-LSTM-Long-Short-Term-Memory"><a href="#3-2-LSTM-Long-Short-Term-Memory" class="headerlink" title="3.2. LSTM(Long Short-Term Memory)"></a>3.2. LSTM(Long Short-Term Memory)</h2><p>没有了$\Gamma_r$，将$1-\Gamma_u$用$\Gamma_f$代替</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_f&=\sigma\left(W_f[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_f\right)\\
\Gamma_o&=\sigma\left(W_o[a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_o\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+\Gamma_f* C^{\langle t-1\rangle}\\
a^{\langle t\rangle}&=\Gamma_o*tanh\left(C^{\langle t\rangle}\right)\\
\tilde y^{\langle t\rangle}&=softmax(a^{\langle t\rangle})\\
\tag{3-2}
\end{align*}</script><p>(注意公式里面的$\Gamma_u$等价于图片中的$\Gamma_i$)</p>
<p><img src="https://img-blog.csdnimg.cn/20200915224534385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图3.2 LSTM的一个基本单元</center>

<p><img src="https://img-blog.csdnimg.cn/2020091522455885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图3.3 标准LSTM模型-输入维数等于输出维数</center>

<h3 id="3-2-1-peephole连接"><a href="#3-2-1-peephole连接" class="headerlink" title="3.2.1. peephole连接"></a>3.2.1. peephole连接</h3><p><img src="https://img-blog.csdnimg.cn/20200915224655547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图3.4 LSTM带有peephole</center>

<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[a^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_f&=\sigma\left(W_f[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_f\right)\\
\Gamma_o&=\sigma\left(W_o[c^{\langle t\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_o\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+\Gamma_f* C^{\langle t-1\rangle}\\
a^{\langle t\rangle}&=\Gamma_o*tanh\left(C^{\langle t\rangle}\right)\\
\tilde y^{\langle t\rangle}&=softmax(a^{\langle t\rangle})\\
\tag{3-3}
\end{align*}</script><h3 id="3-2-2-projection"><a href="#3-2-2-projection" class="headerlink" title="3.2.2 projection"></a>3.2.2 projection</h3><p>对隐藏层状态a进行一次线性变换，降低其维数</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde C^{\langle t\rangle}&=tanh\left(W_c[a^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_c\right)\\
\Gamma_u&=\sigma\left(W_u[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_u\right)\\
\Gamma_f&=\sigma\left(W_f[c^{\langle t-1\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_f\right)\\
\Gamma_o&=\sigma\left(W_o[c^{\langle t\rangle},a^{\langle t-1\rangle},x^{\langle t\rangle}]+b_o\right)\\
C^{\langle t\rangle}&=\Gamma_u*\tilde C^{\langle t\rangle}+\Gamma_f* C^{\langle t-1\rangle}\\
a_0^{\langle t\rangle}&=\Gamma_o*tanh\left(C^{\langle t\rangle}\right)\\
a^{\langle t\rangle}&=W_{proj}a_0^{\langle t\rangle}+b_{proj}\\
\tilde y^{\langle t\rangle}&=softmax(a^{\langle t\rangle})\\
\tag{3-4}
\end{align*}</script><h1 id="4-Encoder-Decoder模型"><a href="#4-Encoder-Decoder模型" class="headerlink" title="4. Encoder-Decoder模型"></a>4. Encoder-Decoder模型</h1><p>由前面的章节我们知道，Encoder-Decoder模型就是输入输出长度为一般情况的RNN模型，示意图如下：<br><img src="https://img-blog.csdnimg.cn/20200915224730164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图4.1 Encoder-Decoder</center>

<p>其中Encoder负责将输入进行编码，得到语义编码向量C；Decoder负责将语义编码向量C进行解码，得到输出。以机器翻译为例，英文作为输入，输出为中文。可以用如下的数学模型来表示：</p>
<script type="math/tex; mode=display">
\begin{align*}
input&= ( x_1,x_2,\cdots,x_n )\\
C&=f(input)\\
y_i&=g( C,y_1,y_2,\cdots,y_{i-1} ),i=1,2,\cdots,m\\
output&=( y_1,y_2,\cdots,y_m )\\
\tag{4-1}
\end{align*}</script><p>从Encoder得到C的方式有多种，可以将Encoder最后一个时刻的隐藏状态作为C，也可以将所有的隐藏状态进行某种变换得到C。<br>语义编码C在Decoder中的作用当时有多种，常见的有如下两种<br>（1） C作为Decoder的初始状态$h_0$。<br><img src="https://img-blog.csdnimg.cn/20200915224752523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图4.2 C作为Decoder的初始状态h0</center>

<p>（2） C作为Decoder的每一步输入。<br><img src="https://img-blog.csdnimg.cn/20200915224806800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图4.2 C作为Decoder的每一步输入</center>


<h2 id="4-1-几种典型的encoder-decoder"><a href="#4-1-几种典型的encoder-decoder" class="headerlink" title="4.1. 几种典型的encoder-decoder"></a>4.1. 几种典型的encoder-decoder</h2><h3 id="4-1-1-第一种-语义编码C作为Decoder的初始输入"><a href="#4-1-1-第一种-语义编码C作为Decoder的初始输入" class="headerlink" title="4.1.1. 第一种:语义编码C作为Decoder的初始输入"></a>4.1.1. 第一种:语义编码C作为Decoder的初始输入</h3><p>本小节的encode-decoder模型可以看成是seq2seq的开山之作，来源于google的论文<a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">https://arxiv.org/abs/1409.3215</a> Sequence to Sequence Learning with Neural Networks。该论文的模型是为了翻译问题而提出的，其中的encoder和decoder都采用LSTM，decoder中采用了beam search来提升效果。此外，该论文还采用了一个tric——将输入源句子倒序输入。这是因为，无论RNN还是LSTM，其实都是有偏的，即顺序越靠后的单词最终占据的信息量越大。如果源句子是正序的话，则采用的是最后一个词对应的state来作为decoder的输入来预测第一个词。这样是是不符合直觉的，因为没有对齐。将源句子倒序后，某种意义上实现了一定的对齐。<br><img src="https://img-blog.csdnimg.cn/20200924234108210.jpg#pic_center" alt="在这里插入图片描述"></p>
<center>图4.3 Encoder-Decoder框架1-C作为Decoder的初始输入</center>

<p>Encoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},x_t]+b)\\
c&=h_T\\
\tag{4-2}
\end{align*}</script><p>Decoder:</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},y_{t-1}]+b),h_0=c\\
o_t&=softmax(Vh_t+d)\\
\tag{4-3}
\end{align*}</script><h3 id="4-1-2-第一种-语义编码C作为Decoder的每一步输入"><a href="#4-1-2-第一种-语义编码C作为Decoder的每一步输入" class="headerlink" title="4.1.2. 第一种:语义编码C作为Decoder的每一步输入"></a>4.1.2. 第一种:语义编码C作为Decoder的每一步输入</h3><p><a href="https://arxiv.org/pdf/1406.1078" target="_blank" rel="noopener">https://arxiv.org/pdf/1406.1078</a> Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation<br><img src="https://img-blog.csdnimg.cn/20200924234038757.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图4.4 Encoder-Decoder框架2-C作为Decoder的每一步输入</center>

<p>Encoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},x_t]+b)\\
c&=(Vh_T)\\
\tag{4-4}
\end{align*}</script><p>Decoder:</p>
<script type="math/tex; mode=display">
\begin{align*}
h_t&=tanh(W[h_{t-1},y_{t-1},c]+b)\\
o_t&=softmax(Vh_t+c)\\
\tag{4-5}
\end{align*}</script><h2 id="4-2-Encoder-Decoder的缺点"><a href="#4-2-Encoder-Decoder的缺点" class="headerlink" title="4.2. Encoder-Decoder的缺点"></a>4.2. Encoder-Decoder的缺点</h2><ol>
<li>对于输入序列的每个分量的重要程度没有区分，这和人的思考过程是不相符的，例如人在翻译的时候，对于某个一词多义的词，可能会结合上下文中某些关键词进行辅助判断。</li>
<li>如果在Decoder阶段，仅仅将C作为初始状态，随着时间往后推进，C的作用会越来越微弱。</li>
</ol>
<p>事实上，Attention机制的提出，主要就是为了解决上述问题。</p>
<h1 id="5-Attention机制详解"><a href="#5-Attention机制详解" class="headerlink" title="5. Attention机制详解"></a>5. Attention机制详解</h1><p>前面讲到，在一般形式的encoder-decoder中，输入信息先经过encoder编码保存在C中，C再被decoder使用。这种“直接粗暴”的方式，可能会导致输入信息没有被合理的利用，尤其是当输入信息过长的时候。为了解决这个问题，Attention机制被提出，解决的思路是：在decoder阶段，每个时间点输入的C是不同的(示意图如下图所示)，需要根据当前时刻要输出的y去合理地选择输入x中的上下文信息。<br><img src="https://img-blog.csdnimg.cn/20200915224835251.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图5.1 Attention机制示意图</center>

<p>具体来讲，就是对encoder的隐藏状态进行加权求和，以便得到不同的C，以中文翻译英文为例，示意图如下：<br><img src="https://img-blog.csdnimg.cn/20200915224848492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图5.2 Attention-对encoder隐藏状态进行加权求和得到不同的C</center>

<p>记$a_{ij}$为encoder中第$j$个隐藏状态$h_j$到decoder中第$i$个隐藏状态$h_i’$对应的$c_i$的权重，可以通过训练确定的，具体计算方法见后文。attention机制的核心思想可以概括为”对输入信息加权求和得到编码信息c”，也即如下公式：</p>
<script type="math/tex; mode=display">
c_i=\sum_{j=1}^{n_x}a_{ij}h_j\tag{5-1}</script><h2 id="5-1-attention机制中权重系数的计算过程"><a href="#5-1-attention机制中权重系数的计算过程" class="headerlink" title="5.1. attention机制中权重系数的计算过程"></a>5.1. attention机制中权重系数的计算过程</h2><p>attention机制中权重系数有多种计算过程，对应于不同种类的attention机制。但是大部分的attention机制，都能表示为下文提到的三个抽象阶段。这里先引入几个概念。<br>我们将模型输入内容记为source，输出内容记为target。<br>source可以表示为一个一个的<key,value>，target则表示为一个一个的query。在机器翻译中，key和value合并为一个，就是输入句子中每个单词对应的隐藏层状态。<br>通过计算Query和各个Key的相似性或者相关性（需要进行softmax归一化），得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。</key,value></p>
<script type="math/tex; mode=display">
Attention(Query_i,Source)=\sum_{j=1}^{L_x}Similarity(Query,key_j)*value_j\tag{5-2}</script><p>具体来说可以分为三个阶段，如下图所示：<br><img src="https://img-blog.csdnimg.cn/2020091522491044.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图5.3 attention机制中计算权重系数的三个阶段</center>

<p>其中第一阶段计算相似性时有多种方法，例如向量点积、余弦相似度，甚至可以用一个小的神经网络来通过学习的方式计算。<br>第二阶段softmax归一化的公式如下：</p>
<script type="math/tex; mode=display">
a_{ij}=softmax(S_{ij})=\frac{exp(Sim_{ij})}{\sum_{j=1}^{L_x}exp(Sim_{ij})}\tag{5-3}</script><h2 id="5-2-几种典型的attention机制"><a href="#5-2-几种典型的attention机制" class="headerlink" title="5.2. 几种典型的attention机制"></a>5.2. 几种典型的attention机制</h2><h3 id="5-2-1-第一种-Bahdanau-Attention"><a href="#5-2-1-第一种-Bahdanau-Attention" class="headerlink" title="5.2.1. 第一种:Bahdanau Attention"></a>5.2.1. 第一种:<strong>Bahdanau Attention</strong></h3><p><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">https://arxiv.org/abs/1409.0473</a> Neural Machine Translation by Jointly Learning to Align and Translate，这篇论文可以看做是attention机制的开山论文。该论文也是为了解决翻译问题而提出的模型，encoder采用了双向的RNN结构，正向RNN和反向RNN的每个时刻的隐藏state拼接成一个两倍长的新的state。如果要用一个简洁的公式来概括attention，那就是<strong>mlp + softmax+加权求和</strong>。之前的模型都是用一个固定长度的语义向量C来将encoder和decode连接起来，而这篇论文里是采用一个attention模型来连接。<br><img src="https://img-blog.csdnimg.cn/20200926002437289.JPG#pic_center" alt="在这里插入图片描述"></p>
<center>图5.4 attention机制框架1</center>


<p>Encoder：</p>
<script type="math/tex; mode=display">
\begin{align*}
h_i&=tanh(W[h_{i-1},x_i]+b)\\
\tag{5-4}
\end{align*}</script><p>语义向量：</p>
<script type="math/tex; mode=display">
\begin{align*}
e_{ij}&=a(s_{i-1},h_j)\\
\alpha_{ij}&=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}\\
c_i&=\sum_{j=1}^{T_x}\alpha_{ij}h_j\\
\tag{5-5}
\end{align*}</script><p>其中$e_{ij}$是Encoder中$j$时刻隐藏层状态$h_j$对Decoder中$i$时刻的处处状态$s_i$的影响程度；$a(s_{i-1},h_j)$称为一个对齐模型，本论文中采用的是一个单隐藏层的多层感知机$a(s_{i-1},h_j)=v_a^T tanh(W_a s_{i-1}+U_ah_i)$，注意在实际推理的时候，$U_ah_i$是可以在推理之前预先算好的以减少推理计算量；$\alpha_{ti}$是对$e_{ti}$进行softmax归一化成的概率，也即前文提到的$a_{ij}$或者叫attention权重；$c_t$是$t$时刻的语义向量。可见上述计算得到权重系数$\alpha_{ij}$的的过程主要分为两步：<strong>MLP+SOFTMAX</strong><br>Decoder:</p>
<script type="math/tex; mode=display">
\begin{align*}
s_i&=tanh(W[s_{i-1},y_{i-1},c_i])\\
o_i&=softmax(Vs_i)\\
\tag{5-6}
\end{align*}</script><h3 id="5-2-2-第二种：-Luong-Attention"><a href="#5-2-2-第二种：-Luong-Attention" class="headerlink" title="5.2.2. 第二种： Luong Attention"></a>5.2.2. 第二种： <strong>Luong Attention</strong></h3><p>源自此论文<a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">https://arxiv.org/abs/1508.04025</a> Effective Approaches to Attention-based Neural Machine Translation。该论文提出了两种不同的attention：global attention和local attention。前者attention将作用在源句子的每个词，计算量较大；后者是出于减小计算量的考虑，只关注一个区间内的词（并且是在一个句子内部）。</p>
<h4 id="5-2-2-1-Global-attention"><a href="#5-2-2-1-Global-attention" class="headerlink" title="5.2.2.1. Global attention"></a>5.2.2.1. Global attention</h4><p><img src="https://img-blog.csdnimg.cn/20210110210144453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图5.5 attention框架2.1-global attention</center>

<p><strong>Encoder与上一小节中的不同之处在于</strong>，采用最顶层的LSTM的隐藏状态用于计算后续的语义向量C。<br><strong>语义向量</strong>与上一小节的不同之处在于，在计算权重系数$\alpha_{ij}$的时候利用的是decoder中第$i$个隐藏状态（上一小节是第$i-1$个）和encoder中第$j$个隐藏状态来计算：</p>
<script type="math/tex; mode=display">
\begin{align*}
e_{ij}&=a(s_{i},h_j)\\
\alpha_{ij}&=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}\\
c_i&=\sum_{j=1}^{T_x}\alpha_{ij}h_j\\
 s_i&=tanh(W[s_{i-1},y_{i-1}])\\
\end{align*}</script><p>其中$a(s_{i},h_j)$的选择有多种：</p>
<script type="math/tex; mode=display">
a(s_{i},h_j)=\left\{  
\begin{align*}  
&s_i^Th_j &dot \\  
&s_i^TW_a h_j &general\\  
&v_a^Ttanh(W_a[s_i;h_j]) &concat \\
\end{align*}  
\right.
\tag{5-7}</script><p><strong>Decoder</strong>：</p>
<script type="math/tex; mode=display">
\begin{align*}
\tilde s_i&=tanh(W[s_i,c_i])\\
o_i&=softmax(V\tilde s_i)\\
\tag{5-8}
\end{align*}</script><h4 id="5-2-2-2-Local-Attention"><a href="#5-2-2-2-Local-Attention" class="headerlink" title="5.2.2.2. Local Attention"></a>5.2.2.2. Local Attention</h4><p>Global attention的缺点在于，预测某个target word时，需要计算该target对应的hidden state与encoder汇总所有hidden state之间的关系。当输入句子比较长时，这将非常耗时。Local attention就散为了解决这个问题，它只关注源句子中一个窗口内的词对应的hidden state。它的思想来源于soft-attention和hard-attention。<br><img src="https://img-blog.csdnimg.cn/20210110210559614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图5.6 attention框架2.2-local attention</center>

<p>具体来说，对于要预测的某个target word， 首先计算出一个对齐位置$p_j$,然后一个以$p_i$为中心的窗口$[p_i-D,p_i+D]$会被用来计算$c_i$。窗口大小由经验给定。计算$c_i$的方法与前面类似。<br>因此，重点在于如何计算$p_i$。论文给出了两种方法：</p>
<ol>
<li>单调对齐方式（Monotonic alignment）：假设源句子和目标句子是单调对齐的。取$p_i=i$。</li>
<li>预测对齐方式（Predictive alignment）：采用如下公式计算$p_i$:<script type="math/tex; mode=display">
p_i=S\cdot sigmoid(v_p^Ttanh(W_ps_i))\tag{5-9}</script>其中S是源句子的长度。同时，为了增大$p_i$附近的hidden state权重，权重系数将再乘上一个高斯分布：<script type="math/tex; mode=display">
a_{ij}=\alpha_{ij}\cdot exp\left(-\frac{(j-p_i)^2}{2\delta^2}\right)\tag{5-10}</script>其中$\delta=D/2$。$p_i$是个实数，而j是个整数<h3 id="5-2-3-Self-Attention"><a href="#5-2-3-Self-Attention" class="headerlink" title="5.2.3. Self-Attention"></a>5.2.3. <strong>Self-Attention</strong></h3>参考<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a><br>Self-Attention最著名的应用是在论文<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">https://arxiv.org/abs/1706.03762</a> Attention Is All You Need中，作为transformer结构的一个重要组成部分。<br>前面提到的几种attention，都是应用于基于RNN或者LSTM的encoder-decoder架构，通过计算decoder中隐藏状态和encoder中各个隐藏状态之间的关系来得到对应的权重。在《Attention Is All You Need》中，完全抛弃RNN/LSTM/CNN等结构，仅采用attention机制来进行机器翻译任务。并且在本论文中的attention机制采用的是self-attention，在encode源句子中一个词时，会计算这个词与源句子中其他词的相关性，减少了外部信息的依赖（这里我的理解是，与前面的几种attention机制不同，self-attention不需要decoder中的信息，也即encode一个句子时只用到了自身的信息，这就是self的含义）。<br>下面个详细讲解self-attention原理。<strong>第一阶段</strong>是计算三个vector，见下图：图片来源<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20210117001503897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图5.7 self-attention vectors</center>

<p>为了简单起见，假设输入句子只有两个词，是”Thinking Machines”。如上图所示：首先得到这两个词的Embedding向量$x_1,x_2$（跟一般的nlp任务中的Embedding向量是一个概念），然后对于每个词，都计算出三个向量：Query Vector，Key Vector和Value Vector。计算过程为分别乘以三个矩阵：$W^Q,W^K,W^V$。<br><strong>第二阶段</strong>是计算attention分数，假设现在要计算的是”Thinking”的attention值，那么需要计算”Thinking”与该句子中所有词的分数。需要计算$score(q_1,k_1),score(q_1,k_2)$。常见是计算公式是内积。过程如下图所示：<br><img src="https://img-blog.csdnimg.cn/20210117001439975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图5.8 self-attention score</center>

<p><strong>第三阶段</strong>是对分数进行归一化。首先除以$\sqrt d_k$（Key Vector的维度），这是为了避免梯度爆炸；然后进行softmax归一化，这样是为了使得分数都在$[0,1]$之间。<br><img src="https://img-blog.csdnimg.cn/20210117001427754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图5.9 self-attention softmax</center>

<p>这个分数代表了encode “Thinking”需要放置多少注意力在各个单词上,不妨记为$s_{1i}$。<br><strong>第四阶段</strong>就是利用上述得到分数对所有的Value Vector加权求和得到”Thinking”的representation： $z_1=\sum_{i=1}^{L_x}s_{1i}*v_i$<br><img src="https://img-blog.csdnimg.cn/20210117001414777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<center>图5.10 self-attention output</center>

<p>值得注意的是，上述过程是可以并行化计算的，这是self-attention相对于RNN,LSTM等序列模型的优势（在处理长序列问题上）。<br>最后，我们给出self-attention的紧凑表达式：</p>
<script type="math/tex; mode=display">
z=softmax(\frac{Q\cdot K}{\sqrt {d_k}})V\tag{5-11}</script><p>这样就一步到位得到了输入句子中所有词的represent vector。<br><img src="https://img-blog.csdnimg.cn/20210117161542872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-HzRS859m-1610871220672)(index_files/self-attention-matrix-calculation-2.png &quot;self-attention 紧凑计算形式&quot;)\]"></p>
<center>图5.11 self-attention 紧凑计算形式</center>

<h3 id="5-2-4-Multi-Head-Attention"><a href="#5-2-4-Multi-Head-Attention" class="headerlink" title="5.2.4. Multi-Head Attention"></a>5.2.4. Multi-Head Attention</h3><p>所谓multi-head attention，其实就是将输入的Embedding切分成多份，对每一份独立地进行self-attention，然后将结果concat在一起。这是为了增加模型对于其他位置的词的注意力能力。因为如果只是一个self-attention，很容易使得某个词的注意力大部分集中在它自身。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/16/统计学之t检验详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/16/统计学之t检验详解/" itemprop="url">统计学之t检验详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-16T00:31:29+08:00">
                2020-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://blog.csdn.net/Tonywu2018/article/details/83897806" target="_blank" rel="noopener">https://blog.csdn.net/Tonywu2018/article/details/83897806</a></p>
<h2 id="0-背景故事"><a href="#0-背景故事" class="headerlink" title="0. 背景故事"></a>0. 背景故事</h2><p>t检验又叫学生t检验（Student‘s t test），它是由20世纪爱尔兰的一家啤酒厂-健力士酒厂的一名员工（戈斯特）采用笔名“Student”发表的学术文章而得名。</p>
<h2 id="1-从一个例子引入t检验的思路"><a href="#1-从一个例子引入t检验的思路" class="headerlink" title="1. 从一个例子引入t检验的思路"></a>1. 从一个例子引入t检验的思路</h2><p>健力士公司是酿啤酒的，啤酒的原材料是麦子，因此公司种了很多麦田。假设有两片麦田，一块采用A工艺（旧）种植，另一块采用B工艺（新）种植。A工艺的麦田平均每株麦子可以结100粒穗子。公司想知道B工艺是否相比A工艺提高了产量。为了节约成本、小小损耗，抠门的资本家老板从B工艺的麦田里随机摘了5株大麦，每株麦子的平均穗子数量为120粒，看起来似乎产量提高了，因为每株麦子的麦穗粒数均值增加了20%。如何确定这样的结论是否可信呢？</p>
<blockquote>
<p>原假设：B工艺没有提高产量，即AB工艺下的每株麦子麦穗数量服从同一个分布<br>备选假设：B工艺提高了产量</p>
</blockquote>
<p>由中心极限定理，A攻一下每株麦穗的粒数服从均值为100，方差未知的正态分布：</p>
<script type="math/tex; mode=display">
X\sim N(\mu,\sigma^2)\tag{1-1}</script><p>B工艺的单株麦穗粒数也可以认为服从正态分布。如果原假设正确的话，B和A服从同样的正态分布。那么这时候我们可以去评估出现5株均值为120的麦穗的概率是否很极端，来判断原假设是否合理。可以对B的每株麦穗数的分布归一化为标准正态分布，再去查表评估其概率值。也即要计算$\frac{\bar x-\mu_0}{\delta_0}$，其中$\bar x$是B工艺的麦穗粒数均值，$\mu_0$为A工艺的麦穗粒数均值，$\delta_0$为A工艺的麦穗粒数均值。由于B工艺是抽取出一定的样本数来计算均值$\bar x$的，因此不能代表总体均值。当样本数很大时，根据大数定理可以直接认为B工艺提高了产量；当样本数很小时，可能是随机误差。因此，不妨对前面的式子再除以一个n相关的数。为此，戈斯特构造了一个新的统计量：</p>
<script type="math/tex; mode=display">
t=\frac{\bar x-\mu_0}{\delta_0/\sqrt n}\tag{1-2}</script><p>该统计量越大，寿命AB工艺导致的差别越大，越有可能说明B工艺提高了产量。</p>
<h2 id="3-t分布"><a href="#3-t分布" class="headerlink" title="3. t分布"></a>3. t分布</h2><p>对于t统计量：<script type="math/tex">t=\frac{\bar x-\mu_0}{\delta_0/\sqrt n}</script>,其对应的概率密度函数也即t分布为：</p>
<script type="math/tex; mode=display">
f(x)=\frac{\Gamma((\nu+1)/2)}{\sqrt(\nu \pi)\Gamma(\nu/2)}(1+t^2/\nu)^{-(\nu+1)/2}\tag{3-1}</script><p>其中$\nu=n-1$称为自由度，$\Gamma(x)=\int_0^{+\infty}t^{x-1}e^{-t}dt(x&gt;0)$是伽马函数。<br>t分布的函数图像与正态分布有点像，给定t值和自由度，可以通过查表的方式去找到对应的P值。t分布表如下：<br><img src="https://img-blog.csdnimg.cn/20200320000514397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="t分布表"></p>
<p>以本文中的例子为例，假设置信水平wie$\alpha=0.05$，查表得T值为2.132（单侧检验）。假设A工艺的标准差为$5\sqrt5$，可计算得出t=4，大于T。因此可以拒绝原假设。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/16/统计学之样本方差与总体方差/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/16/统计学之样本方差与总体方差/" itemprop="url">统计学之样本方差与总体方差</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-16T00:23:00+08:00">
                2020-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考资料：<a href="https://www.cnblogs.com/zzdbullet/p/10087196.html" target="_blank" rel="noopener">https://www.cnblogs.com/zzdbullet/p/10087196.html</a></p>
<h2 id="1-方差（variance）的定义"><a href="#1-方差（variance）的定义" class="headerlink" title="1. 方差（variance）的定义"></a>1. 方差（variance）的定义</h2><p>方差是用来度量随机变量和其数学期望（均值）之间的偏离程度的一个统计量。</p>
<p>统计学中（所有样本）的总体方差公式：</p>
<script type="math/tex; mode=display">
\sigma^2=\frac{\sum(X-\mu)^2}{N} \tag{1-1}</script><p>其中$\sigma^2$是总体方差，$X$是随机变量，$\mu$是总体均值（有时也用$\bar X$表示），$N$是总体样本数。这里提到的样本，是基于样本数量$N$（几乎）无限的假设。对应的各个统计量，也是所有的样本所服从的分布的真实参数，是客观正真实的。</p>
<h2 id="2-样本方差"><a href="#2-样本方差" class="headerlink" title="2. 样本方差"></a>2. 样本方差</h2><p>现实情况中，我们往往得不到所有的无限样本，而只能抽样出一定数量的有限样本。通过有限的样本来计算的方差，称为样本方差，公式如下：</p>
<script type="math/tex; mode=display">
S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar X)^2\tag{2-1}</script><p>注意上式的系数和总体方差公式里面的系数不一样，分母是$n-1$。为什么不用$n$作为分母呢？这是因为如果沿用总体方差的公式得到的样本方差，是对方差的一个有偏估计。用$n$作为分母的样本方差公式，才是对方差的无偏估计。</p>
<h2 id="3-总体方差公式的有偏性证明"><a href="#3-总体方差公式的有偏性证明" class="headerlink" title="3. 总体方差公式的有偏性证明"></a>3. 总体方差公式的有偏性证明</h2><script type="math/tex; mode=display">
\begin{align*}
\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2&=\frac{1}{n}\sum_{i=1}^{n}\left[(X_i-\mu)+(\mu-\bar X)\right]^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2+\frac{2}{n}\sum_{i=1}^{n}(X_i-\mu)(\mu-\bar X)+\frac{1}{n}\sum_{i=1}^{n}(\mu-\bar X)^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2+2(\bar X-\mu)(\mu-\bar X)+(\mu-\bar X)^2\\
&=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2-(\mu-\bar X)^2\\
\tag{3-1}
\end{align*}</script><p>换言之，除非正好有$\bar X=\mu$，否则一定会有</p>
<script type="math/tex; mode=display">
\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2<\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2\tag{3-2}</script><p>上式的右边是对方差的正确估计，左边是有偏估计。<br>产生这一偏差的本质是因为均值用的是样本均值$\bar X$。这将导致采样出来的样本之间不是完全相互独立的，自由度从$n$降为了$n-1$。（注意，一个好的采样有两点要求：随机采样，并且样本之间是相互独立的）这是因为，给定$\bar X$和任意$n-1$个样本，就能确定剩下的一个样本，也即只有$n-1$个样本是完全相互独立的，自由度为$n-1$。</p>
<h2 id="4-样本方差公式分母为n-1的推导"><a href="#4-样本方差公式分母为n-1的推导" class="headerlink" title="4. 样本方差公式分母为n-1的推导"></a>4. 样本方差公式分母为n-1的推导</h2><p>在正式推导之前，先给几个公式作为铺垫：</p>
<ol>
<li>方差计算公式：<script type="math/tex; mode=display">
D(X)=E(X^2)-[E(X)]^2\tag{4-1}</script></li>
<li>均值的均值：<script type="math/tex; mode=display">
\begin{align*}
E(\bar X)&=E\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)\\
&=\frac{1}{n}E(\sum_{i=1}^{n}X_i)\\
&=E(X_i)\\
&=\bar X\tag{4-4}
\end{align*}</script></li>
<li>均值的方差<script type="math/tex; mode=display">
\begin{align*}
D(\bar X)&=D\left(\frac{1}{n}\sum_{i=1}^nX_i\right)\\
&=\frac{1}{n^2}D(\sum_{i=1}^{n}X_i)\\
&=\frac{1}{n}D(X_i)\\
\tag{4-5}
\end{align*}</script></li>
</ol>
<p>对于没有修正的方差计算公式，计算其期望：</p>
<script type="math/tex; mode=display">
\begin{align*}
E(S^2)&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-\frac{2}{n}(X_i)(\bar X)+\frac{1}{n}\sum_{i=1}^{n}(\bar X)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-2(\bar X)^2+(\bar X)^2\right)\\
&=E\left(\frac{1}{n}\sum_{i=1}^{n}(x_i)^2-(\bar X)^2\right)\\
&=E((X_i)^2)-E((\bar X)^2)\\
&=D(X_i)+\left(E(X_i)\right)^2-\left(D(\bar X)+\left(E(\bar X)\right)^2\right)
\tag{4-6}
\end{align*}</script><p>结合{4-4}和{4-5}，可将{4-6}化简为</p>
<script type="math/tex; mode=display">
\begin{align*}
E(S^2)&=D(X_i)-\frac{1}{n}D(X_i)\\
&=\frac{n-1}{n}D(X_i)\\
&=\frac{n-1}{n}\sigma^2\\
\tag{4-7}
\end{align*}</script><p>要使样本方差的期望等于总体方差，就需要进行修正，也即给样本方差乘上$\frac{n}{n-1}$<br>因此得到修正后的样本方差公式:</p>
<script type="math/tex; mode=display">
\begin{align*}
S^2&=\frac{n}{n-1}\left(\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x)^2\right)\\
&=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2\\
\tag{4-8}
\end{align*}</script><p>推导完毕！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/08/15/统计学之假设检验详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/08/15/统计学之假设检验详解/" itemprop="url">统计学之假设检验详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-15T23:37:41+08:00">
                2020-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Statistics/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Statistics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://cosx.org/2010/11/hypotheses-testing/" target="_blank" rel="noopener">https://cosx.org/2010/11/hypotheses-testing/</a></p>
<h2 id="0-背景"><a href="#0-背景" class="headerlink" title="0. 背景"></a>0. 背景</h2><p>在实际生产生活中，我们经常需要对一些逻辑推理进行真假判断，例如</p>
<blockquote>
<p>如果你打了某种疫苗P，就不会得某种流行病Q<br>如果一个疑似病人隔离了14天还没确诊，那他就没有被感染新冠肺炎</p>
</blockquote>
<p>在统计学里面，不会像上面那样说，而是会说：</p>
<blockquote>
<p>如果你打了某种疫苗 ，就有95%的把握不会得流行病Q<br>如果一个疑似病人隔离了14天还没确诊，那他就有95%的把握没有被感染新冠肺炎</p>
</blockquote>
<p>其中的把握水平，在统计推断中用“置信水平”来代替。置信水平是可以人为选取的。</p>
<h2 id="1-从一个硬币的例子来引入假设检验"><a href="#1-从一个硬币的例子来引入假设检验" class="headerlink" title="1. 从一个硬币的例子来引入假设检验"></a>1. 从一个硬币的例子来引入假设检验</h2><p>如何从统计推断的角度来判断一个逻辑推理是否正确呢？通常，我们会给定一个置信水平，然后判断该逻辑推理是否在这个置信水平下成立。这里重新举一个硬币的例子，来引入置信水平的概念。<br>假设有如下命题：</p>
<blockquote>
<p>if P then Q<br>P: 在 100 次投掷中，得到 90 次正面，10 次反面。<br>Q: 硬币不是均匀的。</p>
</blockquote>
<p>我们想知道，如果P成立，判断Q成立的把握有多大。很多时候（但不是所有时候），在统计推断里面，要证明的结论都是直觉上可能性比较大的，直接证明可能不太方便，可以反其道行之，证明Q的反面是否成立，来推断出Q是否成立。为此，列出如下原假设和备择假设：</p>
<blockquote>
<p>H0: 硬币是均匀的（P）<br>Ha: 硬币是不均匀的（not P）</p>
</blockquote>
<p>如果原假设为真，即硬币是均匀的，就不可能会发生这样极端的事情比如：在 100 次投掷中，得到 90 次正面，10 次反面。如果真的观察到了这么极端的事情，就有把握认为硬币不是均匀的，则拒绝原假设，选择备选假设。如果观察到的是60次正面，40个反面，则没有特别大的把握拒绝原假设，这枚硬币是否有偏，需要更多的证据来证明（这通常意味着做更多的实验，比如再投1000次）。</p>
<p>即使观测到100次投掷中90次正面10次反面，也不能说硬币一定是不均匀的（也即不能百分之百的把握拒绝原假设）。如果原假设为真，但是拒绝了原假设，这种情况称为<strong>第一类错误</strong>。发生第一类错误的概率，称为<strong>显著性水平</strong>，用$\alpha$表示。$1-\alpha$称为<strong>置信度</strong>或者<strong>置信水平</strong>，它表示我们根据抽样样本对总体参数的估计的可靠性。$\alpha$一般是人为定的，如0.05，0.01.给定置信水平后，就可以去利用一些统计学的知识去检验原假设是否需要拒绝。<br>如果原假设是错误的，但是没有拒绝原假设，则称为<strong>第二类错误</strong>。如果要求犯第一类错误的概率尽可能小，就会导致第二类错误的概率增大；反之，如果要求第二类错误的Giallo极可能小，就会导致第一类错误的概率增大。在实际中需要权衡。权衡的方式就是调节$\alpha$。在实际中，我们通常认为犯第一类错误的后果比犯第二类错误的后果更为严重。例如，关于打疫苗会后会不会得病的命题，我们通常会将原假设写成：会得病，然后去搜集数据试图拒绝原假设。此时犯第一类错误的后果是比较严重的（实际会得病却认为不会得病，会放松警惕造成大流行），而犯第二类错误的后果不是很严重（实际不糊得病，却没有拒绝原假设，只是会将打疫苗的部分人隔离起来造成一定的不便）</p>
<p>再强调一下，一般都是先提出需要建议的假设，再搜集数据，这是统计推断的原则之一。因为如果现有了数据再提出假设，容易有主观干扰。<br>到这里，我们还是没有解答如何去检验原假设是否需要被拒绝。别急，接着往下看。</p>
<h2 id="2-P值"><a href="#2-P值" class="headerlink" title="2. P值"></a>2. P值</h2><p>如何去定义一个事件是否“极端”呢？首先我们引入“更极端”的概念。更极端，意味着概率更小。例如，91次正面9次反面，比90次正面10次反面，更为极端。因此，很自然地，我们只需要描述出原假设为真，第一类错误恰好为$\alpha$时的事件，然后判断出当前样本集合里面的事件是否比它更极端，就能判断是否要在当前显著性水平下拒绝原假设了。当然，直接这样比较麻烦，可以转换一下思路：计算出发生比当前事件（90次正面，10次反面）更极端的事件的概率P，判断P与$\alpha$的大小，如果$P&lt;\alpha$，则说明如果原假设为真时，发生当前事件的概率很极端（比我们给定的显著性水平$\alpha$还低），因此说明原假设不合理，于是可以拒绝原假设了。此时发生第一类错误的概率小于$\alpha$。这里的概率P，称为<strong>P值</strong>。<br>在硬币投掷实验中，正面出现的次数服$X$服从一个二项分布：$X\sim B(n,p)$，其中$n=100,p-0.5$。根据中心极限定理，二项分布的极限分布是正态分布，因此可以由均值为$np=50$，方差为$np(1-p)=25$的正态分布来近似。我们用这个近似的正态分布的两端去考察所谓“更极端”的事件。取$\alpha=0.05$，由正态分布的性质不难得到，$P$值等于$X<10$或$x>90$的概率值，等于$2\times P(X&lt;10)=1.2442e-15$。这个小于我们给定的$\alpha$，因此该事件很极端，原假设不合理，拒绝原假设。</10$或$x></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2020/02/20/nlp入门基础之语言模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2020/02/20/nlp入门基础之语言模型/" itemprop="url">nlp入门基础之语言模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-20T00:52:22+08:00">
                2020-02-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Tutorials-of-Natural-Language-Processing/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Natural Language Processing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="nlp入门基础之语言模型"><a href="#nlp入门基础之语言模型" class="headerlink" title="nlp入门基础之语言模型"></a>nlp入门基础之语言模型</h1><hr>
<p>@[toc]<br><a href="https://zhuanlan.zhihu.com/p/28080127" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28080127</a><br><a href="https://zhuanlan.zhihu.com/p/52061158" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/52061158</a><br><a href="https://blog.csdn.net/weixin_40056628/article/details/89364456" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40056628/article/details/89364456</a></p>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>语言模型（language model）是用来计算一个句子的概率的模型，或者预测下一个词出现的概率。通俗地讲，就是用来评估一个句子有多大的可能性是有意义的（是人话而不是一堆杂乱无章的词语组合）。<br>一段自然语言文本可以看做是一个离散时间序列$s=\omega_1,\omega_2,\cdots,\omega_T$，而一个语言模型的作用是构建这个时间序列的概率分布$P(s)$。概率计算公式可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align*}
P(s)&=P(\omega_1,\omega_2,\cdots,\omega_T)\\
&=P(\omega_1)P(\omega_2|\omega_1)P(\omega_3|\omega_1,\omega_2)\cdots P(\omega_T|\omega_1,\omega_2,\cdots,\omega_{T-1})\\
&=\prod_{t=1}^{T}P(\omega_t|\omega_1,\omega_2,\cdots,\omega_{t-1})\\
\tag{1-1}
\end{align*}</script><p>对于$P(\omega_t|\omega_1,\omega_2,\cdots,\omega_{t-1})$，可以在大量娱乐库中采用频率统计的方式来近似估计：</p>
<script type="math/tex; mode=display">
\begin{align*}
P(\omega_t|\omega_1,\omega_2,\cdots,\omega_{t-1})&=\frac{C(\omega_1,\omega_2,\cdots,\omega_t)}{\sum_{\omega}C(\omega_1,\omega_2,\cdots,\omega_{t-1},\omega)}\\
&=\frac{C(\omega_1,\omega_2,\cdots,\omega_t)}{C(\omega_1,\omega_2,\cdots,\omega_{t-1})}\\
\tag{1-2}
\end{align*}</script><p>直接计算上式是不现实的。假设词汇表大小为$V$，由上式可以看到，产生第$i$个词$\omega_i$的概率是由已经产生的$i-1$个词$\omega_1,\omega_2,\cdots,\omega_{i-1}$决定的，那么我们必须考虑所有$V^{i-1}$种不同历史情况下，产生第$i$个词的概率。这样模型中就会有$V^i$个自由参数（每个条件概率看成一个参数）。这在实际中几乎是无法从训练数据中估计出这些参数的。并且，很多词的组合可能在语料库中根本不存在，这样会导致最后估计出的概率为零（数据稀疏问题）。<br>因此需要引入语言模型来降低参数个数。语言模型有基于统计模型的，比如n元语法（n-gram），也有基于神经网络的。</p>
<h1 id="2-n元语法"><a href="#2-n元语法" class="headerlink" title="2. n元语法"></a>2. n元语法</h1><p>n元语法(n-grams)是基于n-1阶马尔科夫链的概率语言模型，也即在n-gram模型中，一个词的出现概率只与前面n-1个词有关：</p>
<script type="math/tex; mode=display">
P(\omega_1,\omega_2,\cdots,\omega_T)=\prod_{t=1}^{T}P(\omega_t|\omega_{t-(n-1)},\omega_2,\cdots,\omega_{t-1})\\\tag{2-1}</script><p>每个条件概率需要实现在大量语料库中根据频率近似求得。</p>
<ul>
<li>n=1: unigram，每个词独立于历史</li>
<li>n=2: bigram，每个词只与它前面的一个词有关。实际中常用</li>
<li>n=3: trigram，每个词只与它前面的两个词有关</li>
</ul>
<p>n元语法模型可能的缺陷：</p>
<ol>
<li>参数空间过大</li>
<li>数据稀疏</li>
</ol>
<h2 id="2-1-一元模型（unigram）"><a href="#2-1-一元模型（unigram）" class="headerlink" title="2.1. 一元模型（unigram）"></a>2.1. 一元模型（unigram）</h2><p>一元模型假设每句子中的每个每个词都是独立的，也即：</p>
<script type="math/tex; mode=display">
p(s)=p(\omega_1)p(\omega_2)\cdots p(\omega_T)\\\tag{2-2}</script><p>需要实现在语料库中统计每个字的频率。</p>
<h2 id="2-2-二元模型（bigram）"><a href="#2-2-二元模型（bigram）" class="headerlink" title="2.2. 二元模型（bigram）"></a>2.2. 二元模型（bigram）</h2><script type="math/tex; mode=display">
p(s)=p(\omega_1|start)p(\omega_2|\omega_1)\cdots p(\omega_T|end)\\\tag{2-3}</script><p>注意需要有句子开头和结尾标识符。实践中，需要先统计语料库中词语的两两组合情况的各自频率，再统计每个词的频率，以便得到条件概率。</p>
<h1 id="3-n-gram模型实践"><a href="#3-n-gram模型实践" class="headerlink" title="3. n-gram模型实践"></a>3. n-gram模型实践</h1><h2 id="3-1-文本分类"><a href="#3-1-文本分类" class="headerlink" title="3.1. 文本分类"></a>3.1. 文本分类</h2><p>假设类别有两类：$Y_1,Y_2$,原始文本为$X$。由贝叶斯公司可知：</p>
<script type="math/tex; mode=display">
P(Y_i|X)\propto P(X|Y_i)P(Y_i),i=1,2\tag{2-4}</script><p>$P(Y_i)$就是类别$Y_i$的文本比例；$P(X|Y_i)$就是在类别$Y_i$下句子$X$的概率，可以由n-gram算得</p>
<p>在上述贝叶斯假设条件下，可以简化过程，直接将训练样本的n-gram特征作为输入去训练一个分类器，得到分类模型。</p>
<h1 id="4-神经网络语言模型"><a href="#4-神经网络语言模型" class="headerlink" title="4. 神经网络语言模型"></a>4. 神经网络语言模型</h1><h2 id="4-1-基于前馈神经网络的语言模型"><a href="#4-1-基于前馈神经网络的语言模型" class="headerlink" title="4.1. 基于前馈神经网络的语言模型"></a>4.1. 基于前馈神经网络的语言模型</h2><p>Bengio在2003的论文A Neural Probabilistic Language Model。<br><img src="https://img-blog.csdnimg.cn/202007020011489.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图4.1 前馈神经网络语言模型</center>

<p>先给每个词在连续空间中赋予一个向量(词向量)，再通过神经网络去学习这种分布式表征。利用神经网络去建模当前词出现的概率与其前 n-1 个词之间的约束关系。很显然这种方式相比 n-gram 具有更好的<strong>泛化能力</strong>，<strong>只要词表征足够好。</strong>从而很大程度地降低了数据稀疏带来的问题。但是这个结构的明显缺点是仅包含了<strong>有限的前文信息</strong>。<br>该模型利用前n-1个词去预测下一个词，输入层是n-1个词的one-hot向量（每个向量是$1\times V$），再乘以一个$1\times D$的权重矩阵，得到$1\times D$的中间向量，将n-1个中间向量拼接成隐藏层（长度为$D(n-1)$）。隐藏层的激活函数为tanh，输出层为一个全连接层再接一个softmax函数生成概率分布。该模型的副产物就是词向量（输入层到隐藏层的权重矩阵）<br>这篇论文是词向量的鼻祖，后面的cbow和skip-gram都是由这里启发而来。</p>
<h2 id="4-2-基于循环神经网络的语言模型"><a href="#4-2-基于循环神经网络的语言模型" class="headerlink" title="4.2. 基于循环神经网络的语言模型"></a>4.2. 基于循环神经网络的语言模型</h2><p>为了解决定长信息（只能利用前面n-1个词的信息）的问题，Mikolov在2010的论文 Recurrent neural network based language model。<br><img src="https://img-blog.csdnimg.cn/20200702001205749.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<center>图4.2 RNN语言模型（图片来源见水印）</center>

<p>网络的输入层是”s我想你”，输出层可以看作是分别计算条件概率 P(w|s)、P(w|s我)、P(w|s我想)、P(w|s我想你) 在整个词表V中的值。而我们的目标就是使期望词对应的条件概率尽可能大。</p>
<p>相比单纯的前馈神经网络，隐状态的传递性使得RNN语言模型原则上可以捕捉前向序列的所有信息(虽然可能比较弱)。通过在整个训练集上优化交叉熵来训练模型，使得网络能够尽可能建模出自然语言序列与后续词之间的内在联系。</p>
<p>为了解决依赖的信息过长的问题，后续又有LSTM、attention等改进方法</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog2019/10/15/SVD分解推导证明/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="2019/10/15/SVD分解推导证明/" itemprop="url">SVD分解推导证明</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-15T22:55:48+08:00">
                2019-10-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="categories/Mathematical-Fundamentals/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematical Fundamentals</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SVD分解推导证明"><a href="#SVD分解推导证明" class="headerlink" title="SVD分解推导证明"></a>SVD分解推导证明</h1><hr>
<p>@[toc]</p>
<h2 id="0-线性代数与矩阵基础知识回顾"><a href="#0-线性代数与矩阵基础知识回顾" class="headerlink" title="0. 线性代数与矩阵基础知识回顾"></a>0. 线性代数与矩阵基础知识回顾</h2><p>本文讨论的范围实数空间，不涉及复数空间，因此各种术语和定理都以实空间下的名称为准，当然也可以推广到复数空间，只需进行一些名词的变换（对称矩阵-Hermitian举矩阵，正交矩阵-酉矩阵）。</p>
<h3 id="0-1-正交向量组"><a href="#0-1-正交向量组" class="headerlink" title="0.1. 正交向量组"></a>0.1. 正交向量组</h3><p>对于向量$x_1,x_2,\cdots,x_k\in R^n$，若$x_i^Tx_j=0,\forall (i,j)\in \{(i,j)\lvert 1\leq i&lt;j\leq k\}$，则称$x_1,x_2,\cdots,x_k$是一组正交向量组，简称<strong>正交组</strong>。如果正交组里面的向量还是归一化的，即$x_i^Tx_i=1,i=1,2,\cdots,k$，则称该正交组为<strong>标准正交组</strong>。</p>
<h3 id="0-2-正交矩阵"><a href="#0-2-正交矩阵" class="headerlink" title="0.2. 正交矩阵"></a>0.2. 正交矩阵</h3><p>一个实的正方矩阵$Q\in R^{n\times n}$称为正交矩阵，若：</p>
<script type="math/tex; mode=display">
QQ^T=Q^TQ=I\\
\tag{0,2}</script><p>由上述定义可以推出如下几个等价的叙述：</p>
<ol>
<li>$Q$是正交矩阵</li>
<li>$Q^T$是正交矩阵</li>
<li>$Q$是非奇异的，并且$Q^T=Q^{-1}$</li>
<li>$QQ^T=Q^TQ=I$</li>
<li>$Q=[q_1,q_2,\cdots,q_n]$的列组成标准正交组</li>
<li>$Q$的行组成标准正交组</li>
<li>$\forall x\in R^n, y=Qx$的Euclidean长度与$x$的Euclidean长度相等，也即有$y^Ty=x^Tx$。<h3 id="0-3-正定矩阵"><a href="#0-3-正定矩阵" class="headerlink" title="0.3. 正定矩阵"></a>0.3. 正定矩阵</h3>一个对称矩阵$A$称为</li>
</ol>
<ul>
<li>正定矩阵($A&gt;0$)：若二次型$x^TAx&gt;0,\forall x\neq 0$;</li>
<li>半正定矩阵($A\geq 0$)：若二次型$x^TAx\geq 0,\forall x\neq 0$;</li>
<li>负定矩阵：如果$-A$是正定的</li>
<li>半负定矩阵：如果$-A$是半正定的</li>
</ul>
<p>正定矩阵的等价条件：</p>
<ul>
<li>存在可逆矩阵$C$使$C^TC$等于该矩阵</li>
<li>正定矩阵$&lt;=&gt;$所有特征值取正实数，半正定矩阵$&lt;=&gt;$所有特征值取非负实数</li>
</ul>
<h3 id="0-4-特征值"><a href="#0-4-特征值" class="headerlink" title="0.4. 特征值"></a>0.4. 特征值</h3><ul>
<li>对于n阶对称矩阵$A$的特征值都是实数。</li>
<li>对于n阶对称矩阵$A$，总存在正交阵$Q$，使得$Q^{-1}AQ$是对角阵，且对角线元素就是$A$的特征值按从大到小排列。即$Q^{-1}AQ=diag(\lambda_1,\lambda_2,\cdots,\lambda_n),\lambda_1&gt;\lambda_2&gt;\cdots&gt;\lambda_n$。</li>
</ul>
<h2 id="1-奇异值分解（Singular-Value-Decomposition）"><a href="#1-奇异值分解（Singular-Value-Decomposition）" class="headerlink" title="1. 奇异值分解（Singular Value Decomposition）"></a>1. 奇异值分解（Singular Value Decomposition）</h2><h3 id="1-1-svd的数学描述"><a href="#1-1-svd的数学描述" class="headerlink" title="1.1. svd的数学描述"></a>1.1. svd的数学描述</h3><p>svd告诉我们，对任意$m\times n$的矩阵$A$，都可以表示为下面这种形式：</p>
<script type="math/tex; mode=display">
A=U\Sigma V^T\\
\tag{1,1}</script><p>其中$U$是$m \times m$的正交阵，$V$是$n\times n$的正交阵，$\Sigma$是$m\times n$的对角阵，</p>
<script type="math/tex; mode=display">
\Sigma=\begin{bmatrix} 
\Sigma_1 &0\\
0 &0 \\  
\end{bmatrix} \\
\tag{1,2}</script><p>$\Sigma_1=diag(\sigma_1,\sigma_2,\cdots,\sigma_r),r=rank(A)$，其对角元素按照从大到小排列</p>
<h3 id="1-2-svd的证明"><a href="#1-2-svd的证明" class="headerlink" title="1.2. svd的证明"></a>1.2. svd的证明</h3><p>在开始证明前，先给出几个引理：</p>
<ul>
<li>引理1：$A^TA$可对角化，且具有实的非负特征值（这是因为$A^TA$是半正定矩阵）</li>
<li>引理2：$rank(A)=rank(A^TA)=rank(AA^T)$</li>
<li>引理3：$A=0\ \Leftrightarrow\ A^TA=0$</li>
</ul>
<p>下面开始证明：<br>设$rank(A)=r$，根据引理1和引理2，存在$n\times n$的正交矩阵$V$，使得</p>
<script type="math/tex; mode=display">
V^T(A^TA)V=diag(\lambda_1,\lambda_2,\cdots,\lambda_n)\\
\tag{1,3}</script><p>其中$\lambda_1&gt;\lambda_2&gt;\cdots&gt;\lambda_r&gt;0=\lambda_{r+1}=\cdots=\lambda_n$为$A^TA$的n个非负特征根。<br>令$\Sigma_1=diag(\sigma_1,\sigma_2,\cdots,\sigma_r)=diag(\sqrt\sigma_1,\sqrt\sigma_2,\cdots,\sqrt\sigma_r)$，$V_1=[v_1,v_2,\cdots,v_r]，V_2=[v_{r+1},v_{r+2},\cdots,v_{n}]$，并且有$V=[V_1,V_2]$。<br>从而有$A^TAV_1=V_1\Sigma_1^2$，进一步得到：</p>
<script type="math/tex; mode=display">
\Sigma_1^{-1}V_1^TA^TAV_1\Sigma_1^{-1}=I\\
\tag{1,4}</script><p>令$U_1=AV_1\Sigma_1^{-1}$，则有$U_1^TU_1=I$。此时，我们可以选择$m-r$组标准正交向量与$U_1$的列向量组成一组标准正交基，也即$U=[U_1,U_2]$是一个m阶正交阵，且$U_1^TU_2=0$。<br>另一方面，容易得到：</p>
<script type="math/tex; mode=display">
A^TAV_2=V_20\ \Rightarrow\ V2^TA^TAV_2=0\\
\tag{1,4}</script><p>根据引理3可得$AV_2=0$</p>
<p>综上可得：</p>
<script type="math/tex; mode=display">
\begin{align*}
U^TAV&=\begin{bmatrix} 
U_1^TAV_1 &U_1^TAV_2\\
U_2^TAV_1 &U_2^TAV_2 \\  
\end{bmatrix} \\
&=\begin{bmatrix} 
\Sigma_1 &0\\
U_2^TU_1\Sigma_1 &0 \\  
\end{bmatrix} \\
&=\begin{bmatrix} 
\Sigma_1 &0\\
0 &0 \\  
\end{bmatrix} \\
&=\Sigma\\
\tag{1,5}
\end{align*}</script><p>也即$A=U\Sigma V^T$，到此奇异值分解定理得证。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><a class="extend next" rel="next" href="page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="images/avatar.jpg"
                alt="Guoxing Lan" />
            
              <p class="site-author-name" itemprop="name">Guoxing Lan</p>
              <p class="site-description motion-element" itemprop="description">On The Journey To Truth</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://lankuohsing.github.io/blog/archives">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async="" src="busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guoxing Lan</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
