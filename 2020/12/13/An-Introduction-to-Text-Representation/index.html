<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="../../../../lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="../../../../lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="../../../../css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="../../../../images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="../../../../images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="../../../../images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="../../../../images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Natural Language Processing,Neural Networks," />










<meta name="description" content="[TOC] 1. IntroductionBefore feeding our text data into a model, we need firstly transformed it to a numerical format that the model can understand. That is called text representation, and it is a nec">
<meta name="keywords" content="Natural Language Processing,Neural Networks">
<meta property="og:type" content="article">
<meta property="og:title" content="An Introduction to Text Representation">
<meta property="og:url" content="https://lankuohsing.github.io/blog/2020/12/13/An-Introduction-to-Text-Representation/index.html">
<meta property="og:site_name" content="Guoxing Lan">
<meta property="og:description" content="[TOC] 1. IntroductionBefore feeding our text data into a model, we need firstly transformed it to a numerical format that the model can understand. That is called text representation, and it is a nec">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2021-02-28T16:04:39.553Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Introduction to Text Representation">
<meta name="twitter:description" content="[TOC] 1. IntroductionBefore feeding our text data into a model, we need firstly transformed it to a numerical format that the model can understand. That is called text representation, and it is a nec">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lankuohsing.github.io/blog/2020/12/13/An-Introduction-to-Text-Representation/"/>





  <title>An Introduction to Text Representation | Guoxing Lan</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	<a href="https://github.com/lankuohsing"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Guoxing Lan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Journey To Truth</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="https://lankuohsing.github.io/blog" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://lankuohsing.github.io/blog/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="https://lankuohsing.github.io/blog/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="https://lankuohsing.github.io/blog/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="https://lankuohsing.github.io/blog/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lankuohsing.github.io/blog">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Guoxing Lan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="../../../../images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guoxing Lan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">An Introduction to Text Representation</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-13T23:39:11+08:00">
                2020-12-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="../../../../categories/Tutorials-of-Natural-Language-Processing/" itemprop="url" rel="index">
                    <span itemprop="name">Tutorials of Natural Language Processing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <hr>
<p>[TOC]</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Before feeding our text data into a model, we need firstly transformed it to a numerical format that the model can understand. That is called text representation, and it is a necessary data pre-processing procedure for almost every kind of nlp task. </p>
<p>From the view of language granularity, text representation can be divided into word representation, sentence representation and document representation. Word representation is the fundamental of the other two and we’ll introduce it first. Then we will introduce sentence representation in details. Documentation representation can be simiplified to sentence representation in many situations, so we won’t spend too much time on it.</p>
<h1 id="2-Word-Representation"><a href="#2-Word-Representation" class="headerlink" title="2. Word Representation"></a>2. Word Representation</h1><p>Word representation includes two categories of models: discrete word representation and distributed word representation. The major difference between discrete and distributed representation is whether it considers the relationships among words. Discrete representation assumes that words are independent, and it fills in the word vector mainly by extracting features from the properties of the word itself, such as occurrence and frequency. Therefore, the word vector of discrete representation can’t capture the semantic and syntactic information of the word, and it is usually high dimensional and sparse. On the contrary, distributed representation analyzes relationships among the word and other words and learns a dense and low-dimensional feature vector to represent the word vector. We will firstly introduce one-hot encoding, a classical and simple method of discrete representation. Then we’ll discuss word embedding, the most popular frame of distributed representation techniques in recent years. </p>
<h2 id="2-1-One-hot-Encoding"><a href="#2-1-One-hot-Encoding" class="headerlink" title="2.1. One-hot Encoding"></a>2.1. One-hot Encoding</h2><p>Given a fixed set of vocabulary $V=\{w_1,w_2,\cdots,w_{|V|}\}$, one-hot encoding encodes a word $w_i$ with a $|V|$-dimensional vector $X$, of which the i-th dimension where $w_i$ occurs is 1 and the other dimensions are all zeros.<br>For example, if we have a corpus as follows(This example will also be used in the later chapters, and we mark it as Example 1):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;I like mathematics.&quot;</span><br><span class="line">&quot;I like science.&quot;</span><br><span class="line">&quot;You like computer science.&quot;</span><br><span class="line">&quot;I like computer and science.&quot;</span><br></pre></td></tr></table></figure></p>
<p>Then the vocabulary is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;I, like, mathematics, science, You, computer, and&#125;</span><br></pre></td></tr></table></figure></p>
<p>For word “mathematics”, its one-hot vector is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(0,0,1,0,0,0,0)</span><br></pre></td></tr></table></figure></p>
<p>One-hot encoding only captures the information of the word’s occurrence and position in the vocabulary, neglecting the frequency information and co-occurrence of different words.<br>In practice, one-hot encoding can be transformed into storage of the indexes of words in the vocabulary, so the the characteristics of high dimensionality and sparsity won’t cause computation problems. However, it is seldomly used directly for word representation because of its lack of semantic and syntactic information. Instead, it is used to tokenizing the word in other representation methods.</p>
<h2 id="2-2-Word-Embedding"><a href="#2-2-Word-Embedding" class="headerlink" title="2.2. Word Embedding"></a>2.2. Word Embedding</h2><p>Generally, the phrase “word embedding” has two meanings: 1. The technique to find a function that maps a word to a multi-dimensional and dense vector. 2. The word vector obtained in 1.<br>As peviously mentioned, one-hot encoding has serval problems: high dimension, sparsity and lack of semantics. The first two problems can be easily solved in word embedding by choosing appropriate dimension number of the result vector. What about the third problem? We haven’t discuss the exact definition of semantics. We won’t discuss too much of it since it is inherited from linguistic, instead we only give some intuitions. Semantics represents the meaning of a word and words with similar meanings should have similar semantics.<br>With the hypothesis of distributional semantics——linguistic items with similar distributions have similar meanings, the problem of learning word semantics can be transformed into modeling the relations among the target word and its context, and that is what statistical language model does. In fact, the earliest word embedding is the by-product of neural networks language model. In the following sections, we will introduce some classical word embedding methods. </p>
<h3 id="2-2-1-Word2Vec"><a href="#2-2-1-Word2Vec" class="headerlink" title="2.2.1. Word2Vec"></a>2.2.1. Word2Vec</h3><h4 id="2-2-1-1-Continuous-Bag-of-Words-Model-CBOW"><a href="#2-2-1-1-Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="2.2.1.1. Continuous Bag of Words Model(CBOW)"></a>2.2.1.1. Continuous Bag of Words Model(CBOW)</h4><h4 id="2-2-1-2-Skip-Gram-Model"><a href="#2-2-1-2-Skip-Gram-Model" class="headerlink" title="2.2.1.2. Skip-Gram Model"></a>2.2.1.2. Skip-Gram Model</h4><p>[TODO]</p>
<h1 id="3-Sentence-Representation"><a href="#3-Sentence-Representation" class="headerlink" title="3. Sentence Representation"></a>3. Sentence Representation</h1><h2 id="3-1-Bag-of-Words"><a href="#3-1-Bag-of-Words" class="headerlink" title="3.1. Bag of Words"></a>3.1. Bag of Words</h2><p>The model of Bag-of-Words represents a sentence as a bag of words, and each word of the sentence is represented by one-hot encoding. Then the sentence vector is the sum of all of the word vector:</p>
<script type="math/tex; mode=display">
X(s)=\sum_{w_i\in s}{X(w_i)}</script><p>Where $X(w_i)$ is the one-hot encoding vector of $w_i$ in sentence $s$, and $X(s)$ is the vector of $s$.<br>In example 1, for a sentence “I like computer and science”, its sentence vector is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1,1,0,1,0,1,1)</span><br></pre></td></tr></table></figure></p>
<h3 id="3-1-1-Bag-of-Words-weighted-by-term-count"><a href="#3-1-1-Bag-of-Words-weighted-by-term-count" class="headerlink" title="3.1.1. Bag of Words weighted by term count"></a>3.1.1. Bag of Words weighted by term count</h3><p>In one-hot encoding, every word has the same value—1, in the non-zero component of its feature vector, which ignores importance of different words. Thus, it can’t distinguish these two sentences: “I like computer and science” and “I like computer and computer science”.<br>To evalute the importance of a word, a straightforward way is to multiply the one-hot vector by a weight—the count of the word in the sentence. Then, the vector of “I like computer and computer science” can be represented as:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1,1,0,1,0,2,1)</span><br></pre></td></tr></table></figure></p>
<h3 id="3-1-2-Word-of-Bags-weighted-by-TF-IDF"><a href="#3-1-2-Word-of-Bags-weighted-by-TF-IDF" class="headerlink" title="3.1.2. Word of Bags weighted by TF-IDF"></a>3.1.2. Word of Bags weighted by TF-IDF</h3><p>A better choice is to use TF-IDF(Term Frequency-Inverse Document Frequency. In this section, document is identity with sentence) as the weight, where $TF(t)=\frac{counts\ of\ word\ t\ in\ the\ doc}{num\ of\ words\ in\ the\ doc}$.<br>The original definition of IDF is:</p>
<script type="math/tex; mode=display">
IDF(t)=ln\frac{n}{df(t)}
$$, 
where $n$ is the number of documents while $df(t)$ is number of documents containing the target word $t$. To avoid division by zero, we can use the smoothing technique: $IDF(t)=ln\frac{1+n}{1+df(t)}$, and it looks like that there is a document containing all the words. What's more, to avoid $IDF=0$, we can add one to IDF:</script><p>IDF(t)=ln\frac{1+n}{1+df(t)}+1</p>
<script type="math/tex; mode=display">
Then, TF-IDF$=TF(t)\cdot IDF(t)$
Finally,</script><p>X(s)=\sum_{w_i\in s}{TF(w_i)\cdot IDF(w_i)\cdot X(w_i)}</p>
<p>$$</p>
<p>TF-IDF representation can be easily implemented by calling the following sk-learn functions:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vectorizer=CountVectorizer()</span><br><span class="line">transformer=TfidfTransformer()</span><br><span class="line">tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))</span><br></pre></td></tr></table></figure></p>
<p>or<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">transformer=TfidfVectorizer()</span><br><span class="line">tfidf2=transformer.fit_transform(corpus)</span><br></pre></td></tr></table></figure></p>
<h3 id="3-1-3-Bag-of-Words-with-Word-Embeddings"><a href="#3-1-3-Bag-of-Words-with-Word-Embeddings" class="headerlink" title="3.1.3. Bag of Words with Word Embeddings"></a>3.1.3. Bag of Words with Word Embeddings</h3><p>In this method, each word is represented by word embeddings instead of one-hot encoding. The sentence is then represented by the average or weighted average of all the word vectors, like what we discussed above.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="../../../../tags/Natural-Language-Processing/" rel="tag"># Natural Language Processing</a>
          
            <a href="../../../../tags/Neural-Networks/" rel="tag"># Neural Networks</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="../../../10/03/seq2seq入门详解：从RNN到Attention/" rel="next" title="seq2seq入门详解：从RNN到Attention">
                <i class="fa fa-chevron-left"></i> seq2seq入门详解：从RNN到Attention
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="../../../../2021/01/13/pytorch教程之自动求导机制-AUTOGRAD-从梯度和Jacobian矩阵讲起/" rel="prev" title="pytorch教程之自动求导机制(AUTOGRAD)-从梯度和Jacobian矩阵讲起">
                pytorch教程之自动求导机制(AUTOGRAD)-从梯度和Jacobian矩阵讲起 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="../../../../images/avatar.jpg"
                alt="Guoxing Lan" />
            
              <p class="site-author-name" itemprop="name">Guoxing Lan</p>
              <p class="site-description motion-element" itemprop="description">On The Journey To Truth</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="https://lankuohsing.github.io/blog/archives">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="../../../../categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="../../../../tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-number">1.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Word-Representation"><span class="nav-number">2.</span> <span class="nav-text">2. Word Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-One-hot-Encoding"><span class="nav-number">2.1.</span> <span class="nav-text">2.1. One-hot Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Word-Embedding"><span class="nav-number">2.2.</span> <span class="nav-text">2.2. Word Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-Word2Vec"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1. Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-1-Continuous-Bag-of-Words-Model-CBOW"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">2.2.1.1. Continuous Bag of Words Model(CBOW)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-2-Skip-Gram-Model"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">2.2.1.2. Skip-Gram Model</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Sentence-Representation"><span class="nav-number">3.</span> <span class="nav-text">3. Sentence Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Bag-of-Words"><span class="nav-number">3.1.</span> <span class="nav-text">3.1. Bag of Words</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-Bag-of-Words-weighted-by-term-count"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1. Bag of Words weighted by term count</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-Word-of-Bags-weighted-by-TF-IDF"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2. Word of Bags weighted by TF-IDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-Bag-of-Words-with-Word-Embeddings"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3. Bag of Words with Word Embeddings</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async="" src="busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guoxing Lan</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="../../../../lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="../../../../lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="../../../../lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="../../../../lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="../../../../lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="../../../../lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="../../../../js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="../../../../js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="../../../../js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="../../../../js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="../../../../js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="../../../../js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="../../../../js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
